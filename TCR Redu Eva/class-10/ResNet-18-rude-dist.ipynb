{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dea4f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers,regularizers,metrics,optimizers\n",
    "import random\n",
    "import pandas as pd\n",
    "from scipy.linalg import sqrtm\n",
    "import pickle\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\n",
    "import math\n",
    "import scipy.stats as st\n",
    "from scipy.special import comb\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import json\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c459c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "config=tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "config.gpu_options.allow_growth=True\n",
    "sess=tf.compat.v1.Session(config=config) \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047a8aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This algorithm is used to evaluate the structural redundancy of ResNet-18\n",
    "and outputs the evaluation criteria of hidden layer redundancy as well as \n",
    "the entire redundancy evaluation criteria under each pruning parameter. \n",
    "Here, \"Lam\" refers to the pruning parameter set used in the evaluation \n",
    "algorithm, and \"repeats\" represents the number of times the pruning network \n",
    "is repeatedly fine-tuned.\"\"\"\n",
    "Lam=[1.0,0.9,0.8,0.7]\n",
    "repeats=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e869de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "y_train_onehot=tf.keras.utils.to_categorical(y_train,num_classes=10)\n",
    "y_test_onehot=tf.keras.utils.to_categorical(y_test,num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1760ce31-8ae9-4c9b-9ae2-a1f51229e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_dist_ResNet_18.pkl', 'rb') as f:\n",
    "    [x_dist,y_dist]=pickle.load(f)\n",
    "x_dist=x_dist.numpy()\n",
    "y_dist=y_dist.numpy().reshape(len(y_dist),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b29e82-1535-4890-8962-851dfbb01ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 0.1\n",
    "weight_decay = 1e-4\n",
    "epochs = 200\n",
    "warmup_epochs = 5\n",
    "batch_size = 128\n",
    "image_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a00d3648-b747-4730-8a76-75ce7c9a318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, base_lr, total_steps, warmup_steps, warmup_lr=0.0):\n",
    "        super().__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.warmup_lr = warmup_lr\n",
    "    def __call__(self, step):\n",
    "        if step is None:\n",
    "            step = tf.constant(0)\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        total_steps = tf.cast(self.total_steps, tf.float32)\n",
    "        warmup_percent_done = step / warmup_steps\n",
    "        learning_rate = tf.where(\n",
    "            step < warmup_steps,\n",
    "            self.warmup_lr + (self.base_lr - self.warmup_lr) * warmup_percent_done,\n",
    "            self.base_lr * 0.5 * (1.0 + tf.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps)))\n",
    "        )\n",
    "        return learning_rate\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"base_lr\": self.base_lr,\n",
    "            \"total_steps\": self.total_steps,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"warmup_lr\": self.warmup_lr,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d66d49e7-17f5-4ca8-af13-e7e02c3a029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWeightDecaySGD(tf.keras.optimizers.SGD):\n",
    "    def __init__(self, weight_decay, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.weight_decay = weight_decay\n",
    "    def apply_gradients(self, grads_and_vars, name=None, experimental_aggregate_gradients=True):\n",
    "        super().apply_gradients(grads_and_vars, name, experimental_aggregate_gradients)\n",
    "        for grad, var in grads_and_vars:\n",
    "            if ('kernel' in var.name) and ('bn' not in var.name.lower()):\n",
    "                var.assign_sub(self.weight_decay * var)\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"weight_decay\": float(self.weight_decay),  # 确保是float\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b07c493-36c0-4ced-8d68-913c1e0a8d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastNSaver(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, n=10):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.history = deque(maxlen=n)  \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_acc = logs.get(\"val_accuracy\")\n",
    "        if val_acc is not None:\n",
    "            weights = self.model.get_weights()\n",
    "            self.history.append((val_acc, weights))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if not self.history:\n",
    "            return\n",
    "        best_acc, best_weights = max(self.history, key=lambda x: x[0])\n",
    "        print(f\" Using best val_acc={best_acc:.4f} from last {self.n} epochs\")\n",
    "        self.model.set_weights(best_weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47d65cd3-569b-4d19-a999-d595b1b517bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Res():\n",
    "    model = tf.keras.models.load_model('Res18_cifar10.h5',custom_objects={\n",
    "        'CustomWeightDecaySGD': CustomWeightDecaySGD,\n",
    "        'WarmUpCosine': WarmUpCosine\n",
    "    })\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e2e2c6f-0487-4831-9852-adf0323c615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_relu(x, filters, kernel_size, strides=1):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding='same',use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    return tf.keras.layers.ReLU()(x)\n",
    "\n",
    "def residual_block(x, filter1, filter2, downsample=False):\n",
    "    shortcut = x\n",
    "    strides = 2 if downsample else 1\n",
    "    x = conv_bn_relu(x, filter1, 3, strides)\n",
    "    x = tf.keras.layers.Conv2D(filter2, 3, strides=1, padding='same',use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    if downsample:\n",
    "        shortcut = tf.keras.layers.Conv2D(filter2, 1, strides=strides, padding='same',use_bias=False)(shortcut)\n",
    "        shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
    "    x = tf.keras.layers.add([x, shortcut])\n",
    "    return tf.keras.layers.ReLU()(x)\n",
    "\n",
    "def Res_model(NN,input_shape=(32,32,3), num_classes=10):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = conv_bn_relu(inputs, NN[2], 3)\n",
    "    #x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = residual_block(x, NN[0], NN[2])\n",
    "    x = residual_block(x, NN[1], NN[2])\n",
    "    x = residual_block(x, NN[3], NN[5], downsample=True)\n",
    "    x = residual_block(x, NN[4], NN[5])\n",
    "    x = residual_block(x, NN[6], NN[8], downsample=True)\n",
    "    x = residual_block(x, NN[7], NN[8])\n",
    "    x = residual_block(x, NN[9], NN[11], downsample=True)\n",
    "    x = residual_block(x, NN[10],NN[11])\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes,activation='softmax')(x)\n",
    "    return tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78515397",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_Res()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c71b4f71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 64)   1728        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 64)  256         ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 32, 32, 64)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 64)   36864       ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 64)  256         ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 32, 32, 64)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 64)   36864       ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 64)  256         ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 64)   0           ['batch_normalization_2[0][0]',  \n",
      "                                                                  're_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 32, 32, 64)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 64)   36864       ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 64)  256         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 32, 32, 64)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 64)   36864       ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 64)  256         ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 64)   0           ['batch_normalization_4[0][0]',  \n",
      "                                                                  're_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 32, 32, 64)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 16, 16, 128)  73728       ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 16, 16, 128)  512        ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 16, 16, 128)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 16, 16, 128)  147456      ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 16, 16, 128)  8192        ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 16, 16, 128)  512        ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 16, 16, 128)  512        ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 16, 16, 128)  0           ['batch_normalization_6[0][0]',  \n",
      "                                                                  'batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 16, 16, 128)  0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 16, 16, 128)  147456      ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 16, 16, 128)  512        ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 16, 16, 128)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 16, 16, 128)  147456      ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 16, 16, 128)  512        ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 16, 16, 128)  0           ['batch_normalization_9[0][0]',  \n",
      "                                                                  're_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)                 (None, 16, 16, 128)  0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 8, 8, 256)    294912      ['re_lu_8[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)                 (None, 8, 8, 256)    0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 8, 8, 256)    589824      ['re_lu_9[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 8, 8, 256)    32768       ['re_lu_8[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_11[0][0]', \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)                (None, 8, 8, 256)    0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 8, 8, 256)    589824      ['re_lu_10[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)                (None, 8, 8, 256)    0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 8, 8, 256)    589824      ['re_lu_11[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_14[0][0]', \n",
      "                                                                  're_lu_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)                (None, 8, 8, 256)    0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 4, 4, 512)    1179648     ['re_lu_12[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)                (None, 4, 4, 512)    0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 4, 4, 512)    2359296     ['re_lu_13[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 4, 4, 512)    131072      ['re_lu_12[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4, 4, 512)    0           ['batch_normalization_16[0][0]', \n",
      "                                                                  'batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)                (None, 4, 4, 512)    0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 4, 4, 512)    2359296     ['re_lu_14[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)                (None, 4, 4, 512)    0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 4, 4, 512)    2359296     ['re_lu_15[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4, 4, 512)    0           ['batch_normalization_19[0][0]', \n",
      "                                                                  're_lu_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)                (None, 4, 4, 512)    0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 512)         0           ['re_lu_16[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           5130        ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,183,562\n",
      "Trainable params: 11,173,962\n",
      "Non-trainable params: 9,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f58084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def JW(m, M):\n",
    "    \"\"\"\n",
    "    Compute the binary MD-LP J_w_value.\n",
    "\n",
    "    Args:\n",
    "        m: 1-D tensor of shape [d], the mean of the Minkowski difference of a \n",
    "           binary classification dataset.\n",
    "        M: 2-D tensor of shape [d, N], binary classification dataset Minkowski \n",
    "           difference set.\n",
    "           \n",
    "    Key idea:\n",
    "        - Calculate the approximate solution m_weighted for the optimal weights \n",
    "          in the MD-LP.\n",
    "        - Calculate the MD-LP based on the approximately optimal weights, and \n",
    "          perform a left truncation at 0.5. \n",
    "    Returns:\n",
    "        Binary MD-LP value.\n",
    "    \"\"\"\n",
    "    row_norm_sq = tf.reduce_sum(tf.square(M), axis=1)  \n",
    "    reciprocal_norm = tf.where(row_norm_sq != 0,\n",
    "                               tf.math.reciprocal(row_norm_sq),\n",
    "                               tf.zeros_like(row_norm_sq))  \n",
    "    m_weighted = m * reciprocal_norm  \n",
    "    m_weighted = tf.reshape(m_weighted, [1, -1])  \n",
    "    mM = tf.matmul(m_weighted, M)\n",
    "    L1 = tf.reduce_sum(mM)\n",
    "    L_1 = tf.reduce_sum(tf.abs(mM))\n",
    "    J_w_value = tf.abs(L1) / (L_1 + 1e-8)\n",
    "    J_w_value = tf.maximum(J_w_value, 0.5)\n",
    "    return J_w_value\n",
    "def W(X, Y, k, n_c=10):\n",
    "    \"\"\"\n",
    "    This function is used to calculate the top k largest binary classification \n",
    "    problems MD-LP used in the multi-classification problem calculation. Here, \n",
    "    the binary classification problems are obtained by combining each pair of \n",
    "    categories of the multi-classification problem.\n",
    "    Args:\n",
    "        X: Tensor/array of shape [b, l, w]. Channel output.\n",
    "        Y: Tensor/array of labels of shape [b]. Data labels.\n",
    "        k: Number of the largest binary MD-LP to keep.\n",
    "        n_c: Number of classes.\n",
    "    Returns:\n",
    "        JK_list: Tensor of shape [k], the top-k MD-LP.\n",
    "    \"\"\"\n",
    "    b, l, w = X.shape\n",
    "    X = tf.reshape(X, [b, l*w])   # flatten\n",
    "    J_list = []\n",
    "    for i, j in itertools.combinations(range(n_c), 2):\n",
    "        mask_1 = tf.reshape(tf.equal(Y, i), [-1])\n",
    "        mask_2 = tf.reshape(tf.equal(Y, j), [-1])\n",
    "        X1 = tf.boolean_mask(X, mask_1)\n",
    "        X2 = tf.boolean_mask(X, mask_2)\n",
    "        n1 = tf.shape(X1)[0]\n",
    "        n2 = tf.shape(X2)[0]\n",
    "        m_i = tf.reduce_sum(X1, axis=0) * tf.cast(n2, tf.float32) - tf.reduce_sum(X2, axis=0) * tf.cast(n1, tf.float32)\n",
    "        m_i = m_i / tf.linalg.norm(m_i + 1e-8)\n",
    "        M_i = tf.reshape(X1[:, None, :] - X2[None, :, :], [-1, l*w])\n",
    "        M_i = tf.transpose(M_i)\n",
    "        J = JW(m_i, M_i)\n",
    "        J_list.append(J)\n",
    "    J_list = tf.stack(J_list)\n",
    "    JK_list , JK_inde = tf.math.top_k(J_list,k)\n",
    "    return JK_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "779631d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1_channel(x_L,y, nnn, alpha=2.5):\n",
    "    \"\"\"\n",
    "    This function computes TCR measure of each channel in convolutional layer.\n",
    "    \n",
    "    Given the output of a convolutional layer, this function will execute:\n",
    "    - Treating each channel independently and computing a multi-class MD-LP \n",
    "      via function W;\n",
    "    - By applying nonlinear transformation, a TCR measure is constructed \n",
    "      to enhance the separability of MD-LP.\n",
    "    \n",
    "    Key Args:\n",
    "    x_L (Tensor):\n",
    "        Output of a convolutional hidden layer, with shape \n",
    "        [batch_size, height, width, channels].\n",
    "    y (Tensor):\n",
    "        Ground-truth labels corresponding to the input samples.\n",
    "    alpha (float, optional):\n",
    "        LP transformation parameter. Used to enhance the separability \n",
    "        of the MD-LP close to 1.\n",
    "    \n",
    "    Returns:\n",
    "    jw:\n",
    "        TCR measure of each channel.\n",
    "    \"\"\"\n",
    "    a, b, d, c = x_L.shape\n",
    "    jw = tf.zeros([c], dtype=tf.float32)\n",
    "    alpha = tf.cast(alpha, tf.float32)\n",
    "    for j in tf.range(c):\n",
    "        N_tf = W(x_L[:,:,:,j], y, nnn)\n",
    "        jw_j = tf.norm(N_tf) / tf.sqrt(float(nnn))\n",
    "        jw_j = (tf.exp(alpha * (2*jw_j-1)) - 1.0) / (tf.exp(alpha) - 1.0)\n",
    "        jw = tf.tensor_scatter_nd_update(jw, [[j]], [jw_j])\n",
    "    return jw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2acfaba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_channel(x_LG, y, prune_rate, nnn=45, esp=1e-8):\n",
    "    \"\"\"\n",
    "    This function computes the structural redundancy evaluation criterion R_L \n",
    "    and determines the set of retained channel indices `channel_i_label` used \n",
    "    by the pruning algorithm for a layer group.\n",
    "    \n",
    "    Given the output of a convolutional layer, this function will execute:\n",
    "    - By analyzing the propensity calculation of TCR measure, an evaluation \n",
    "      criterion for evaluating the redundancy of convolutional layers is derived.\n",
    "    - Based on the TCR measure, the pruning threshold is calculated and the \n",
    "      channels that remain after pruning are selected.\n",
    "    \n",
    "    Key Args:\n",
    "    x_LG (a list of Tensors):\n",
    "        Output of a group of convolutional hidden layers.\n",
    "    y (Tensor):\n",
    "        Ground-truth labels corresponding to the input samples.\n",
    "    prune_rate (float):\n",
    "        Pruning parameter. Used to control the strictness of pruning.\n",
    "    \n",
    "    Returns:\n",
    "    channel_i_label (ndarray):\n",
    "        Indices of channels retained after pruning.\n",
    "    R_L (float):\n",
    "        Structural redundancy evaluation criterion of the layer group,\n",
    "    \"\"\"\n",
    "    a, b, d, c = x_LG[-1].shape\n",
    "    L1_list = []\n",
    "    for i in range(len(x_LG)):\n",
    "        L1_i = L1_channel(x_LG[i], y, nnn)\n",
    "        L1_list.append(L1_i)\n",
    "    L1 = tf.stack(L1_list, axis=0)  \n",
    "    jw = tf.reduce_mean(L1, axis=0) \n",
    "    jw_min = tf.maximum(tf.reduce_min(jw) - esp, 0.0)\n",
    "    jw_max = tf.reduce_max(jw)\n",
    "    me = tf.sqrt(tf.reduce_mean(tf.square(jw - jw_min)))\n",
    "    jd = jw_min + prune_rate * me\n",
    "    mean = tf.maximum(tf.reduce_mean(jw) - esp, 0.0)\n",
    "    R_L = tf.reduce_mean(tf.sign(jw - mean))\n",
    "    channel_i_label = tf.where(jw >= jd)[:,0]\n",
    "    return channel_i_label.numpy(), R_L.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62555afb-03be-491e-a73b-8a314b2941ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Group_1(x_L,w,S):\n",
    "    \"\"\"This function is used to collect the key hidden layer outputs of the first block \n",
    "    in ResNet. Here, x_L1 is used to update the list of output from the ResNet \n",
    "    convolutional layers, and x_L2 is used to provide the hidden layer outputs required \n",
    "    for layer group pruning.\"\"\"\n",
    "    wb=w[0]\n",
    "    bn=w[1]\n",
    "    x_L1=[]\n",
    "    x_L2=[]\n",
    "    x_1=layer_xL(\"conv2d\",x_L,w=wb[0])\n",
    "    x_1=layer_xL(\"batch_normalization\",x_1,w=bn[0])\n",
    "    x_1=layer_xL(\"activation\",x_1)\n",
    "    x_L2.append(deepcopy(x_1))\n",
    "    #x_1=layer_xL(\"maxpooling\",x_1)\n",
    "    x_L1.append(deepcopy(x_1))\n",
    "\n",
    "    x_2=layer_xL(\"conv2d\",x_1,w=wb[1])\n",
    "    x_2=layer_xL(\"batch_normalization\",x_2,w=bn[1])\n",
    "    x_2=layer_xL(\"activation\",x_2)\n",
    "    x_L1.append(deepcopy(x_2))\n",
    "    \n",
    "    x_2=layer_xL(\"conv2d\",x_2,w=wb[2])\n",
    "    x_2=layer_xL(\"batch_normalization\",x_2,w=bn[2])\n",
    "    x_1=layer_xL(\"add\",[x_2,x_1])\n",
    "    x_1=layer_xL(\"activation\",x_1)\n",
    "    x_L1.append(deepcopy(x_1))\n",
    "    x_L2.append(deepcopy(x_1))\n",
    "\n",
    "    x_2=layer_xL(\"conv2d\",x_1,w=wb[3])\n",
    "    x_2=layer_xL(\"batch_normalization\",x_2,w=bn[3])\n",
    "    x_2=layer_xL(\"activation\",x_2)\n",
    "    x_L1.append(deepcopy(x_2))\n",
    "    \n",
    "    x_2=layer_xL(\"conv2d\",x_2,w=wb[4])\n",
    "    x_2=layer_xL(\"batch_normalization\",x_2,w=bn[4])\n",
    "    x_1=layer_xL(\"add\",[x_2,x_1])\n",
    "    x_1=layer_xL(\"activation\",x_1)\n",
    "    x_L1.append(deepcopy(x_1))\n",
    "    x_L2.append(deepcopy(x_1))\n",
    "    return x_L1,x_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b224b60b-5f03-4f1d-9d30-287e9dcc8aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Group_L(x_L,w,S):\n",
    "    \"\"\"This function is used to collect the key hidden layer outputs of the other block \n",
    "    in ResNet. Here, x_L1 is used to update the list of output from the ResNet \n",
    "    convolutional layers, and x_L2 is used to provide the hidden layer outputs required \n",
    "    for layer group pruning.\"\"\"\n",
    "    wb=w[0]\n",
    "    bn=w[1]\n",
    "    x_L1=[]\n",
    "    x_L2=[]\n",
    "    x_1=layer_xL(\"conv2d\",x_L,w=wb[0],S=2)\n",
    "    x_1=layer_xL(\"batch_normalization\",x_1,w=bn[0])\n",
    "    x_1=layer_xL(\"activation\",x_1)\n",
    "    x_L1.append(deepcopy(x_1))\n",
    "            \n",
    "    x_1=layer_xL(\"conv2d\",x_1,w=wb[1])\n",
    "    x_2=layer_xL(\"conv2d\",x_L,w=wb[2],S=2)\n",
    "    x_1=layer_xL(\"batch_normalization\",x_1,w=bn[1])\n",
    "    x_2=layer_xL(\"batch_normalization\",x_2,w=bn[2])\n",
    "    x_1=layer_xL(\"add\",[x_1,x_2])\n",
    "    x_1=layer_xL(\"activation\",x_1)\n",
    "    x_L1.append(deepcopy(x_1))\n",
    "    x_L2.append(deepcopy(x_1))\n",
    "            \n",
    "    x_2=layer_xL(\"conv2d\",x_1,w=wb[3])\n",
    "    x_2=layer_xL(\"batch_normalization\",x_2,w=bn[3])\n",
    "    x_2=layer_xL(\"activation\",x_2)\n",
    "    x_L1.append(deepcopy(x_2))\n",
    "    \n",
    "    x_2=layer_xL(\"conv2d\",x_2,w=wb[4])\n",
    "    x_2=layer_xL(\"batch_normalization\",x_2,w=bn[4])\n",
    "    x_1=layer_xL(\"add\",[x_2,x_1])\n",
    "    x_1=layer_xL(\"activation\",x_1)\n",
    "    x_L1.append(deepcopy(x_1))\n",
    "    x_L2.append(deepcopy(x_1))\n",
    "    return x_L1,x_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b348156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_xL(layer_name,x_L,w=None,F=False,S=1):\n",
    "    \"\"\"This function is used to obtain the outputs of various hidden layers or layer groups in the \n",
    "    network, which is utilized for updating the outputs of hidden layers or for performing layer \n",
    "    group pruning calculations.\"\"\"\n",
    "    if \"conv2d\" in layer_name:\n",
    "        weight=w[0]\n",
    "        strides=[1,S,S,1]\n",
    "        x_L1=tf.nn.conv2d(x_L,weight,strides=strides,padding=\"SAME\")\n",
    "        #x_L1=tf.nn.bias_add(x_L1,bias)\n",
    "        return x_L1\n",
    "    if \"first\" in layer_name:\n",
    "        x_L1,x_L2=Group_1(x_L,w,S)\n",
    "        if F==True:\n",
    "            return x_L2\n",
    "        else:\n",
    "            return x_L1\n",
    "    if \"group\" in layer_name:\n",
    "        x_L1,x_L2=Group_L(x_L,w,S)\n",
    "        if F==True:\n",
    "            return x_L2\n",
    "        else:\n",
    "            return x_L1\n",
    "    if \"batch_normalization\" in layer_name:\n",
    "        gamma,beta,mean,var=w\n",
    "        x_L1=tf.nn.batch_normalization(x_L,mean=mean,\n",
    "                                          variance=var,\n",
    "                                          offset=beta,\n",
    "                                          scale=gamma,variance_epsilon=1e-5)\n",
    "        return x_L1\n",
    "    if \"activation\" in layer_name:\n",
    "        x_L1=tf.nn.relu(x_L)\n",
    "        return x_L1\n",
    "    if \"add\" in layer_name:\n",
    "        x1,x2=x_L\n",
    "        x_L1=tf.math.add(x1,x2)\n",
    "        return x_L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "355a0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_block(a,b,G,weight_list,x_LG,First=False,Group=False,R=False,F=False):\n",
    "    \"\"\"This function is used to update the list of hidden layer outputs \n",
    "    of ResNet after function pruning. For single-hidden-layer pruning, \n",
    "    it only needs to update the output of the convolutional layer within \n",
    "    the block. However, for layer group pruning, since it involves multiple \n",
    "    blocks, the output of all convolutional layers in these blocks needs \n",
    "    to be updated.\"\"\"\n",
    "    if Group==False:\n",
    "        if R==True:\n",
    "            x_1=x_LG[b]\n",
    "            layer_l=G[a][0]\n",
    "            wc=weight_list[layer_l]\n",
    "            wb=weight_list[layer_l+1]\n",
    "            x_2=layer_xL(\"conv2d\",x_1,wc,S=2)\n",
    "            x_2=layer_xL(\"batch_normalization\",x_2,wb)\n",
    "            x_2=layer_xL(\"activation\",x_2)\n",
    "            x_LG[b+1]=x_2\n",
    "            layer_l=G[a+2][0]\n",
    "            wc=weight_list[layer_l]\n",
    "            wb=weight_list[layer_l+2]\n",
    "            x_2=layer_xL(\"conv2d\",x_2,wc)\n",
    "            x_2=layer_xL(\"batch_normalization\",x_2,wb)\n",
    "            layer_l=G[a+2][1]\n",
    "            wc=weight_list[layer_l]\n",
    "            wb=weight_list[layer_l+2]\n",
    "            x_1=layer_xL(\"conv2d\",x_1,wc,S=2)\n",
    "            x_1=layer_xL(\"batch_normalization\",x_1,wb)\n",
    "            x_1=layer_xL(\"add\",[x_2,x_1])\n",
    "            x_1=layer_xL(\"activation\",x_1)\n",
    "            x_LG[b+2]=x_1\n",
    "            return x_LG\n",
    "        if R==False:\n",
    "            x_1=x_LG[b]\n",
    "            layer_l=G[a][0]\n",
    "            wc=weight_list[layer_l]\n",
    "            wb=weight_list[layer_l+1]\n",
    "            x_2=layer_xL(\"conv2d\",x_1,wc)\n",
    "            x_2=layer_xL(\"batch_normalization\",x_2,wb)\n",
    "            x_2=layer_xL(\"activation\",x_2)\n",
    "            x_LG[b+1]=x_2\n",
    "            if First==True:\n",
    "                layer_l=G[a+2][1]\n",
    "            else:\n",
    "                layer_l=G[a+1][2]\n",
    "            wc=weight_list[layer_l]\n",
    "            wb=weight_list[layer_l+1]\n",
    "            x_2=layer_xL(\"conv2d\",x_2,wc)\n",
    "            x_2=layer_xL(\"batch_normalization\",x_2,wb)\n",
    "            x_1=layer_xL(\"add\",[x_2,x_1])\n",
    "            x_1=layer_xL(\"activation\",x_1)\n",
    "            x_LG[b+2]=x_1\n",
    "            return x_LG\n",
    "    if Group==True:\n",
    "        if First==True:\n",
    "            label=[G[a][0],G[a-2][0],G[a][1],G[a-1][0],G[a][2]]\n",
    "            w1=[]\n",
    "            w2=[]\n",
    "            x_1=x_LG[b]\n",
    "            for i in range(5):\n",
    "                w1.append(weight_list[label[i]])\n",
    "                w2.append(weight_list[label[i]+1])\n",
    "            w=[w1,w2]\n",
    "            x_1=layer_xL(\"first\",x_1,w,F=False,S=1)\n",
    "            for i in range(5):\n",
    "                x_LG[b+i+1]=x_1[i]\n",
    "            return x_LG\n",
    "        else:\n",
    "            label=[G[a-2][0],G[a][0],G[a][1],G[a-1][0],G[a][2]]\n",
    "            w1=[]\n",
    "            w2=[]\n",
    "            x_1=x_LG[b]\n",
    "            l=[1,2,2,1,1]\n",
    "            for i in range(5):\n",
    "                w1.append(weight_list[label[i]])\n",
    "                w2.append(weight_list[label[i]+l[i]])\n",
    "            w=[w1,w2]\n",
    "            x_1=layer_xL(\"group\",x_1,w)\n",
    "            for i in range(4):\n",
    "                x_LG[b+i+1]=x_1[i]\n",
    "            return x_LG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4ac50a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(a,b,G,x_LG,weight_list,First=False):\n",
    "    \"\"\"This function is used to provide the required output list \n",
    "    for 'prune_channel' function when performing layer group pruning.\"\"\"\n",
    "    x_l=x_LG[b]\n",
    "    if First==True:\n",
    "        label=[G[a][0],G[a-2][0],G[a][1],G[a-1][0],G[a][2]]\n",
    "    else:\n",
    "        label=[G[a-2][0],G[a][0],G[a][1],G[a-1][0],G[a][2]]\n",
    "    w1=[]\n",
    "    w2=[]\n",
    "    for i in range(5):\n",
    "        w1.append(weight_list[label[i]])\n",
    "    l=[1,2,2,1,1]\n",
    "    if First==True:\n",
    "        for i in range(5):\n",
    "            w2.append(weight_list[label[i]+1])\n",
    "        w=[w1,w2]\n",
    "        x_L=layer_xL('first',x_l,w,F=True)\n",
    "    else:\n",
    "        for i in range(5):\n",
    "            w2.append(weight_list[label[i]+l[i]])\n",
    "        w=[w1,w2]\n",
    "        x_L=layer_xL('group',x_l,w,F=True)\n",
    "    return x_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f4a2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model,G,P,x,y,prune_rate,q=67):\n",
    "    \"\"\"\n",
    "    Structured Channel Group Pruning Function Based on MD-LP (Channel-wise Pruning) \n",
    "    \n",
    "    This function is the main function for pruning in ResNet-18. It achieves \n",
    "    the pruning of layer groups by using the provided convolution layer groups G.\n",
    "    The main process of this function is as follows:\n",
    "    - Pruning Preparation: Based on the hidden layer positions provided by P, \n",
    "      construct the input/output lists for the pruning-required convolutional layers, \n",
    "      as well as the list of convolution kernel parameters and BN layer parameters.\n",
    "    - Layer Group Pruning: In accordance with the sequence in G, the pruning_channel \n",
    "      function is used to perform pruning successively, resulting in the channels \n",
    "      that are retained after pruning, which are labeled as channel_new_label.\n",
    "    - Output/Parameter Update: Based on the channel_new_label, the parameters of the \n",
    "      convolutional layers included in the group, as well as the parameters of the \n",
    "      convolutional layers whose outputs are used as inputs, are updated. And the \n",
    "      input/output lists of the convolutional layers are updated according to the \n",
    "      updated parameters.\n",
    "    \n",
    "    Input:\n",
    "    model : Original Keras ResNet-18\n",
    "    x : Network input samples (used for forward propagation and channel evaluation)\n",
    "    y : Sample labels (used for metric calculation in prune_channel)\n",
    "    prune_rate : Pruning parameter\n",
    "    G : The grouped list of network convolution layers, based on the ReNet network \n",
    "    structure, divides the hidden layers of ResNet. It is used to ensure that the \n",
    "    output results of the network hidden layers within the same group after pruning \n",
    "    can still maintain the same size and can be added together.\n",
    "    \n",
    "    Output:\n",
    "    weight_list: List of weights for each convolutional / BN layer after pruning.\n",
    "    channel_label: Record of the number of retained channels for each layer group.   \n",
    "    \"\"\"\n",
    "    layer_outputs =[layer.output for layer in model.layers] \n",
    "    weight_list=[]\n",
    "    # =========================\n",
    "    # Collect the input/output of the convolutional layers, and the parameters\n",
    "    # of the convolutional layers and BN layers in the ResNet network.\n",
    "    # =========================\n",
    "    for i in range(q+1):\n",
    "        layer=model.layers[i]\n",
    "        if \"conv\" in layer.name:\n",
    "            w=layer.get_weights()\n",
    "            weight_list.append(w)\n",
    "            #print(i)\n",
    "        elif \"dense\" in layer.name:\n",
    "            w,b=layer.get_weights()\n",
    "            weight_list.append([w,b])\n",
    "        elif \"batch_normalization\" in layer.name:\n",
    "            g,b,m,v=layer.get_weights()\n",
    "            weight_list.append([g,b,m,v])\n",
    "        else:\n",
    "            weight_list.append(None)\n",
    "    x_LG=[]\n",
    "    for i in range(len(P)):\n",
    "        activation_model = tf.keras.models.Model(inputs=model.input,outputs=layer_outputs[P[i]])\n",
    "        layer_x=activation_model.predict(x)\n",
    "        x_LG.append(layer_x)\n",
    "    channel_label=[]\n",
    "    b=1\n",
    "    for i in range(len(G)):\n",
    "        if len(G[i])==1:\n",
    "            # =========================\n",
    "            # According to the prune_channel function, the single convolution layer in G is pruned, \n",
    "            # resulting in the retained channels after pruning. Based on the pruning results, the \n",
    "            # network parameters and network input/output are updated.\n",
    "            # =========================\n",
    "            if (b-1)%4==0:\n",
    "                a=G[i][0]\n",
    "                channel_new_label,r_L=prune_channel([x_LG[b+1]],y,prune_rate)\n",
    "                print(len(channel_new_label),0)\n",
    "                weight_list[a][0]=weight_list[a][0][:,:,:,channel_new_label]\n",
    "                for j in range(4):\n",
    "                    weight_list[a+1][j]=weight_list[a+1][j][channel_new_label]\n",
    "                if i==0:\n",
    "                    a1=G[i+2][1]\n",
    "                    weight_list[a1][0]=weight_list[a1][0][:,:,channel_new_label,:]\n",
    "                    x_LG=x_block(i,b,G,weight_list,x_LG,First=True)\n",
    "                else:\n",
    "                    a1=G[i+2][0]\n",
    "                    weight_list[a1][0]=weight_list[a1][0][:,:,channel_new_label,:]\n",
    "                    x_LG=x_block(i,b,G,weight_list,x_LG,First=False,R=True)\n",
    "                channel_label.append(len(channel_new_label))\n",
    "                b+=2\n",
    "                continue\n",
    "            if (b-1)%4==2:\n",
    "                a=G[i][0]\n",
    "                a1=G[i+1][2]\n",
    "                channel_new_label,r_L=prune_channel([x_LG[b+1]],y,prune_rate)\n",
    "                print(len(channel_new_label),1)\n",
    "                weight_list[a][0]=weight_list[a][0][:,:,:,channel_new_label]\n",
    "                for j in range(4):\n",
    "                    weight_list[a+1][j]=weight_list[a+1][j][channel_new_label]\n",
    "                weight_list[a1][0]=weight_list[a1][0][:,:,channel_new_label,:]\n",
    "                x_LG=x_block(i,b,G,weight_list,x_LG)\n",
    "                channel_label.append(len(channel_new_label))\n",
    "                if i==1:\n",
    "                    b-=3\n",
    "                else:\n",
    "                    b-=2\n",
    "                continue\n",
    "        if len(G[i])>1:\n",
    "            # =========================\n",
    "            # According to the prune_channel function, the layer group in G with multiple convolutional \n",
    "            # layers is pruned to obtain a unified set of pruned channels. Based on the pruning results, \n",
    "            # the network parameters and network input/output are updated.\n",
    "            # =========================\n",
    "            if i==2:\n",
    "                x_LP=get_x(i,b,G,x_LG,weight_list,First=True)\n",
    "            else:\n",
    "                x_LP=get_x(i,b,G,x_LG,weight_list)\n",
    "            channel_new_label,r_L=prune_channel(x_LP,y,prune_rate)\n",
    "            print(len(channel_new_label),2)\n",
    "            for g in G[i]:\n",
    "                weight_list[g][0]=weight_list[g][0][:,:,:,channel_new_label]\n",
    "            weight_list[G[i-1][0]][0]=weight_list[G[i-1][0]][0][:,:,channel_new_label,:]\n",
    "            if i==2:\n",
    "                for g in G[i]:\n",
    "                    for j in range(4):\n",
    "                        weight_list[g+1][j]=weight_list[g+1][j][channel_new_label]\n",
    "                weight_list[G[i-2][0]][0]=weight_list[G[i-2][0]][0][:,:,channel_new_label,:]\n",
    "            else:\n",
    "                l=[2,2,1]\n",
    "                for g in range(3):\n",
    "                    for j in range(4):\n",
    "                        weight_list[G[i][g]+l[g]][j]=weight_list[G[i][g]+l[g]][j][channel_new_label]\n",
    "            if i!=len(G)-1:\n",
    "                weight_list[G[i+1][0]][0]=weight_list[G[i+1][0]][0][:,:,channel_new_label,:]\n",
    "                weight_list[G[i+3][1]][0]=weight_list[G[i+3][1]][0][:,:,channel_new_label,:]\n",
    "            if i==2:\n",
    "                x_LG=x_block(i,b,G,weight_list,x_LG,First=True,R=False,Group=True)\n",
    "                b+=5\n",
    "            else:\n",
    "                x_LG=x_block(i,b,G,weight_list,x_LG,First=False,R=False,Group=True)\n",
    "                b+=4\n",
    "            channel_label.append(len(channel_new_label))\n",
    "            continue\n",
    "    weight_list[q][0]=weight_list[q][0][channel_new_label]\n",
    "    return weight_list,channel_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71aa9470-af2e-4ebb-9657-5ff1ae7d5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pr(model,weight_list,channel_label):\n",
    "    \"\"\"This function is used to construct a pruned network by using \n",
    "    the given pruned network structure and parameters.\"\"\"\n",
    "    model_p=Res_model(channel_label)\n",
    "    for i in range(len(weight_list)):\n",
    "        if weight_list[i]!=None:\n",
    "            w = [ww for ww in weight_list[i]]\n",
    "            model_p.layers[i].set_weights(w)\n",
    "    return model_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21977bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "G=[[4],[11],[1,7,14],[18],[27],[21,22,30],[34],[43],[37,38,46],[50],[59],[53,54,62]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73763289",
   "metadata": {},
   "outputs": [],
   "source": [
    "P=[0,3,6,10,13,17,20,26,29,33,36,42,45,49,52,58,61,65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c451b93e-3b25-44cf-b461-e4edd32e51c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d9dd391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain(model,x_train,y_train,x_test,y_test):\n",
    "    \"\"\"This function is used to fine-tune the pruned network \n",
    "    using the same method as the original network training.\"\"\"\n",
    "    total_steps = epochs * (x_train.shape[0] // batch_size)\n",
    "    warmup_steps = warmup_epochs * (x_train.shape[0] // batch_size)\n",
    "    lr_schedule = WarmUpCosine(initial_lr, total_steps, warmup_steps)\n",
    "    optimizer = CustomWeightDecaySGD(weight_decay=weight_decay,learning_rate=lr_schedule,momentum=0.9,nesterov=True)\n",
    "    loss_fn=tf.keras.losses.CategoricalCrossentropy()\n",
    "    model.compile(optimizer=optimizer,loss=loss_fn,metrics=['accuracy'])\n",
    "    saver = LastNSaver(n=20)\n",
    "    model.fit(datagen.flow(x_train, y_train_onehot,batch_size=batch_size),\n",
    "                            steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=(x_test, y_test_onehot),verbose=2,callbacks=[saver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34c12c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"These functions are used to calculate the FLOPs \n",
    "and the number of parameters of the network.\"\"\"\n",
    "def conv_flops_params(layer, input_shape):\n",
    "    h_in, w_in, cin = input_shape[1:]\n",
    "    h_out, w_out, cout = layer.output_shape[1:]\n",
    "    k_h, k_w = layer.kernel_size\n",
    "    flops = h_out * w_out * cin * cout * k_h * k_w\n",
    "    params = cin * cout * k_h * k_w\n",
    "    if layer.use_bias:\n",
    "        params += cout\n",
    "    return flops, params, (h_out, w_out, cout)\n",
    "def dense_flops_params(layer, input_shape):\n",
    "    cin = input_shape[-1]\n",
    "    cout = layer.units\n",
    "    flops = cin * cout\n",
    "    params = cin * cout\n",
    "    if layer.use_bias:\n",
    "        params += cout\n",
    "    return flops, params, (cout,)\n",
    "def compute_flops_params(model, input_shape=(32, 32, 3)):\n",
    "    total_flops = 0\n",
    "    total_params = 0\n",
    "    dummy_input = tf.zeros((1, *input_shape))\n",
    "    _ = model(dummy_input)\n",
    "    current_shape = (1, *input_shape)\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            flops, params, out_shape = conv_flops_params(layer, current_shape)\n",
    "            total_flops += flops\n",
    "            total_params += params\n",
    "            current_shape = (1, *out_shape)\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            flops, params, out_shape = dense_flops_params(layer, current_shape)\n",
    "            total_flops += flops\n",
    "            total_params += params\n",
    "            current_shape = (1, *out_shape)\n",
    "    return total_flops, total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c45c7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_layers(model,x,y,R=P[1:]):\n",
    "    \"\"\"This function is used to obtain the structural redundancy \n",
    "    criterion of each block in the ResNet-18 network.\"\"\"\n",
    "    layer_outputs =[layer.output for layer in model.layers] \n",
    "    R_L=[]\n",
    "    channel_label=[]\n",
    "    for i in range(len(R)):\n",
    "        print('start')\n",
    "        activation_model = tf.keras.models.Model(inputs=model.input,outputs=layer_outputs[R[i]])\n",
    "        x_L=activation_model.predict(x)\n",
    "        channel_new_label,r_L=prune_channel([x_L],y,0,nnn=15)\n",
    "        print('finish')\n",
    "        r_L=float(r_L)\n",
    "        R_L.append(r_L)\n",
    "        print(r_L)\n",
    "    R_L=np.array(R_L)\n",
    "    R=np.mean(R_L)\n",
    "    LLL=[1,2,2,2,2,2,2,2,2]\n",
    "    RR_L=[]\n",
    "    iii=0\n",
    "    for k in range(len(LLL)):\n",
    "        print(R_L)\n",
    "        if LLL[k]==1:\n",
    "            RR_L.append(R_L[0])\n",
    "        if LLL[k]==2:\n",
    "            RR_L.append(R_L[iii:iii+2].sum()/2)\n",
    "        iii+=LLL[k]\n",
    "    return R,RR_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "590d84b3-a696-4d89-aea1-bfbc1f329efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_G(model):\n",
    "    \"\"\"This function is used to obtain the number of channels of the layer group.\"\"\"\n",
    "    C=[]\n",
    "    for i in range(len(G)):\n",
    "        CG=0\n",
    "        for g in G[i]:\n",
    "            a,b,d,c=model.layers[g].output.shape\n",
    "            CG+=c\n",
    "        C.append(CG)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d459fc8f-bf7c-4d52-9d2e-40a725de1a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 7ms/step - loss: 0.1839 - accuracy: 0.9439\n",
      "561714176\n"
     ]
    }
   ],
   "source": [
    "P_list=[]\n",
    "E_list=[]\n",
    "F_list=[]\n",
    "#RP_list=[]\n",
    "#RRP_list=[]\n",
    "C_list=[]\n",
    "flops,par=compute_flops_params(model)\n",
    "loss, acc = model.evaluate(x_test, y_test_onehot)\n",
    "C_0=channel_G(model)\n",
    "print(flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fc8a916-f843-4b9f-9adf-9455099b9c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_FILE = \"training_ResNet18_log.json\"\n",
    "def load_progress():\n",
    "    if os.path.exists(SAVE_FILE):\n",
    "        with open(SAVE_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"results\": [], \n",
    "            \"RR_L\": [],\n",
    "            \"P_list\": [],\n",
    "            \"E_list\": [],\n",
    "            \"F_list\": [],\n",
    "            \"C_list\": [],\n",
    "            \"last_lam_idx\": 0,\n",
    "            \"last_repeat\": 0,\n",
    "            \"RL_exist\": 0,\n",
    "            \"Cri_exist\": 0}\n",
    "def save_progress(progress):\n",
    "    with open(SAVE_FILE, \"w\") as f:\n",
    "        json.dump(progress, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08fb37cb-7102-4473-a87d-3bf39385a061",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "progress = load_progress()\n",
    "start_lr_idx = progress[\"last_lam_idx\"]\n",
    "start_repeat = progress[\"last_repeat\"]\n",
    "If_RL = progress[\"RL_exist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10fa0dde-dee4-4a37-b2fc-e6a3cc6f0071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if If_RL == 0:\n",
    "    model=load_Res()\n",
    "    R_L,RR_L=R_layers(model,x_dist,y_dist)\n",
    "    progress[\"RR_L\"].append(RR_L)\n",
    "    progress[\"RL_exist\"] = 1\n",
    "    save_progress(progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f466d331-c2c1-48b4-9cea-7d5ee2275d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03125, -0.03125, 0.296875, 0.2421875, 0.1953125, 0.15625, 0.1015625, 0.146484375, 0.591796875]]\n"
     ]
    }
   ],
   "source": [
    "print(progress[\"RR_L\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "292b2d60-8e8a-46d3-a364-db471457e613",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77fb5c44-9329-40c2-9b35-2dae5904eaee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " lambda: Lam=1.0, Repeat=2/3\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "132506468 561714176\n",
      "Epoch 1/200\n",
      "390/390 - 25s - loss: 1.0519 - accuracy: 0.6380 - val_loss: 3.0781 - val_accuracy: 0.2232 - 25s/epoch - 64ms/step\n",
      "Epoch 2/200\n",
      "390/390 - 22s - loss: 0.7249 - accuracy: 0.7511 - val_loss: 2.2710 - val_accuracy: 0.4803 - 22s/epoch - 55ms/step\n",
      "Epoch 3/200\n",
      "390/390 - 21s - loss: 0.6209 - accuracy: 0.7882 - val_loss: 1.1878 - val_accuracy: 0.6627 - 21s/epoch - 54ms/step\n",
      "Epoch 4/200\n",
      "390/390 - 21s - loss: 0.5594 - accuracy: 0.8075 - val_loss: 0.9046 - val_accuracy: 0.7076 - 21s/epoch - 54ms/step\n",
      "Epoch 5/200\n",
      "390/390 - 20s - loss: 0.5123 - accuracy: 0.8228 - val_loss: 1.4879 - val_accuracy: 0.5927 - 20s/epoch - 53ms/step\n",
      "Epoch 6/200\n",
      "390/390 - 22s - loss: 0.4674 - accuracy: 0.8401 - val_loss: 1.2169 - val_accuracy: 0.6743 - 22s/epoch - 57ms/step\n",
      "Epoch 7/200\n",
      "390/390 - 22s - loss: 0.4149 - accuracy: 0.8581 - val_loss: 0.8488 - val_accuracy: 0.7497 - 22s/epoch - 56ms/step\n",
      "Epoch 8/200\n",
      "390/390 - 22s - loss: 0.3818 - accuracy: 0.8686 - val_loss: 0.5495 - val_accuracy: 0.8278 - 22s/epoch - 55ms/step\n",
      "Epoch 9/200\n",
      "390/390 - 22s - loss: 0.3598 - accuracy: 0.8767 - val_loss: 0.5551 - val_accuracy: 0.8185 - 22s/epoch - 55ms/step\n",
      "Epoch 10/200\n",
      "390/390 - 21s - loss: 0.3356 - accuracy: 0.8835 - val_loss: 0.4888 - val_accuracy: 0.8348 - 21s/epoch - 53ms/step\n",
      "Epoch 11/200\n",
      "390/390 - 21s - loss: 0.3239 - accuracy: 0.8885 - val_loss: 0.4080 - val_accuracy: 0.8600 - 21s/epoch - 54ms/step\n",
      "Epoch 12/200\n",
      "390/390 - 21s - loss: 0.3093 - accuracy: 0.8932 - val_loss: 0.4653 - val_accuracy: 0.8454 - 21s/epoch - 54ms/step\n",
      "Epoch 13/200\n",
      "390/390 - 20s - loss: 0.2964 - accuracy: 0.8976 - val_loss: 0.8618 - val_accuracy: 0.7502 - 20s/epoch - 52ms/step\n",
      "Epoch 14/200\n",
      "390/390 - 20s - loss: 0.2827 - accuracy: 0.9023 - val_loss: 0.4274 - val_accuracy: 0.8645 - 20s/epoch - 52ms/step\n",
      "Epoch 15/200\n",
      "390/390 - 20s - loss: 0.2735 - accuracy: 0.9052 - val_loss: 0.4521 - val_accuracy: 0.8583 - 20s/epoch - 53ms/step\n",
      "Epoch 16/200\n",
      "390/390 - 22s - loss: 0.2638 - accuracy: 0.9073 - val_loss: 0.5414 - val_accuracy: 0.8347 - 22s/epoch - 57ms/step\n",
      "Epoch 17/200\n",
      "390/390 - 28s - loss: 0.2546 - accuracy: 0.9116 - val_loss: 0.4816 - val_accuracy: 0.8532 - 28s/epoch - 72ms/step\n",
      "Epoch 18/200\n",
      "390/390 - 22s - loss: 0.2488 - accuracy: 0.9134 - val_loss: 0.4058 - val_accuracy: 0.8715 - 22s/epoch - 55ms/step\n",
      "Epoch 19/200\n",
      "390/390 - 22s - loss: 0.2424 - accuracy: 0.9151 - val_loss: 0.5113 - val_accuracy: 0.8436 - 22s/epoch - 56ms/step\n",
      "Epoch 20/200\n",
      "390/390 - 20s - loss: 0.2384 - accuracy: 0.9164 - val_loss: 0.3874 - val_accuracy: 0.8765 - 20s/epoch - 52ms/step\n",
      "Epoch 21/200\n",
      "390/390 - 20s - loss: 0.2283 - accuracy: 0.9198 - val_loss: 0.4608 - val_accuracy: 0.8525 - 20s/epoch - 52ms/step\n",
      "Epoch 22/200\n",
      "390/390 - 20s - loss: 0.2246 - accuracy: 0.9203 - val_loss: 0.3893 - val_accuracy: 0.8772 - 20s/epoch - 52ms/step\n",
      "Epoch 23/200\n",
      "390/390 - 21s - loss: 0.2186 - accuracy: 0.9236 - val_loss: 0.3691 - val_accuracy: 0.8875 - 21s/epoch - 54ms/step\n",
      "Epoch 24/200\n",
      "390/390 - 22s - loss: 0.2146 - accuracy: 0.9251 - val_loss: 0.3953 - val_accuracy: 0.8715 - 22s/epoch - 56ms/step\n",
      "Epoch 25/200\n",
      "390/390 - 20s - loss: 0.2153 - accuracy: 0.9240 - val_loss: 0.4967 - val_accuracy: 0.8418 - 20s/epoch - 52ms/step\n",
      "Epoch 26/200\n",
      "390/390 - 20s - loss: 0.2108 - accuracy: 0.9274 - val_loss: 0.4870 - val_accuracy: 0.8499 - 20s/epoch - 52ms/step\n",
      "Epoch 27/200\n",
      "390/390 - 20s - loss: 0.2021 - accuracy: 0.9289 - val_loss: 0.4282 - val_accuracy: 0.8571 - 20s/epoch - 52ms/step\n",
      "Epoch 28/200\n",
      "390/390 - 21s - loss: 0.2025 - accuracy: 0.9284 - val_loss: 0.3374 - val_accuracy: 0.8901 - 21s/epoch - 55ms/step\n",
      "Epoch 29/200\n",
      "390/390 - 20s - loss: 0.1953 - accuracy: 0.9321 - val_loss: 0.3705 - val_accuracy: 0.8863 - 20s/epoch - 52ms/step\n",
      "Epoch 30/200\n",
      "390/390 - 20s - loss: 0.1963 - accuracy: 0.9299 - val_loss: 0.5092 - val_accuracy: 0.8462 - 20s/epoch - 52ms/step\n",
      "Epoch 31/200\n",
      "390/390 - 21s - loss: 0.1942 - accuracy: 0.9311 - val_loss: 0.4879 - val_accuracy: 0.8556 - 21s/epoch - 55ms/step\n",
      "Epoch 32/200\n",
      "390/390 - 20s - loss: 0.1907 - accuracy: 0.9334 - val_loss: 0.4787 - val_accuracy: 0.8561 - 20s/epoch - 52ms/step\n",
      "Epoch 33/200\n",
      "390/390 - 20s - loss: 0.1870 - accuracy: 0.9347 - val_loss: 0.4451 - val_accuracy: 0.8611 - 20s/epoch - 52ms/step\n",
      "Epoch 34/200\n",
      "390/390 - 20s - loss: 0.1861 - accuracy: 0.9347 - val_loss: 0.3652 - val_accuracy: 0.8855 - 20s/epoch - 53ms/step\n",
      "Epoch 35/200\n",
      "390/390 - 21s - loss: 0.1824 - accuracy: 0.9355 - val_loss: 0.3657 - val_accuracy: 0.8845 - 21s/epoch - 54ms/step\n",
      "Epoch 36/200\n",
      "390/390 - 21s - loss: 0.1838 - accuracy: 0.9351 - val_loss: 0.4788 - val_accuracy: 0.8659 - 21s/epoch - 54ms/step\n",
      "Epoch 37/200\n",
      "390/390 - 21s - loss: 0.1795 - accuracy: 0.9356 - val_loss: 0.4503 - val_accuracy: 0.8675 - 21s/epoch - 53ms/step\n",
      "Epoch 38/200\n",
      "390/390 - 21s - loss: 0.1734 - accuracy: 0.9392 - val_loss: 0.4733 - val_accuracy: 0.8653 - 21s/epoch - 54ms/step\n",
      "Epoch 39/200\n",
      "390/390 - 22s - loss: 0.1745 - accuracy: 0.9394 - val_loss: 0.4078 - val_accuracy: 0.8749 - 22s/epoch - 56ms/step\n",
      "Epoch 40/200\n",
      "390/390 - 21s - loss: 0.1714 - accuracy: 0.9404 - val_loss: 0.5135 - val_accuracy: 0.8510 - 21s/epoch - 54ms/step\n",
      "Epoch 41/200\n",
      "390/390 - 21s - loss: 0.1660 - accuracy: 0.9413 - val_loss: 0.5378 - val_accuracy: 0.8455 - 21s/epoch - 54ms/step\n",
      "Epoch 42/200\n",
      "390/390 - 20s - loss: 0.1667 - accuracy: 0.9412 - val_loss: 0.3859 - val_accuracy: 0.8823 - 20s/epoch - 52ms/step\n",
      "Epoch 43/200\n",
      "390/390 - 20s - loss: 0.1658 - accuracy: 0.9415 - val_loss: 0.3601 - val_accuracy: 0.8883 - 20s/epoch - 52ms/step\n",
      "Epoch 44/200\n",
      "390/390 - 20s - loss: 0.1650 - accuracy: 0.9430 - val_loss: 0.4852 - val_accuracy: 0.8563 - 20s/epoch - 52ms/step\n",
      "Epoch 45/200\n",
      "390/390 - 22s - loss: 0.1681 - accuracy: 0.9418 - val_loss: 0.5436 - val_accuracy: 0.8397 - 22s/epoch - 56ms/step\n",
      "Epoch 46/200\n",
      "390/390 - 20s - loss: 0.1605 - accuracy: 0.9426 - val_loss: 0.3922 - val_accuracy: 0.8826 - 20s/epoch - 52ms/step\n",
      "Epoch 47/200\n",
      "390/390 - 21s - loss: 0.1641 - accuracy: 0.9421 - val_loss: 0.3798 - val_accuracy: 0.8843 - 21s/epoch - 54ms/step\n",
      "Epoch 48/200\n",
      "390/390 - 21s - loss: 0.1595 - accuracy: 0.9438 - val_loss: 0.4584 - val_accuracy: 0.8618 - 21s/epoch - 54ms/step\n",
      "Epoch 49/200\n",
      "390/390 - 20s - loss: 0.1616 - accuracy: 0.9442 - val_loss: 0.4577 - val_accuracy: 0.8598 - 20s/epoch - 52ms/step\n",
      "Epoch 50/200\n",
      "390/390 - 21s - loss: 0.1546 - accuracy: 0.9452 - val_loss: 0.4138 - val_accuracy: 0.8813 - 21s/epoch - 54ms/step\n",
      "Epoch 51/200\n",
      "390/390 - 21s - loss: 0.1538 - accuracy: 0.9473 - val_loss: 0.4519 - val_accuracy: 0.8687 - 21s/epoch - 55ms/step\n",
      "Epoch 52/200\n",
      "390/390 - 22s - loss: 0.1539 - accuracy: 0.9458 - val_loss: 0.4025 - val_accuracy: 0.8747 - 22s/epoch - 56ms/step\n",
      "Epoch 53/200\n",
      "390/390 - 22s - loss: 0.1564 - accuracy: 0.9460 - val_loss: 0.3691 - val_accuracy: 0.8861 - 22s/epoch - 56ms/step\n",
      "Epoch 54/200\n",
      "390/390 - 22s - loss: 0.1501 - accuracy: 0.9474 - val_loss: 0.4327 - val_accuracy: 0.8766 - 22s/epoch - 55ms/step\n",
      "Epoch 55/200\n",
      "390/390 - 20s - loss: 0.1524 - accuracy: 0.9477 - val_loss: 0.4260 - val_accuracy: 0.8759 - 20s/epoch - 52ms/step\n",
      "Epoch 56/200\n",
      "390/390 - 20s - loss: 0.1546 - accuracy: 0.9453 - val_loss: 0.4661 - val_accuracy: 0.8657 - 20s/epoch - 52ms/step\n",
      "Epoch 57/200\n",
      "390/390 - 20s - loss: 0.1493 - accuracy: 0.9482 - val_loss: 0.6299 - val_accuracy: 0.8356 - 20s/epoch - 52ms/step\n",
      "Epoch 58/200\n",
      "390/390 - 21s - loss: 0.1489 - accuracy: 0.9480 - val_loss: 0.5196 - val_accuracy: 0.8580 - 21s/epoch - 54ms/step\n",
      "Epoch 59/200\n",
      "390/390 - 21s - loss: 0.1456 - accuracy: 0.9487 - val_loss: 0.3641 - val_accuracy: 0.8901 - 21s/epoch - 55ms/step\n",
      "Epoch 60/200\n",
      "390/390 - 21s - loss: 0.1453 - accuracy: 0.9491 - val_loss: 0.4364 - val_accuracy: 0.8740 - 21s/epoch - 55ms/step\n",
      "Epoch 61/200\n",
      "390/390 - 22s - loss: 0.1457 - accuracy: 0.9493 - val_loss: 0.3939 - val_accuracy: 0.8828 - 22s/epoch - 56ms/step\n",
      "Epoch 62/200\n",
      "390/390 - 20s - loss: 0.1409 - accuracy: 0.9503 - val_loss: 0.4251 - val_accuracy: 0.8722 - 20s/epoch - 53ms/step\n",
      "Epoch 63/200\n",
      "390/390 - 22s - loss: 0.1449 - accuracy: 0.9488 - val_loss: 0.3908 - val_accuracy: 0.8829 - 22s/epoch - 55ms/step\n",
      "Epoch 64/200\n",
      "390/390 - 21s - loss: 0.1367 - accuracy: 0.9524 - val_loss: 0.4322 - val_accuracy: 0.8686 - 21s/epoch - 54ms/step\n",
      "Epoch 65/200\n",
      "390/390 - 20s - loss: 0.1433 - accuracy: 0.9504 - val_loss: 0.3871 - val_accuracy: 0.8829 - 20s/epoch - 52ms/step\n",
      "Epoch 66/200\n",
      "390/390 - 20s - loss: 0.1403 - accuracy: 0.9498 - val_loss: 0.3923 - val_accuracy: 0.8813 - 20s/epoch - 52ms/step\n",
      "Epoch 67/200\n",
      "390/390 - 21s - loss: 0.1373 - accuracy: 0.9515 - val_loss: 0.4187 - val_accuracy: 0.8714 - 21s/epoch - 54ms/step\n",
      "Epoch 68/200\n",
      "390/390 - 22s - loss: 0.1416 - accuracy: 0.9506 - val_loss: 0.4176 - val_accuracy: 0.8774 - 22s/epoch - 56ms/step\n",
      "Epoch 69/200\n",
      "390/390 - 21s - loss: 0.1399 - accuracy: 0.9511 - val_loss: 0.5545 - val_accuracy: 0.8477 - 21s/epoch - 54ms/step\n",
      "Epoch 70/200\n",
      "390/390 - 22s - loss: 0.1313 - accuracy: 0.9524 - val_loss: 0.4133 - val_accuracy: 0.8808 - 22s/epoch - 56ms/step\n",
      "Epoch 71/200\n",
      "390/390 - 21s - loss: 0.1341 - accuracy: 0.9533 - val_loss: 0.4617 - val_accuracy: 0.8706 - 21s/epoch - 54ms/step\n",
      "Epoch 72/200\n",
      "390/390 - 21s - loss: 0.1373 - accuracy: 0.9521 - val_loss: 0.4167 - val_accuracy: 0.8781 - 21s/epoch - 54ms/step\n",
      "Epoch 73/200\n",
      "390/390 - 21s - loss: 0.1371 - accuracy: 0.9512 - val_loss: 0.3316 - val_accuracy: 0.8959 - 21s/epoch - 54ms/step\n",
      "Epoch 74/200\n",
      "390/390 - 20s - loss: 0.1319 - accuracy: 0.9537 - val_loss: 0.3589 - val_accuracy: 0.8926 - 20s/epoch - 52ms/step\n",
      "Epoch 75/200\n",
      "390/390 - 20s - loss: 0.1331 - accuracy: 0.9541 - val_loss: 0.4307 - val_accuracy: 0.8782 - 20s/epoch - 52ms/step\n",
      "Epoch 76/200\n",
      "390/390 - 21s - loss: 0.1311 - accuracy: 0.9544 - val_loss: 0.3302 - val_accuracy: 0.9026 - 21s/epoch - 55ms/step\n",
      "Epoch 77/200\n",
      "390/390 - 21s - loss: 0.1327 - accuracy: 0.9526 - val_loss: 0.4406 - val_accuracy: 0.8747 - 21s/epoch - 55ms/step\n",
      "Epoch 78/200\n",
      "390/390 - 21s - loss: 0.1313 - accuracy: 0.9530 - val_loss: 0.6075 - val_accuracy: 0.8350 - 21s/epoch - 54ms/step\n",
      "Epoch 79/200\n",
      "390/390 - 21s - loss: 0.1299 - accuracy: 0.9549 - val_loss: 0.4865 - val_accuracy: 0.8683 - 21s/epoch - 54ms/step\n",
      "Epoch 80/200\n",
      "390/390 - 21s - loss: 0.1271 - accuracy: 0.9551 - val_loss: 0.4682 - val_accuracy: 0.8678 - 21s/epoch - 55ms/step\n",
      "Epoch 81/200\n",
      "390/390 - 21s - loss: 0.1301 - accuracy: 0.9534 - val_loss: 0.3909 - val_accuracy: 0.8800 - 21s/epoch - 54ms/step\n",
      "Epoch 82/200\n",
      "390/390 - 20s - loss: 0.1295 - accuracy: 0.9555 - val_loss: 0.4165 - val_accuracy: 0.8805 - 20s/epoch - 52ms/step\n",
      "Epoch 83/200\n",
      "390/390 - 21s - loss: 0.1294 - accuracy: 0.9545 - val_loss: 0.4225 - val_accuracy: 0.8771 - 21s/epoch - 54ms/step\n",
      "Epoch 84/200\n",
      "390/390 - 21s - loss: 0.1229 - accuracy: 0.9564 - val_loss: 0.4163 - val_accuracy: 0.8748 - 21s/epoch - 54ms/step\n",
      "Epoch 85/200\n",
      "390/390 - 20s - loss: 0.1276 - accuracy: 0.9557 - val_loss: 0.5828 - val_accuracy: 0.8446 - 20s/epoch - 52ms/step\n",
      "Epoch 86/200\n",
      "390/390 - 21s - loss: 0.1263 - accuracy: 0.9569 - val_loss: 0.4003 - val_accuracy: 0.8859 - 21s/epoch - 55ms/step\n",
      "Epoch 87/200\n",
      "390/390 - 22s - loss: 0.1217 - accuracy: 0.9576 - val_loss: 0.4585 - val_accuracy: 0.8752 - 22s/epoch - 56ms/step\n",
      "Epoch 88/200\n",
      "390/390 - 21s - loss: 0.1232 - accuracy: 0.9566 - val_loss: 0.3739 - val_accuracy: 0.8908 - 21s/epoch - 53ms/step\n",
      "Epoch 89/200\n",
      "390/390 - 21s - loss: 0.1218 - accuracy: 0.9577 - val_loss: 0.4197 - val_accuracy: 0.8820 - 21s/epoch - 54ms/step\n",
      "Epoch 90/200\n",
      "390/390 - 21s - loss: 0.1219 - accuracy: 0.9577 - val_loss: 0.3735 - val_accuracy: 0.8937 - 21s/epoch - 55ms/step\n",
      "Epoch 91/200\n",
      "390/390 - 22s - loss: 0.1222 - accuracy: 0.9572 - val_loss: 0.4332 - val_accuracy: 0.8769 - 22s/epoch - 57ms/step\n",
      "Epoch 92/200\n",
      "390/390 - 20s - loss: 0.1209 - accuracy: 0.9586 - val_loss: 0.3707 - val_accuracy: 0.8904 - 20s/epoch - 52ms/step\n",
      "Epoch 93/200\n",
      "390/390 - 21s - loss: 0.1170 - accuracy: 0.9593 - val_loss: 0.4037 - val_accuracy: 0.8851 - 21s/epoch - 55ms/step\n",
      "Epoch 94/200\n",
      "390/390 - 22s - loss: 0.1239 - accuracy: 0.9575 - val_loss: 0.3934 - val_accuracy: 0.8861 - 22s/epoch - 57ms/step\n",
      "Epoch 95/200\n",
      "390/390 - 22s - loss: 0.1178 - accuracy: 0.9587 - val_loss: 0.4056 - val_accuracy: 0.8830 - 22s/epoch - 56ms/step\n",
      "Epoch 96/200\n",
      "390/390 - 21s - loss: 0.1153 - accuracy: 0.9595 - val_loss: 0.3687 - val_accuracy: 0.8929 - 21s/epoch - 54ms/step\n",
      "Epoch 97/200\n",
      "390/390 - 21s - loss: 0.1224 - accuracy: 0.9572 - val_loss: 0.3939 - val_accuracy: 0.8841 - 21s/epoch - 54ms/step\n",
      "Epoch 98/200\n",
      "390/390 - 21s - loss: 0.1160 - accuracy: 0.9594 - val_loss: 0.4149 - val_accuracy: 0.8797 - 21s/epoch - 55ms/step\n",
      "Epoch 99/200\n",
      "390/390 - 20s - loss: 0.1201 - accuracy: 0.9580 - val_loss: 0.4167 - val_accuracy: 0.8833 - 20s/epoch - 52ms/step\n",
      "Epoch 100/200\n",
      "390/390 - 20s - loss: 0.1172 - accuracy: 0.9583 - val_loss: 0.3893 - val_accuracy: 0.8904 - 20s/epoch - 52ms/step\n",
      "Epoch 101/200\n",
      "390/390 - 21s - loss: 0.1119 - accuracy: 0.9604 - val_loss: 0.4640 - val_accuracy: 0.8723 - 21s/epoch - 55ms/step\n",
      "Epoch 102/200\n",
      "390/390 - 20s - loss: 0.1172 - accuracy: 0.9583 - val_loss: 0.3996 - val_accuracy: 0.8822 - 20s/epoch - 52ms/step\n",
      "Epoch 103/200\n",
      "390/390 - 22s - loss: 0.1144 - accuracy: 0.9603 - val_loss: 0.4020 - val_accuracy: 0.8866 - 22s/epoch - 56ms/step\n",
      "Epoch 104/200\n",
      "390/390 - 20s - loss: 0.1125 - accuracy: 0.9601 - val_loss: 0.4716 - val_accuracy: 0.8684 - 20s/epoch - 52ms/step\n",
      "Epoch 105/200\n",
      "390/390 - 22s - loss: 0.1134 - accuracy: 0.9597 - val_loss: 0.6013 - val_accuracy: 0.8481 - 22s/epoch - 55ms/step\n",
      "Epoch 106/200\n",
      "390/390 - 22s - loss: 0.1148 - accuracy: 0.9605 - val_loss: 0.3719 - val_accuracy: 0.8941 - 22s/epoch - 55ms/step\n",
      "Epoch 107/200\n",
      "390/390 - 21s - loss: 0.1098 - accuracy: 0.9615 - val_loss: 0.3919 - val_accuracy: 0.8886 - 21s/epoch - 54ms/step\n",
      "Epoch 108/200\n",
      "390/390 - 20s - loss: 0.1123 - accuracy: 0.9610 - val_loss: 0.3662 - val_accuracy: 0.8928 - 20s/epoch - 52ms/step\n",
      "Epoch 109/200\n",
      "390/390 - 21s - loss: 0.1083 - accuracy: 0.9630 - val_loss: 0.3996 - val_accuracy: 0.8833 - 21s/epoch - 54ms/step\n",
      "Epoch 110/200\n",
      "390/390 - 21s - loss: 0.1104 - accuracy: 0.9613 - val_loss: 0.3880 - val_accuracy: 0.8939 - 21s/epoch - 55ms/step\n",
      "Epoch 111/200\n",
      "390/390 - 20s - loss: 0.1107 - accuracy: 0.9617 - val_loss: 0.3229 - val_accuracy: 0.9052 - 20s/epoch - 53ms/step\n",
      "Epoch 112/200\n",
      "390/390 - 21s - loss: 0.1073 - accuracy: 0.9627 - val_loss: 0.4359 - val_accuracy: 0.8795 - 21s/epoch - 54ms/step\n",
      "Epoch 113/200\n",
      "390/390 - 22s - loss: 0.1066 - accuracy: 0.9627 - val_loss: 0.3634 - val_accuracy: 0.8971 - 22s/epoch - 57ms/step\n",
      "Epoch 114/200\n",
      "390/390 - 20s - loss: 0.1053 - accuracy: 0.9630 - val_loss: 0.4117 - val_accuracy: 0.8819 - 20s/epoch - 52ms/step\n",
      "Epoch 115/200\n",
      "390/390 - 21s - loss: 0.1021 - accuracy: 0.9644 - val_loss: 0.4383 - val_accuracy: 0.8775 - 21s/epoch - 54ms/step\n",
      "Epoch 116/200\n",
      "390/390 - 21s - loss: 0.1070 - accuracy: 0.9634 - val_loss: 0.4152 - val_accuracy: 0.8864 - 21s/epoch - 54ms/step\n",
      "Epoch 117/200\n",
      "390/390 - 21s - loss: 0.1030 - accuracy: 0.9640 - val_loss: 0.3279 - val_accuracy: 0.9045 - 21s/epoch - 55ms/step\n",
      "Epoch 118/200\n",
      "390/390 - 22s - loss: 0.1049 - accuracy: 0.9625 - val_loss: 0.5234 - val_accuracy: 0.8596 - 22s/epoch - 57ms/step\n",
      "Epoch 119/200\n",
      "390/390 - 21s - loss: 0.1035 - accuracy: 0.9636 - val_loss: 0.4343 - val_accuracy: 0.8772 - 21s/epoch - 54ms/step\n",
      "Epoch 120/200\n",
      "390/390 - 20s - loss: 0.1053 - accuracy: 0.9622 - val_loss: 0.4780 - val_accuracy: 0.8724 - 20s/epoch - 52ms/step\n",
      "Epoch 121/200\n",
      "390/390 - 20s - loss: 0.1003 - accuracy: 0.9649 - val_loss: 0.4107 - val_accuracy: 0.8908 - 20s/epoch - 52ms/step\n",
      "Epoch 122/200\n",
      "390/390 - 20s - loss: 0.1030 - accuracy: 0.9647 - val_loss: 0.3609 - val_accuracy: 0.8928 - 20s/epoch - 52ms/step\n",
      "Epoch 123/200\n",
      "390/390 - 21s - loss: 0.1000 - accuracy: 0.9647 - val_loss: 0.3557 - val_accuracy: 0.8971 - 21s/epoch - 55ms/step\n",
      "Epoch 124/200\n",
      "390/390 - 20s - loss: 0.0996 - accuracy: 0.9649 - val_loss: 0.3586 - val_accuracy: 0.8960 - 20s/epoch - 52ms/step\n",
      "Epoch 125/200\n",
      "390/390 - 21s - loss: 0.0998 - accuracy: 0.9649 - val_loss: 0.3735 - val_accuracy: 0.8893 - 21s/epoch - 55ms/step\n",
      "Epoch 126/200\n",
      "390/390 - 22s - loss: 0.1023 - accuracy: 0.9641 - val_loss: 0.4923 - val_accuracy: 0.8674 - 22s/epoch - 56ms/step\n",
      "Epoch 127/200\n",
      "390/390 - 20s - loss: 0.0990 - accuracy: 0.9653 - val_loss: 0.3489 - val_accuracy: 0.8987 - 20s/epoch - 52ms/step\n",
      "Epoch 128/200\n",
      "390/390 - 21s - loss: 0.0952 - accuracy: 0.9667 - val_loss: 0.3641 - val_accuracy: 0.8952 - 21s/epoch - 54ms/step\n",
      "Epoch 129/200\n",
      "390/390 - 21s - loss: 0.0976 - accuracy: 0.9657 - val_loss: 0.3726 - val_accuracy: 0.8960 - 21s/epoch - 54ms/step\n",
      "Epoch 130/200\n",
      "390/390 - 20s - loss: 0.0951 - accuracy: 0.9670 - val_loss: 0.3668 - val_accuracy: 0.8923 - 20s/epoch - 52ms/step\n",
      "Epoch 131/200\n",
      "390/390 - 22s - loss: 0.0983 - accuracy: 0.9656 - val_loss: 0.3724 - val_accuracy: 0.8981 - 22s/epoch - 57ms/step\n",
      "Epoch 132/200\n",
      "390/390 - 21s - loss: 0.0956 - accuracy: 0.9667 - val_loss: 0.3691 - val_accuracy: 0.8978 - 21s/epoch - 54ms/step\n",
      "Epoch 133/200\n",
      "390/390 - 20s - loss: 0.0949 - accuracy: 0.9671 - val_loss: 0.3826 - val_accuracy: 0.8909 - 20s/epoch - 52ms/step\n",
      "Epoch 134/200\n",
      "390/390 - 21s - loss: 0.0974 - accuracy: 0.9659 - val_loss: 0.3380 - val_accuracy: 0.8990 - 21s/epoch - 55ms/step\n",
      "Epoch 135/200\n",
      "390/390 - 20s - loss: 0.0897 - accuracy: 0.9687 - val_loss: 0.3738 - val_accuracy: 0.8949 - 20s/epoch - 52ms/step\n",
      "Epoch 136/200\n",
      "390/390 - 21s - loss: 0.0871 - accuracy: 0.9690 - val_loss: 0.3388 - val_accuracy: 0.9033 - 21s/epoch - 55ms/step\n",
      "Epoch 137/200\n",
      "390/390 - 20s - loss: 0.0893 - accuracy: 0.9692 - val_loss: 0.3632 - val_accuracy: 0.8963 - 20s/epoch - 52ms/step\n",
      "Epoch 138/200\n",
      "390/390 - 22s - loss: 0.0917 - accuracy: 0.9680 - val_loss: 0.4838 - val_accuracy: 0.8723 - 22s/epoch - 55ms/step\n",
      "Epoch 139/200\n",
      "390/390 - 22s - loss: 0.0891 - accuracy: 0.9691 - val_loss: 0.3516 - val_accuracy: 0.9006 - 22s/epoch - 56ms/step\n",
      "Epoch 140/200\n",
      "390/390 - 20s - loss: 0.0889 - accuracy: 0.9689 - val_loss: 0.3648 - val_accuracy: 0.8975 - 20s/epoch - 52ms/step\n",
      "Epoch 141/200\n",
      "390/390 - 21s - loss: 0.0901 - accuracy: 0.9689 - val_loss: 0.3732 - val_accuracy: 0.8928 - 21s/epoch - 54ms/step\n",
      "Epoch 142/200\n",
      "390/390 - 22s - loss: 0.0788 - accuracy: 0.9729 - val_loss: 0.3641 - val_accuracy: 0.8972 - 22s/epoch - 55ms/step\n",
      "Epoch 143/200\n",
      "390/390 - 22s - loss: 0.0862 - accuracy: 0.9704 - val_loss: 0.5798 - val_accuracy: 0.8426 - 22s/epoch - 56ms/step\n",
      "Epoch 144/200\n",
      "390/390 - 20s - loss: 0.0823 - accuracy: 0.9715 - val_loss: 0.4898 - val_accuracy: 0.8681 - 20s/epoch - 52ms/step\n",
      "Epoch 145/200\n",
      "390/390 - 21s - loss: 0.0855 - accuracy: 0.9700 - val_loss: 0.4035 - val_accuracy: 0.8880 - 21s/epoch - 54ms/step\n",
      "Epoch 146/200\n",
      "390/390 - 21s - loss: 0.0814 - accuracy: 0.9713 - val_loss: 0.3498 - val_accuracy: 0.9006 - 21s/epoch - 54ms/step\n",
      "Epoch 147/200\n",
      "390/390 - 20s - loss: 0.0813 - accuracy: 0.9720 - val_loss: 0.3897 - val_accuracy: 0.8951 - 20s/epoch - 52ms/step\n",
      "Epoch 148/200\n",
      "390/390 - 21s - loss: 0.0827 - accuracy: 0.9711 - val_loss: 0.3108 - val_accuracy: 0.9130 - 21s/epoch - 53ms/step\n",
      "Epoch 149/200\n",
      "390/390 - 21s - loss: 0.0780 - accuracy: 0.9743 - val_loss: 0.4322 - val_accuracy: 0.8810 - 21s/epoch - 55ms/step\n",
      "Epoch 150/200\n",
      "390/390 - 20s - loss: 0.0732 - accuracy: 0.9749 - val_loss: 0.4582 - val_accuracy: 0.8830 - 20s/epoch - 52ms/step\n",
      "Epoch 151/200\n",
      "390/390 - 21s - loss: 0.0848 - accuracy: 0.9713 - val_loss: 0.3379 - val_accuracy: 0.9029 - 21s/epoch - 54ms/step\n",
      "Epoch 152/200\n",
      "390/390 - 21s - loss: 0.0800 - accuracy: 0.9720 - val_loss: 0.3547 - val_accuracy: 0.8987 - 21s/epoch - 54ms/step\n",
      "Epoch 153/200\n",
      "390/390 - 20s - loss: 0.0735 - accuracy: 0.9748 - val_loss: 0.4844 - val_accuracy: 0.8701 - 20s/epoch - 52ms/step\n",
      "Epoch 154/200\n",
      "390/390 - 20s - loss: 0.0731 - accuracy: 0.9743 - val_loss: 0.4142 - val_accuracy: 0.8850 - 20s/epoch - 52ms/step\n",
      "Epoch 155/200\n",
      "390/390 - 20s - loss: 0.0808 - accuracy: 0.9721 - val_loss: 0.3885 - val_accuracy: 0.8924 - 20s/epoch - 52ms/step\n",
      "Epoch 156/200\n",
      "390/390 - 20s - loss: 0.0762 - accuracy: 0.9735 - val_loss: 0.3576 - val_accuracy: 0.9002 - 20s/epoch - 52ms/step\n",
      "Epoch 157/200\n",
      "390/390 - 20s - loss: 0.0702 - accuracy: 0.9757 - val_loss: 0.3387 - val_accuracy: 0.9062 - 20s/epoch - 52ms/step\n",
      "Epoch 158/200\n",
      "390/390 - 22s - loss: 0.0651 - accuracy: 0.9778 - val_loss: 0.3779 - val_accuracy: 0.8963 - 22s/epoch - 55ms/step\n",
      "Epoch 159/200\n",
      "390/390 - 22s - loss: 0.0693 - accuracy: 0.9758 - val_loss: 0.3475 - val_accuracy: 0.9049 - 22s/epoch - 56ms/step\n",
      "Epoch 160/200\n",
      "390/390 - 20s - loss: 0.0678 - accuracy: 0.9769 - val_loss: 0.4432 - val_accuracy: 0.8817 - 20s/epoch - 52ms/step\n",
      "Epoch 161/200\n",
      "390/390 - 21s - loss: 0.0669 - accuracy: 0.9778 - val_loss: 0.3180 - val_accuracy: 0.9120 - 21s/epoch - 54ms/step\n",
      "Epoch 162/200\n",
      "390/390 - 22s - loss: 0.0645 - accuracy: 0.9781 - val_loss: 0.3117 - val_accuracy: 0.9131 - 22s/epoch - 56ms/step\n",
      "Epoch 163/200\n",
      "390/390 - 20s - loss: 0.0654 - accuracy: 0.9777 - val_loss: 0.3623 - val_accuracy: 0.8991 - 20s/epoch - 52ms/step\n",
      "Epoch 164/200\n",
      "390/390 - 21s - loss: 0.0660 - accuracy: 0.9775 - val_loss: 0.2845 - val_accuracy: 0.9162 - 21s/epoch - 55ms/step\n",
      "Epoch 165/200\n",
      "390/390 - 22s - loss: 0.0650 - accuracy: 0.9776 - val_loss: 0.3409 - val_accuracy: 0.9061 - 22s/epoch - 57ms/step\n",
      "Epoch 166/200\n",
      "390/390 - 21s - loss: 0.0628 - accuracy: 0.9781 - val_loss: 0.3019 - val_accuracy: 0.9129 - 21s/epoch - 55ms/step\n",
      "Epoch 167/200\n",
      "390/390 - 21s - loss: 0.0555 - accuracy: 0.9806 - val_loss: 0.3254 - val_accuracy: 0.9076 - 21s/epoch - 55ms/step\n",
      "Epoch 168/200\n",
      "390/390 - 21s - loss: 0.0562 - accuracy: 0.9820 - val_loss: 0.3650 - val_accuracy: 0.9001 - 21s/epoch - 54ms/step\n",
      "Epoch 169/200\n",
      "390/390 - 21s - loss: 0.0559 - accuracy: 0.9815 - val_loss: 0.2963 - val_accuracy: 0.9157 - 21s/epoch - 54ms/step\n",
      "Epoch 170/200\n",
      "390/390 - 21s - loss: 0.0536 - accuracy: 0.9820 - val_loss: 0.3468 - val_accuracy: 0.9059 - 21s/epoch - 54ms/step\n",
      "Epoch 171/200\n",
      "390/390 - 22s - loss: 0.0525 - accuracy: 0.9828 - val_loss: 0.3426 - val_accuracy: 0.9092 - 22s/epoch - 55ms/step\n",
      "Epoch 172/200\n",
      "390/390 - 21s - loss: 0.0493 - accuracy: 0.9838 - val_loss: 0.2907 - val_accuracy: 0.9206 - 21s/epoch - 53ms/step\n",
      "Epoch 173/200\n",
      "390/390 - 22s - loss: 0.0510 - accuracy: 0.9826 - val_loss: 0.2870 - val_accuracy: 0.9194 - 22s/epoch - 56ms/step\n",
      "Epoch 174/200\n",
      "390/390 - 21s - loss: 0.0434 - accuracy: 0.9859 - val_loss: 0.3140 - val_accuracy: 0.9145 - 21s/epoch - 54ms/step\n",
      "Epoch 175/200\n",
      "390/390 - 22s - loss: 0.0418 - accuracy: 0.9869 - val_loss: 0.2993 - val_accuracy: 0.9184 - 22s/epoch - 56ms/step\n",
      "Epoch 176/200\n",
      "390/390 - 21s - loss: 0.0419 - accuracy: 0.9866 - val_loss: 0.3413 - val_accuracy: 0.9104 - 21s/epoch - 54ms/step\n",
      "Epoch 177/200\n",
      "390/390 - 22s - loss: 0.0413 - accuracy: 0.9863 - val_loss: 0.2857 - val_accuracy: 0.9198 - 22s/epoch - 55ms/step\n",
      "Epoch 178/200\n",
      "390/390 - 20s - loss: 0.0390 - accuracy: 0.9875 - val_loss: 0.3228 - val_accuracy: 0.9145 - 20s/epoch - 51ms/step\n",
      "Epoch 179/200\n",
      "390/390 - 21s - loss: 0.0349 - accuracy: 0.9892 - val_loss: 0.2964 - val_accuracy: 0.9183 - 21s/epoch - 54ms/step\n",
      "Epoch 180/200\n",
      "390/390 - 20s - loss: 0.0352 - accuracy: 0.9891 - val_loss: 0.2654 - val_accuracy: 0.9252 - 20s/epoch - 52ms/step\n",
      "Epoch 181/200\n",
      "390/390 - 20s - loss: 0.0291 - accuracy: 0.9913 - val_loss: 0.2736 - val_accuracy: 0.9231 - 20s/epoch - 52ms/step\n",
      "Epoch 182/200\n",
      "390/390 - 22s - loss: 0.0289 - accuracy: 0.9916 - val_loss: 0.2718 - val_accuracy: 0.9223 - 22s/epoch - 57ms/step\n",
      "Epoch 183/200\n",
      "390/390 - 20s - loss: 0.0259 - accuracy: 0.9924 - val_loss: 0.2490 - val_accuracy: 0.9277 - 20s/epoch - 52ms/step\n",
      "Epoch 184/200\n",
      "390/390 - 20s - loss: 0.0273 - accuracy: 0.9922 - val_loss: 0.2564 - val_accuracy: 0.9316 - 20s/epoch - 52ms/step\n",
      "Epoch 185/200\n",
      "390/390 - 22s - loss: 0.0220 - accuracy: 0.9945 - val_loss: 0.2858 - val_accuracy: 0.9200 - 22s/epoch - 56ms/step\n",
      "Epoch 186/200\n",
      "390/390 - 20s - loss: 0.0204 - accuracy: 0.9948 - val_loss: 0.2688 - val_accuracy: 0.9247 - 20s/epoch - 52ms/step\n",
      "Epoch 187/200\n",
      "390/390 - 20s - loss: 0.0211 - accuracy: 0.9943 - val_loss: 0.2618 - val_accuracy: 0.9260 - 20s/epoch - 52ms/step\n",
      "Epoch 188/200\n",
      "390/390 - 21s - loss: 0.0211 - accuracy: 0.9946 - val_loss: 0.2351 - val_accuracy: 0.9321 - 21s/epoch - 54ms/step\n",
      "Epoch 189/200\n",
      "390/390 - 20s - loss: 0.0189 - accuracy: 0.9954 - val_loss: 0.2313 - val_accuracy: 0.9332 - 20s/epoch - 52ms/step\n",
      "Epoch 190/200\n",
      "390/390 - 20s - loss: 0.0176 - accuracy: 0.9964 - val_loss: 0.2212 - val_accuracy: 0.9344 - 20s/epoch - 52ms/step\n",
      "Epoch 191/200\n",
      "390/390 - 22s - loss: 0.0162 - accuracy: 0.9966 - val_loss: 0.2236 - val_accuracy: 0.9352 - 22s/epoch - 56ms/step\n",
      "Epoch 192/200\n",
      "390/390 - 21s - loss: 0.0152 - accuracy: 0.9974 - val_loss: 0.2211 - val_accuracy: 0.9369 - 21s/epoch - 55ms/step\n",
      "Epoch 193/200\n",
      "390/390 - 20s - loss: 0.0152 - accuracy: 0.9975 - val_loss: 0.2095 - val_accuracy: 0.9390 - 20s/epoch - 52ms/step\n",
      "Epoch 194/200\n",
      "390/390 - 21s - loss: 0.0157 - accuracy: 0.9978 - val_loss: 0.2167 - val_accuracy: 0.9362 - 21s/epoch - 54ms/step\n",
      "Epoch 195/200\n",
      "390/390 - 21s - loss: 0.0163 - accuracy: 0.9981 - val_loss: 0.2056 - val_accuracy: 0.9395 - 21s/epoch - 54ms/step\n",
      "Epoch 196/200\n",
      "390/390 - 20s - loss: 0.0183 - accuracy: 0.9979 - val_loss: 0.2015 - val_accuracy: 0.9388 - 20s/epoch - 52ms/step\n",
      "Epoch 197/200\n",
      "390/390 - 21s - loss: 0.0197 - accuracy: 0.9979 - val_loss: 0.2006 - val_accuracy: 0.9394 - 21s/epoch - 55ms/step\n",
      "Epoch 198/200\n",
      "390/390 - 22s - loss: 0.0223 - accuracy: 0.9980 - val_loss: 0.1984 - val_accuracy: 0.9401 - 22s/epoch - 57ms/step\n",
      "Epoch 199/200\n",
      "390/390 - 20s - loss: 0.0265 - accuracy: 0.9979 - val_loss: 0.1975 - val_accuracy: 0.9406 - 20s/epoch - 52ms/step\n",
      "Epoch 200/200\n",
      "390/390 - 21s - loss: 0.0316 - accuracy: 0.9981 - val_loss: 0.1986 - val_accuracy: 0.9405 - 21s/epoch - 54ms/step\n",
      " Using best val_acc=0.9406 from last 20 epochs\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.1975 - accuracy: 0.9406\n",
      " Finished: Lam=1.0, Repeat=2, Acc=0.9406\n",
      "\n",
      " lambda: Lam=1.0, Repeat=3/3\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "132506468 561714176\n",
      "Epoch 1/200\n",
      "390/390 - 22s - loss: 1.0554 - accuracy: 0.6374 - val_loss: 6.2687 - val_accuracy: 0.3148 - 22s/epoch - 57ms/step\n",
      "Epoch 2/200\n",
      "390/390 - 21s - loss: 0.7225 - accuracy: 0.7538 - val_loss: 1.8001 - val_accuracy: 0.5537 - 21s/epoch - 55ms/step\n",
      "Epoch 3/200\n",
      "390/390 - 20s - loss: 0.6212 - accuracy: 0.7885 - val_loss: 1.6325 - val_accuracy: 0.5554 - 20s/epoch - 52ms/step\n",
      "Epoch 4/200\n",
      "390/390 - 20s - loss: 0.5581 - accuracy: 0.8089 - val_loss: 1.3395 - val_accuracy: 0.6261 - 20s/epoch - 52ms/step\n",
      "Epoch 5/200\n",
      "390/390 - 20s - loss: 0.5119 - accuracy: 0.8243 - val_loss: 0.7380 - val_accuracy: 0.7500 - 20s/epoch - 53ms/step\n",
      "Epoch 6/200\n",
      "390/390 - 22s - loss: 0.4620 - accuracy: 0.8420 - val_loss: 0.5504 - val_accuracy: 0.8157 - 22s/epoch - 56ms/step\n",
      "Epoch 7/200\n",
      "390/390 - 20s - loss: 0.4193 - accuracy: 0.8567 - val_loss: 0.6406 - val_accuracy: 0.7844 - 20s/epoch - 52ms/step\n",
      "Epoch 8/200\n",
      "390/390 - 21s - loss: 0.3893 - accuracy: 0.8652 - val_loss: 0.5537 - val_accuracy: 0.8250 - 21s/epoch - 55ms/step\n",
      "Epoch 9/200\n",
      "390/390 - 21s - loss: 0.3616 - accuracy: 0.8750 - val_loss: 0.5851 - val_accuracy: 0.8149 - 21s/epoch - 54ms/step\n",
      "Epoch 10/200\n",
      "390/390 - 22s - loss: 0.3389 - accuracy: 0.8823 - val_loss: 0.4903 - val_accuracy: 0.8421 - 22s/epoch - 56ms/step\n",
      "Epoch 11/200\n",
      "390/390 - 20s - loss: 0.3271 - accuracy: 0.8866 - val_loss: 0.5545 - val_accuracy: 0.8205 - 20s/epoch - 52ms/step\n",
      "Epoch 12/200\n",
      "390/390 - 23s - loss: 0.3099 - accuracy: 0.8934 - val_loss: 0.5529 - val_accuracy: 0.8259 - 23s/epoch - 58ms/step\n",
      "Epoch 13/200\n",
      "390/390 - 22s - loss: 0.2960 - accuracy: 0.8986 - val_loss: 0.5743 - val_accuracy: 0.8267 - 22s/epoch - 56ms/step\n",
      "Epoch 14/200\n",
      "390/390 - 21s - loss: 0.2844 - accuracy: 0.9020 - val_loss: 0.4919 - val_accuracy: 0.8451 - 21s/epoch - 54ms/step\n",
      "Epoch 15/200\n",
      "390/390 - 20s - loss: 0.2761 - accuracy: 0.9033 - val_loss: 0.3587 - val_accuracy: 0.8803 - 20s/epoch - 52ms/step\n",
      "Epoch 16/200\n",
      "390/390 - 21s - loss: 0.2631 - accuracy: 0.9086 - val_loss: 0.3475 - val_accuracy: 0.8785 - 21s/epoch - 54ms/step\n",
      "Epoch 17/200\n",
      "390/390 - 20s - loss: 0.2601 - accuracy: 0.9086 - val_loss: 0.5430 - val_accuracy: 0.8300 - 20s/epoch - 52ms/step\n",
      "Epoch 18/200\n",
      "390/390 - 21s - loss: 0.2503 - accuracy: 0.9124 - val_loss: 0.5434 - val_accuracy: 0.8194 - 21s/epoch - 53ms/step\n",
      "Epoch 19/200\n",
      "390/390 - 20s - loss: 0.2464 - accuracy: 0.9151 - val_loss: 0.3639 - val_accuracy: 0.8837 - 20s/epoch - 52ms/step\n",
      "Epoch 20/200\n",
      "390/390 - 22s - loss: 0.2391 - accuracy: 0.9165 - val_loss: 0.6144 - val_accuracy: 0.8162 - 22s/epoch - 56ms/step\n",
      "Epoch 21/200\n",
      "390/390 - 20s - loss: 0.2343 - accuracy: 0.9184 - val_loss: 0.4939 - val_accuracy: 0.8407 - 20s/epoch - 52ms/step\n",
      "Epoch 22/200\n",
      "390/390 - 20s - loss: 0.2333 - accuracy: 0.9184 - val_loss: 0.4428 - val_accuracy: 0.8569 - 20s/epoch - 52ms/step\n",
      "Epoch 23/200\n",
      "390/390 - 20s - loss: 0.2241 - accuracy: 0.9209 - val_loss: 0.3871 - val_accuracy: 0.8718 - 20s/epoch - 52ms/step\n",
      "Epoch 24/200\n",
      "390/390 - 20s - loss: 0.2200 - accuracy: 0.9217 - val_loss: 0.7465 - val_accuracy: 0.7962 - 20s/epoch - 52ms/step\n",
      "Epoch 25/200\n",
      "390/390 - 21s - loss: 0.2137 - accuracy: 0.9246 - val_loss: 0.4313 - val_accuracy: 0.8644 - 21s/epoch - 54ms/step\n",
      "Epoch 26/200\n",
      "390/390 - 20s - loss: 0.2128 - accuracy: 0.9248 - val_loss: 0.6995 - val_accuracy: 0.8006 - 20s/epoch - 52ms/step\n",
      "Epoch 27/200\n",
      "390/390 - 21s - loss: 0.2049 - accuracy: 0.9278 - val_loss: 0.4880 - val_accuracy: 0.8536 - 21s/epoch - 54ms/step\n",
      "Epoch 28/200\n",
      "390/390 - 20s - loss: 0.2066 - accuracy: 0.9287 - val_loss: 0.4257 - val_accuracy: 0.8747 - 20s/epoch - 52ms/step\n",
      "Epoch 29/200\n",
      "390/390 - 20s - loss: 0.1964 - accuracy: 0.9307 - val_loss: 0.4164 - val_accuracy: 0.8738 - 20s/epoch - 52ms/step\n",
      "Epoch 30/200\n",
      "390/390 - 20s - loss: 0.1998 - accuracy: 0.9299 - val_loss: 0.4842 - val_accuracy: 0.8474 - 20s/epoch - 52ms/step\n",
      "Epoch 31/200\n",
      "390/390 - 22s - loss: 0.1975 - accuracy: 0.9302 - val_loss: 0.4506 - val_accuracy: 0.8628 - 22s/epoch - 55ms/step\n",
      "Epoch 32/200\n",
      "390/390 - 22s - loss: 0.1945 - accuracy: 0.9313 - val_loss: 0.3967 - val_accuracy: 0.8722 - 22s/epoch - 56ms/step\n",
      "Epoch 33/200\n",
      "390/390 - 20s - loss: 0.1833 - accuracy: 0.9361 - val_loss: 0.5465 - val_accuracy: 0.8319 - 20s/epoch - 52ms/step\n",
      "Epoch 34/200\n",
      "390/390 - 20s - loss: 0.1865 - accuracy: 0.9333 - val_loss: 0.4031 - val_accuracy: 0.8725 - 20s/epoch - 52ms/step\n",
      "Epoch 35/200\n",
      "390/390 - 20s - loss: 0.1833 - accuracy: 0.9342 - val_loss: 0.4249 - val_accuracy: 0.8647 - 20s/epoch - 52ms/step\n",
      "Epoch 36/200\n",
      "390/390 - 20s - loss: 0.1794 - accuracy: 0.9362 - val_loss: 0.4439 - val_accuracy: 0.8599 - 20s/epoch - 52ms/step\n",
      "Epoch 37/200\n",
      "390/390 - 21s - loss: 0.1748 - accuracy: 0.9382 - val_loss: 0.5231 - val_accuracy: 0.8409 - 21s/epoch - 54ms/step\n",
      "Epoch 38/200\n",
      "390/390 - 20s - loss: 0.1763 - accuracy: 0.9380 - val_loss: 0.3791 - val_accuracy: 0.8782 - 20s/epoch - 52ms/step\n",
      "Epoch 39/200\n",
      "390/390 - 20s - loss: 0.1759 - accuracy: 0.9380 - val_loss: 0.3789 - val_accuracy: 0.8769 - 20s/epoch - 52ms/step\n",
      "Epoch 40/200\n",
      "390/390 - 22s - loss: 0.1722 - accuracy: 0.9391 - val_loss: 0.4762 - val_accuracy: 0.8650 - 22s/epoch - 57ms/step\n",
      "Epoch 41/200\n",
      "390/390 - 20s - loss: 0.1695 - accuracy: 0.9398 - val_loss: 0.3838 - val_accuracy: 0.8782 - 20s/epoch - 52ms/step\n",
      "Epoch 42/200\n",
      "390/390 - 20s - loss: 0.1671 - accuracy: 0.9402 - val_loss: 0.4118 - val_accuracy: 0.8734 - 20s/epoch - 52ms/step\n",
      "Epoch 43/200\n",
      "390/390 - 20s - loss: 0.1690 - accuracy: 0.9401 - val_loss: 0.4407 - val_accuracy: 0.8621 - 20s/epoch - 52ms/step\n",
      "Epoch 44/200\n",
      "390/390 - 20s - loss: 0.1623 - accuracy: 0.9430 - val_loss: 0.5370 - val_accuracy: 0.8473 - 20s/epoch - 52ms/step\n",
      "Epoch 45/200\n",
      "390/390 - 22s - loss: 0.1642 - accuracy: 0.9410 - val_loss: 0.5310 - val_accuracy: 0.8430 - 22s/epoch - 57ms/step\n",
      "Epoch 46/200\n",
      "390/390 - 20s - loss: 0.1649 - accuracy: 0.9430 - val_loss: 0.4217 - val_accuracy: 0.8721 - 20s/epoch - 52ms/step\n",
      "Epoch 47/200\n",
      "390/390 - 21s - loss: 0.1615 - accuracy: 0.9432 - val_loss: 0.4124 - val_accuracy: 0.8767 - 21s/epoch - 54ms/step\n",
      "Epoch 48/200\n",
      "390/390 - 21s - loss: 0.1582 - accuracy: 0.9449 - val_loss: 0.6510 - val_accuracy: 0.8210 - 21s/epoch - 55ms/step\n",
      "Epoch 49/200\n",
      "390/390 - 21s - loss: 0.1582 - accuracy: 0.9448 - val_loss: 0.5063 - val_accuracy: 0.8520 - 21s/epoch - 53ms/step\n",
      "Epoch 50/200\n",
      "390/390 - 22s - loss: 0.1573 - accuracy: 0.9453 - val_loss: 0.4248 - val_accuracy: 0.8727 - 22s/epoch - 55ms/step\n",
      "Epoch 51/200\n",
      "390/390 - 22s - loss: 0.1554 - accuracy: 0.9442 - val_loss: 0.4227 - val_accuracy: 0.8736 - 22s/epoch - 55ms/step\n",
      "Epoch 52/200\n",
      "390/390 - 21s - loss: 0.1504 - accuracy: 0.9478 - val_loss: 0.4271 - val_accuracy: 0.8702 - 21s/epoch - 55ms/step\n",
      "Epoch 53/200\n",
      "390/390 - 22s - loss: 0.1550 - accuracy: 0.9453 - val_loss: 0.3597 - val_accuracy: 0.8885 - 22s/epoch - 56ms/step\n",
      "Epoch 54/200\n",
      "390/390 - 20s - loss: 0.1520 - accuracy: 0.9468 - val_loss: 0.5084 - val_accuracy: 0.8515 - 20s/epoch - 52ms/step\n",
      "Epoch 55/200\n",
      "390/390 - 22s - loss: 0.1476 - accuracy: 0.9474 - val_loss: 0.3258 - val_accuracy: 0.8976 - 22s/epoch - 56ms/step\n",
      "Epoch 56/200\n",
      "390/390 - 21s - loss: 0.1534 - accuracy: 0.9462 - val_loss: 0.3993 - val_accuracy: 0.8864 - 21s/epoch - 55ms/step\n",
      "Epoch 57/200\n",
      "390/390 - 20s - loss: 0.1563 - accuracy: 0.9454 - val_loss: 0.4839 - val_accuracy: 0.8543 - 20s/epoch - 52ms/step\n",
      "Epoch 58/200\n",
      "390/390 - 21s - loss: 0.1475 - accuracy: 0.9474 - val_loss: 0.3351 - val_accuracy: 0.9011 - 21s/epoch - 54ms/step\n",
      "Epoch 59/200\n",
      "390/390 - 20s - loss: 0.1476 - accuracy: 0.9476 - val_loss: 0.5125 - val_accuracy: 0.8551 - 20s/epoch - 52ms/step\n",
      "Epoch 60/200\n",
      "390/390 - 20s - loss: 0.1428 - accuracy: 0.9504 - val_loss: 0.3892 - val_accuracy: 0.8839 - 20s/epoch - 52ms/step\n",
      "Epoch 61/200\n",
      "390/390 - 22s - loss: 0.1462 - accuracy: 0.9492 - val_loss: 0.3627 - val_accuracy: 0.8879 - 22s/epoch - 57ms/step\n",
      "Epoch 62/200\n",
      "390/390 - 20s - loss: 0.1457 - accuracy: 0.9482 - val_loss: 0.4628 - val_accuracy: 0.8634 - 20s/epoch - 52ms/step\n",
      "Epoch 63/200\n",
      "390/390 - 21s - loss: 0.1404 - accuracy: 0.9512 - val_loss: 0.4916 - val_accuracy: 0.8562 - 21s/epoch - 54ms/step\n",
      "Epoch 64/200\n",
      "390/390 - 21s - loss: 0.1413 - accuracy: 0.9508 - val_loss: 0.3513 - val_accuracy: 0.8972 - 21s/epoch - 55ms/step\n",
      "Epoch 65/200\n",
      "390/390 - 20s - loss: 0.1383 - accuracy: 0.9506 - val_loss: 0.5086 - val_accuracy: 0.8536 - 20s/epoch - 52ms/step\n",
      "Epoch 66/200\n",
      "390/390 - 21s - loss: 0.1395 - accuracy: 0.9502 - val_loss: 0.4279 - val_accuracy: 0.8778 - 21s/epoch - 53ms/step\n",
      "Epoch 67/200\n",
      "390/390 - 20s - loss: 0.1381 - accuracy: 0.9522 - val_loss: 0.3520 - val_accuracy: 0.8953 - 20s/epoch - 52ms/step\n",
      "Epoch 68/200\n",
      "390/390 - 20s - loss: 0.1335 - accuracy: 0.9529 - val_loss: 0.3989 - val_accuracy: 0.8847 - 20s/epoch - 52ms/step\n",
      "Epoch 69/200\n",
      "390/390 - 20s - loss: 0.1373 - accuracy: 0.9516 - val_loss: 0.4818 - val_accuracy: 0.8704 - 20s/epoch - 52ms/step\n",
      "Epoch 70/200\n",
      "390/390 - 21s - loss: 0.1405 - accuracy: 0.9509 - val_loss: 0.3879 - val_accuracy: 0.8847 - 21s/epoch - 53ms/step\n",
      "Epoch 71/200\n",
      "390/390 - 22s - loss: 0.1379 - accuracy: 0.9522 - val_loss: 0.3594 - val_accuracy: 0.8924 - 22s/epoch - 55ms/step\n",
      "Epoch 72/200\n",
      "390/390 - 22s - loss: 0.1309 - accuracy: 0.9532 - val_loss: 0.5467 - val_accuracy: 0.8389 - 22s/epoch - 55ms/step\n",
      "Epoch 73/200\n",
      "390/390 - 22s - loss: 0.1328 - accuracy: 0.9521 - val_loss: 0.7180 - val_accuracy: 0.8221 - 22s/epoch - 55ms/step\n",
      "Epoch 74/200\n",
      "390/390 - 20s - loss: 0.1321 - accuracy: 0.9530 - val_loss: 0.4385 - val_accuracy: 0.8792 - 20s/epoch - 52ms/step\n",
      "Epoch 75/200\n",
      "390/390 - 20s - loss: 0.1332 - accuracy: 0.9532 - val_loss: 0.4765 - val_accuracy: 0.8662 - 20s/epoch - 52ms/step\n",
      "Epoch 76/200\n",
      "390/390 - 22s - loss: 0.1322 - accuracy: 0.9537 - val_loss: 0.3391 - val_accuracy: 0.8963 - 22s/epoch - 55ms/step\n",
      "Epoch 77/200\n",
      "390/390 - 20s - loss: 0.1342 - accuracy: 0.9527 - val_loss: 0.3427 - val_accuracy: 0.8954 - 20s/epoch - 52ms/step\n",
      "Epoch 78/200\n",
      "390/390 - 22s - loss: 0.1303 - accuracy: 0.9532 - val_loss: 0.4051 - val_accuracy: 0.8836 - 22s/epoch - 55ms/step\n",
      "Epoch 79/200\n",
      "390/390 - 22s - loss: 0.1300 - accuracy: 0.9545 - val_loss: 0.4176 - val_accuracy: 0.8798 - 22s/epoch - 56ms/step\n",
      "Epoch 80/200\n",
      "390/390 - 21s - loss: 0.1309 - accuracy: 0.9542 - val_loss: 0.5052 - val_accuracy: 0.8556 - 21s/epoch - 53ms/step\n",
      "Epoch 81/200\n",
      "390/390 - 21s - loss: 0.1292 - accuracy: 0.9546 - val_loss: 0.4682 - val_accuracy: 0.8682 - 21s/epoch - 55ms/step\n",
      "Epoch 82/200\n",
      "390/390 - 21s - loss: 0.1302 - accuracy: 0.9548 - val_loss: 0.3672 - val_accuracy: 0.8855 - 21s/epoch - 55ms/step\n",
      "Epoch 83/200\n",
      "390/390 - 20s - loss: 0.1267 - accuracy: 0.9558 - val_loss: 0.5385 - val_accuracy: 0.8583 - 20s/epoch - 52ms/step\n",
      "Epoch 84/200\n",
      "390/390 - 22s - loss: 0.1287 - accuracy: 0.9549 - val_loss: 0.3972 - val_accuracy: 0.8816 - 22s/epoch - 55ms/step\n",
      "Epoch 85/200\n",
      "390/390 - 20s - loss: 0.1249 - accuracy: 0.9563 - val_loss: 0.3584 - val_accuracy: 0.8922 - 20s/epoch - 52ms/step\n",
      "Epoch 86/200\n",
      "390/390 - 20s - loss: 0.1260 - accuracy: 0.9552 - val_loss: 0.6165 - val_accuracy: 0.8374 - 20s/epoch - 52ms/step\n",
      "Epoch 87/200\n",
      "390/390 - 21s - loss: 0.1233 - accuracy: 0.9564 - val_loss: 0.4394 - val_accuracy: 0.8767 - 21s/epoch - 55ms/step\n",
      "Epoch 88/200\n",
      "390/390 - 21s - loss: 0.1248 - accuracy: 0.9560 - val_loss: 0.3665 - val_accuracy: 0.8882 - 21s/epoch - 55ms/step\n",
      "Epoch 89/200\n",
      "390/390 - 20s - loss: 0.1264 - accuracy: 0.9557 - val_loss: 0.4898 - val_accuracy: 0.8672 - 20s/epoch - 52ms/step\n",
      "Epoch 90/200\n",
      "390/390 - 20s - loss: 0.1209 - accuracy: 0.9574 - val_loss: 0.4894 - val_accuracy: 0.8627 - 20s/epoch - 52ms/step\n",
      "Epoch 91/200\n",
      "390/390 - 21s - loss: 0.1214 - accuracy: 0.9577 - val_loss: 0.4015 - val_accuracy: 0.8829 - 21s/epoch - 54ms/step\n",
      "Epoch 92/200\n",
      "390/390 - 21s - loss: 0.1231 - accuracy: 0.9568 - val_loss: 0.3232 - val_accuracy: 0.8993 - 21s/epoch - 55ms/step\n",
      "Epoch 93/200\n",
      "390/390 - 20s - loss: 0.1139 - accuracy: 0.9609 - val_loss: 0.3916 - val_accuracy: 0.8864 - 20s/epoch - 52ms/step\n",
      "Epoch 94/200\n",
      "390/390 - 22s - loss: 0.1193 - accuracy: 0.9591 - val_loss: 0.3713 - val_accuracy: 0.8901 - 22s/epoch - 57ms/step\n",
      "Epoch 95/200\n",
      "390/390 - 20s - loss: 0.1173 - accuracy: 0.9585 - val_loss: 0.4523 - val_accuracy: 0.8714 - 20s/epoch - 52ms/step\n",
      "Epoch 96/200\n",
      "390/390 - 21s - loss: 0.1195 - accuracy: 0.9584 - val_loss: 0.4753 - val_accuracy: 0.8655 - 21s/epoch - 55ms/step\n",
      "Epoch 97/200\n",
      "390/390 - 22s - loss: 0.1204 - accuracy: 0.9576 - val_loss: 0.3907 - val_accuracy: 0.8882 - 22s/epoch - 55ms/step\n",
      "Epoch 98/200\n",
      "390/390 - 21s - loss: 0.1173 - accuracy: 0.9587 - val_loss: 0.4331 - val_accuracy: 0.8691 - 21s/epoch - 54ms/step\n",
      "Epoch 99/200\n",
      "390/390 - 21s - loss: 0.1146 - accuracy: 0.9592 - val_loss: 0.4671 - val_accuracy: 0.8685 - 21s/epoch - 54ms/step\n",
      "Epoch 100/200\n",
      "390/390 - 22s - loss: 0.1163 - accuracy: 0.9593 - val_loss: 0.3956 - val_accuracy: 0.8847 - 22s/epoch - 57ms/step\n",
      "Epoch 101/200\n",
      "390/390 - 20s - loss: 0.1124 - accuracy: 0.9606 - val_loss: 0.4427 - val_accuracy: 0.8762 - 20s/epoch - 52ms/step\n",
      "Epoch 102/200\n",
      "390/390 - 21s - loss: 0.1158 - accuracy: 0.9600 - val_loss: 0.5179 - val_accuracy: 0.8484 - 21s/epoch - 54ms/step\n",
      "Epoch 103/200\n",
      "390/390 - 20s - loss: 0.1120 - accuracy: 0.9612 - val_loss: 0.3897 - val_accuracy: 0.8842 - 20s/epoch - 52ms/step\n",
      "Epoch 104/200\n",
      "390/390 - 20s - loss: 0.1136 - accuracy: 0.9602 - val_loss: 0.5040 - val_accuracy: 0.8659 - 20s/epoch - 53ms/step\n",
      "Epoch 105/200\n",
      "390/390 - 22s - loss: 0.1157 - accuracy: 0.9596 - val_loss: 0.4103 - val_accuracy: 0.8815 - 22s/epoch - 57ms/step\n",
      "Epoch 106/200\n",
      "390/390 - 21s - loss: 0.1087 - accuracy: 0.9631 - val_loss: 0.3545 - val_accuracy: 0.9013 - 21s/epoch - 55ms/step\n",
      "Epoch 107/200\n",
      "390/390 - 21s - loss: 0.1129 - accuracy: 0.9613 - val_loss: 0.5365 - val_accuracy: 0.8573 - 21s/epoch - 55ms/step\n",
      "Epoch 108/200\n",
      "390/390 - 21s - loss: 0.1098 - accuracy: 0.9627 - val_loss: 0.3807 - val_accuracy: 0.8922 - 21s/epoch - 55ms/step\n",
      "Epoch 109/200\n",
      "390/390 - 21s - loss: 0.1080 - accuracy: 0.9615 - val_loss: 0.4700 - val_accuracy: 0.8692 - 21s/epoch - 54ms/step\n",
      "Epoch 110/200\n",
      "390/390 - 22s - loss: 0.1106 - accuracy: 0.9615 - val_loss: 0.4717 - val_accuracy: 0.8627 - 22s/epoch - 58ms/step\n",
      "Epoch 111/200\n",
      "390/390 - 20s - loss: 0.1074 - accuracy: 0.9629 - val_loss: 0.4663 - val_accuracy: 0.8707 - 20s/epoch - 52ms/step\n",
      "Epoch 112/200\n",
      "390/390 - 22s - loss: 0.1075 - accuracy: 0.9623 - val_loss: 0.3918 - val_accuracy: 0.8851 - 22s/epoch - 55ms/step\n",
      "Epoch 113/200\n",
      "390/390 - 22s - loss: 0.1101 - accuracy: 0.9609 - val_loss: 0.3248 - val_accuracy: 0.8997 - 22s/epoch - 56ms/step\n",
      "Epoch 114/200\n",
      "390/390 - 22s - loss: 0.1061 - accuracy: 0.9629 - val_loss: 0.3780 - val_accuracy: 0.8913 - 22s/epoch - 58ms/step\n",
      "Epoch 115/200\n",
      "390/390 - 21s - loss: 0.1056 - accuracy: 0.9635 - val_loss: 0.3360 - val_accuracy: 0.9013 - 21s/epoch - 55ms/step\n",
      "Epoch 116/200\n",
      "390/390 - 22s - loss: 0.1047 - accuracy: 0.9636 - val_loss: 0.3361 - val_accuracy: 0.9033 - 22s/epoch - 55ms/step\n",
      "Epoch 117/200\n",
      "390/390 - 22s - loss: 0.1080 - accuracy: 0.9626 - val_loss: 0.3674 - val_accuracy: 0.8947 - 22s/epoch - 57ms/step\n",
      "Epoch 118/200\n",
      "390/390 - 20s - loss: 0.1067 - accuracy: 0.9634 - val_loss: 0.3350 - val_accuracy: 0.9009 - 20s/epoch - 52ms/step\n",
      "Epoch 119/200\n",
      "390/390 - 20s - loss: 0.0998 - accuracy: 0.9645 - val_loss: 0.4388 - val_accuracy: 0.8771 - 20s/epoch - 53ms/step\n",
      "Epoch 120/200\n",
      "390/390 - 21s - loss: 0.1059 - accuracy: 0.9624 - val_loss: 0.4506 - val_accuracy: 0.8764 - 21s/epoch - 55ms/step\n",
      "Epoch 121/200\n",
      "390/390 - 20s - loss: 0.1062 - accuracy: 0.9622 - val_loss: 0.4877 - val_accuracy: 0.8617 - 20s/epoch - 52ms/step\n",
      "Epoch 122/200\n",
      "390/390 - 21s - loss: 0.1002 - accuracy: 0.9654 - val_loss: 0.4297 - val_accuracy: 0.8876 - 21s/epoch - 55ms/step\n",
      "Epoch 123/200\n",
      "390/390 - 21s - loss: 0.1020 - accuracy: 0.9631 - val_loss: 0.3525 - val_accuracy: 0.8994 - 21s/epoch - 54ms/step\n",
      "Epoch 124/200\n",
      "390/390 - 22s - loss: 0.0987 - accuracy: 0.9660 - val_loss: 0.3634 - val_accuracy: 0.8917 - 22s/epoch - 56ms/step\n",
      "Epoch 125/200\n",
      "390/390 - 21s - loss: 0.1001 - accuracy: 0.9658 - val_loss: 0.3612 - val_accuracy: 0.8939 - 21s/epoch - 55ms/step\n",
      "Epoch 126/200\n",
      "390/390 - 22s - loss: 0.0998 - accuracy: 0.9652 - val_loss: 0.4201 - val_accuracy: 0.8811 - 22s/epoch - 56ms/step\n",
      "Epoch 127/200\n",
      "390/390 - 20s - loss: 0.0954 - accuracy: 0.9668 - val_loss: 0.3346 - val_accuracy: 0.9048 - 20s/epoch - 52ms/step\n",
      "Epoch 128/200\n",
      "390/390 - 21s - loss: 0.0945 - accuracy: 0.9664 - val_loss: 0.4406 - val_accuracy: 0.8804 - 21s/epoch - 53ms/step\n",
      "Epoch 129/200\n",
      "390/390 - 21s - loss: 0.0964 - accuracy: 0.9666 - val_loss: 0.3795 - val_accuracy: 0.8899 - 21s/epoch - 54ms/step\n",
      "Epoch 130/200\n",
      "390/390 - 21s - loss: 0.0970 - accuracy: 0.9667 - val_loss: 0.4097 - val_accuracy: 0.8849 - 21s/epoch - 55ms/step\n",
      "Epoch 131/200\n",
      "390/390 - 20s - loss: 0.0954 - accuracy: 0.9660 - val_loss: 0.4915 - val_accuracy: 0.8631 - 20s/epoch - 52ms/step\n",
      "Epoch 132/200\n",
      "390/390 - 22s - loss: 0.0928 - accuracy: 0.9675 - val_loss: 0.3797 - val_accuracy: 0.8871 - 22s/epoch - 56ms/step\n",
      "Epoch 133/200\n",
      "390/390 - 22s - loss: 0.0932 - accuracy: 0.9679 - val_loss: 0.3527 - val_accuracy: 0.8993 - 22s/epoch - 56ms/step\n",
      "Epoch 134/200\n",
      "390/390 - 21s - loss: 0.0899 - accuracy: 0.9694 - val_loss: 0.3698 - val_accuracy: 0.8951 - 21s/epoch - 54ms/step\n",
      "Epoch 135/200\n",
      "390/390 - 21s - loss: 0.0899 - accuracy: 0.9695 - val_loss: 0.3735 - val_accuracy: 0.8979 - 21s/epoch - 55ms/step\n",
      "Epoch 136/200\n",
      "390/390 - 22s - loss: 0.0917 - accuracy: 0.9689 - val_loss: 0.3381 - val_accuracy: 0.8999 - 22s/epoch - 56ms/step\n",
      "Epoch 137/200\n",
      "390/390 - 20s - loss: 0.0904 - accuracy: 0.9688 - val_loss: 0.3641 - val_accuracy: 0.9014 - 20s/epoch - 52ms/step\n",
      "Epoch 138/200\n",
      "390/390 - 21s - loss: 0.0925 - accuracy: 0.9687 - val_loss: 0.3638 - val_accuracy: 0.8965 - 21s/epoch - 55ms/step\n",
      "Epoch 139/200\n",
      "390/390 - 22s - loss: 0.0886 - accuracy: 0.9692 - val_loss: 0.4628 - val_accuracy: 0.8762 - 22s/epoch - 58ms/step\n",
      "Epoch 140/200\n",
      "390/390 - 21s - loss: 0.0904 - accuracy: 0.9682 - val_loss: 0.5564 - val_accuracy: 0.8546 - 21s/epoch - 53ms/step\n",
      "Epoch 141/200\n",
      "390/390 - 21s - loss: 0.0898 - accuracy: 0.9678 - val_loss: 0.3679 - val_accuracy: 0.8924 - 21s/epoch - 53ms/step\n",
      "Epoch 142/200\n",
      "390/390 - 22s - loss: 0.0851 - accuracy: 0.9708 - val_loss: 0.3358 - val_accuracy: 0.9038 - 22s/epoch - 57ms/step\n",
      "Epoch 143/200\n",
      "390/390 - 21s - loss: 0.0846 - accuracy: 0.9702 - val_loss: 0.3465 - val_accuracy: 0.9039 - 21s/epoch - 55ms/step\n",
      "Epoch 144/200\n",
      "390/390 - 20s - loss: 0.0840 - accuracy: 0.9701 - val_loss: 0.4295 - val_accuracy: 0.8855 - 20s/epoch - 52ms/step\n",
      "Epoch 145/200\n",
      "390/390 - 20s - loss: 0.0850 - accuracy: 0.9704 - val_loss: 0.3859 - val_accuracy: 0.8936 - 20s/epoch - 52ms/step\n",
      "Epoch 146/200\n",
      "390/390 - 22s - loss: 0.0854 - accuracy: 0.9705 - val_loss: 0.3398 - val_accuracy: 0.9033 - 22s/epoch - 57ms/step\n",
      "Epoch 147/200\n",
      "390/390 - 20s - loss: 0.0844 - accuracy: 0.9707 - val_loss: 0.4529 - val_accuracy: 0.8757 - 20s/epoch - 52ms/step\n",
      "Epoch 148/200\n",
      "390/390 - 22s - loss: 0.0791 - accuracy: 0.9735 - val_loss: 0.3578 - val_accuracy: 0.8988 - 22s/epoch - 56ms/step\n",
      "Epoch 149/200\n",
      "390/390 - 21s - loss: 0.0826 - accuracy: 0.9715 - val_loss: 0.3480 - val_accuracy: 0.9004 - 21s/epoch - 55ms/step\n",
      "Epoch 150/200\n",
      "390/390 - 20s - loss: 0.0767 - accuracy: 0.9738 - val_loss: 0.3208 - val_accuracy: 0.9081 - 20s/epoch - 52ms/step\n",
      "Epoch 151/200\n",
      "390/390 - 20s - loss: 0.0756 - accuracy: 0.9740 - val_loss: 0.3133 - val_accuracy: 0.9129 - 20s/epoch - 52ms/step\n",
      "Epoch 152/200\n",
      "390/390 - 21s - loss: 0.0764 - accuracy: 0.9736 - val_loss: 0.3992 - val_accuracy: 0.8914 - 21s/epoch - 55ms/step\n",
      "Epoch 153/200\n",
      "390/390 - 20s - loss: 0.0736 - accuracy: 0.9749 - val_loss: 0.4420 - val_accuracy: 0.8809 - 20s/epoch - 52ms/step\n",
      "Epoch 154/200\n",
      "390/390 - 20s - loss: 0.0792 - accuracy: 0.9726 - val_loss: 0.6046 - val_accuracy: 0.8417 - 20s/epoch - 52ms/step\n",
      "Epoch 155/200\n",
      "390/390 - 20s - loss: 0.0735 - accuracy: 0.9742 - val_loss: 0.3707 - val_accuracy: 0.8950 - 20s/epoch - 52ms/step\n",
      "Epoch 156/200\n",
      "390/390 - 22s - loss: 0.0769 - accuracy: 0.9738 - val_loss: 0.3909 - val_accuracy: 0.8917 - 22s/epoch - 55ms/step\n",
      "Epoch 157/200\n",
      "390/390 - 20s - loss: 0.0710 - accuracy: 0.9752 - val_loss: 0.4366 - val_accuracy: 0.8849 - 20s/epoch - 52ms/step\n",
      "Epoch 158/200\n",
      "390/390 - 20s - loss: 0.0698 - accuracy: 0.9763 - val_loss: 0.4720 - val_accuracy: 0.8677 - 20s/epoch - 52ms/step\n",
      "Epoch 159/200\n",
      "390/390 - 21s - loss: 0.0691 - accuracy: 0.9761 - val_loss: 0.3547 - val_accuracy: 0.8983 - 21s/epoch - 55ms/step\n",
      "Epoch 160/200\n",
      "390/390 - 21s - loss: 0.0692 - accuracy: 0.9759 - val_loss: 0.3302 - val_accuracy: 0.9080 - 21s/epoch - 54ms/step\n",
      "Epoch 161/200\n",
      "390/390 - 22s - loss: 0.0687 - accuracy: 0.9761 - val_loss: 0.3327 - val_accuracy: 0.9080 - 22s/epoch - 57ms/step\n",
      "Epoch 162/200\n",
      "390/390 - 21s - loss: 0.0663 - accuracy: 0.9767 - val_loss: 0.4144 - val_accuracy: 0.8908 - 21s/epoch - 55ms/step\n",
      "Epoch 163/200\n",
      "390/390 - 21s - loss: 0.0633 - accuracy: 0.9789 - val_loss: 0.3386 - val_accuracy: 0.9029 - 21s/epoch - 55ms/step\n",
      "Epoch 164/200\n",
      "390/390 - 20s - loss: 0.0642 - accuracy: 0.9786 - val_loss: 0.3488 - val_accuracy: 0.9006 - 20s/epoch - 52ms/step\n",
      "Epoch 165/200\n",
      "390/390 - 21s - loss: 0.0612 - accuracy: 0.9792 - val_loss: 0.3294 - val_accuracy: 0.9066 - 21s/epoch - 55ms/step\n",
      "Epoch 166/200\n",
      "390/390 - 20s - loss: 0.0582 - accuracy: 0.9797 - val_loss: 0.2993 - val_accuracy: 0.9133 - 20s/epoch - 52ms/step\n",
      "Epoch 167/200\n",
      "390/390 - 20s - loss: 0.0545 - accuracy: 0.9822 - val_loss: 0.3180 - val_accuracy: 0.9093 - 20s/epoch - 52ms/step\n",
      "Epoch 168/200\n",
      "390/390 - 21s - loss: 0.0559 - accuracy: 0.9813 - val_loss: 0.3335 - val_accuracy: 0.9055 - 21s/epoch - 54ms/step\n",
      "Epoch 169/200\n",
      "390/390 - 21s - loss: 0.0544 - accuracy: 0.9814 - val_loss: 0.3315 - val_accuracy: 0.9057 - 21s/epoch - 55ms/step\n",
      "Epoch 170/200\n",
      "390/390 - 21s - loss: 0.0531 - accuracy: 0.9816 - val_loss: 0.3320 - val_accuracy: 0.9097 - 21s/epoch - 55ms/step\n",
      "Epoch 171/200\n",
      "390/390 - 21s - loss: 0.0542 - accuracy: 0.9818 - val_loss: 0.3287 - val_accuracy: 0.9047 - 21s/epoch - 55ms/step\n",
      "Epoch 172/200\n",
      "390/390 - 21s - loss: 0.0499 - accuracy: 0.9834 - val_loss: 0.2794 - val_accuracy: 0.9188 - 21s/epoch - 54ms/step\n",
      "Epoch 173/200\n",
      "390/390 - 22s - loss: 0.0441 - accuracy: 0.9855 - val_loss: 0.3719 - val_accuracy: 0.8987 - 22s/epoch - 57ms/step\n",
      "Epoch 174/200\n",
      "390/390 - 22s - loss: 0.0473 - accuracy: 0.9846 - val_loss: 0.3094 - val_accuracy: 0.9104 - 22s/epoch - 56ms/step\n",
      "Epoch 175/200\n",
      "390/390 - 20s - loss: 0.0428 - accuracy: 0.9861 - val_loss: 0.2814 - val_accuracy: 0.9206 - 20s/epoch - 52ms/step\n",
      "Epoch 176/200\n",
      "390/390 - 22s - loss: 0.0426 - accuracy: 0.9861 - val_loss: 0.3262 - val_accuracy: 0.9127 - 22s/epoch - 56ms/step\n",
      "Epoch 177/200\n",
      "390/390 - 20s - loss: 0.0424 - accuracy: 0.9872 - val_loss: 0.3258 - val_accuracy: 0.9109 - 20s/epoch - 52ms/step\n",
      "Epoch 178/200\n",
      "390/390 - 20s - loss: 0.0402 - accuracy: 0.9871 - val_loss: 0.2737 - val_accuracy: 0.9211 - 20s/epoch - 52ms/step\n",
      "Epoch 179/200\n",
      "390/390 - 22s - loss: 0.0350 - accuracy: 0.9888 - val_loss: 0.2715 - val_accuracy: 0.9210 - 22s/epoch - 55ms/step\n",
      "Epoch 180/200\n",
      "390/390 - 20s - loss: 0.0340 - accuracy: 0.9899 - val_loss: 0.2846 - val_accuracy: 0.9193 - 20s/epoch - 52ms/step\n",
      "Epoch 181/200\n",
      "390/390 - 20s - loss: 0.0308 - accuracy: 0.9905 - val_loss: 0.2807 - val_accuracy: 0.9194 - 20s/epoch - 52ms/step\n",
      "Epoch 182/200\n",
      "390/390 - 21s - loss: 0.0287 - accuracy: 0.9916 - val_loss: 0.2572 - val_accuracy: 0.9270 - 21s/epoch - 55ms/step\n",
      "Epoch 183/200\n",
      "390/390 - 20s - loss: 0.0257 - accuracy: 0.9925 - val_loss: 0.2620 - val_accuracy: 0.9241 - 20s/epoch - 52ms/step\n",
      "Epoch 184/200\n",
      "390/390 - 21s - loss: 0.0250 - accuracy: 0.9934 - val_loss: 0.2427 - val_accuracy: 0.9313 - 21s/epoch - 55ms/step\n",
      "Epoch 185/200\n",
      "390/390 - 21s - loss: 0.0233 - accuracy: 0.9933 - val_loss: 0.2548 - val_accuracy: 0.9270 - 21s/epoch - 54ms/step\n",
      "Epoch 186/200\n",
      "390/390 - 20s - loss: 0.0212 - accuracy: 0.9945 - val_loss: 0.2621 - val_accuracy: 0.9252 - 20s/epoch - 52ms/step\n",
      "Epoch 187/200\n",
      "390/390 - 21s - loss: 0.0187 - accuracy: 0.9953 - val_loss: 0.2490 - val_accuracy: 0.9292 - 21s/epoch - 54ms/step\n",
      "Epoch 188/200\n",
      "390/390 - 22s - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2453 - val_accuracy: 0.9295 - 22s/epoch - 57ms/step\n",
      "Epoch 189/200\n",
      "390/390 - 20s - loss: 0.0183 - accuracy: 0.9962 - val_loss: 0.2144 - val_accuracy: 0.9338 - 20s/epoch - 52ms/step\n",
      "Epoch 190/200\n",
      "390/390 - 21s - loss: 0.0160 - accuracy: 0.9969 - val_loss: 0.2220 - val_accuracy: 0.9353 - 21s/epoch - 55ms/step\n",
      "Epoch 191/200\n",
      "390/390 - 22s - loss: 0.0164 - accuracy: 0.9967 - val_loss: 0.2198 - val_accuracy: 0.9341 - 22s/epoch - 57ms/step\n",
      "Epoch 192/200\n",
      "390/390 - 22s - loss: 0.0154 - accuracy: 0.9973 - val_loss: 0.2137 - val_accuracy: 0.9355 - 22s/epoch - 57ms/step\n",
      "Epoch 193/200\n",
      "390/390 - 22s - loss: 0.0166 - accuracy: 0.9969 - val_loss: 0.2091 - val_accuracy: 0.9362 - 22s/epoch - 57ms/step\n",
      "Epoch 194/200\n",
      "390/390 - 20s - loss: 0.0153 - accuracy: 0.9979 - val_loss: 0.2109 - val_accuracy: 0.9358 - 20s/epoch - 52ms/step\n",
      "Epoch 195/200\n",
      "390/390 - 21s - loss: 0.0159 - accuracy: 0.9985 - val_loss: 0.2008 - val_accuracy: 0.9380 - 21s/epoch - 55ms/step\n",
      "Epoch 196/200\n",
      "390/390 - 21s - loss: 0.0181 - accuracy: 0.9980 - val_loss: 0.1976 - val_accuracy: 0.9378 - 21s/epoch - 54ms/step\n",
      "Epoch 197/200\n",
      "390/390 - 21s - loss: 0.0199 - accuracy: 0.9982 - val_loss: 0.1976 - val_accuracy: 0.9375 - 21s/epoch - 53ms/step\n",
      "Epoch 198/200\n",
      "390/390 - 22s - loss: 0.0221 - accuracy: 0.9983 - val_loss: 0.1935 - val_accuracy: 0.9383 - 22s/epoch - 56ms/step\n",
      "Epoch 199/200\n",
      "390/390 - 21s - loss: 0.0267 - accuracy: 0.9980 - val_loss: 0.1951 - val_accuracy: 0.9391 - 21s/epoch - 55ms/step\n",
      "Epoch 200/200\n",
      "390/390 - 21s - loss: 0.0314 - accuracy: 0.9980 - val_loss: 0.1963 - val_accuracy: 0.9385 - 21s/epoch - 54ms/step\n",
      " Using best val_acc=0.9391 from last 20 epochs\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.1951 - accuracy: 0.9391\n",
      " Finished: Lam=1.0, Repeat=3, Acc=0.9391\n",
      "\n",
      " lambda: Lam=0.9, Repeat=1/3\n",
      "25 0\n",
      "44 1\n",
      "40 2\n",
      "78 0\n",
      "83 1\n",
      "77 2\n",
      "156 0\n",
      "152 1\n",
      "154 2\n",
      "269 0\n",
      "367 1\n",
      "270 2\n",
      "199343756 561714176\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "390/390 - 24s - loss: 0.8601 - accuracy: 0.7157 - val_loss: 2.7571 - val_accuracy: 0.3810 - 24s/epoch - 62ms/step\n",
      "Epoch 2/200\n",
      "390/390 - 21s - loss: 0.6597 - accuracy: 0.7769 - val_loss: 2.2355 - val_accuracy: 0.4841 - 21s/epoch - 54ms/step\n",
      "Epoch 3/200\n",
      "390/390 - 21s - loss: 0.5617 - accuracy: 0.8091 - val_loss: 0.7927 - val_accuracy: 0.7427 - 21s/epoch - 55ms/step\n",
      "Epoch 4/200\n",
      "390/390 - 22s - loss: 0.4984 - accuracy: 0.8314 - val_loss: 1.3957 - val_accuracy: 0.6403 - 22s/epoch - 55ms/step\n",
      "Epoch 5/200\n",
      "390/390 - 20s - loss: 0.4536 - accuracy: 0.8445 - val_loss: 1.0387 - val_accuracy: 0.6838 - 20s/epoch - 52ms/step\n",
      "Epoch 6/200\n",
      "390/390 - 21s - loss: 0.4102 - accuracy: 0.8588 - val_loss: 0.7868 - val_accuracy: 0.7746 - 21s/epoch - 55ms/step\n",
      "Epoch 7/200\n",
      "390/390 - 22s - loss: 0.3582 - accuracy: 0.8759 - val_loss: 0.5158 - val_accuracy: 0.8327 - 22s/epoch - 55ms/step\n",
      "Epoch 8/200\n",
      "390/390 - 20s - loss: 0.3263 - accuracy: 0.8883 - val_loss: 0.5305 - val_accuracy: 0.8361 - 20s/epoch - 51ms/step\n",
      "Epoch 9/200\n",
      "390/390 - 22s - loss: 0.3069 - accuracy: 0.8944 - val_loss: 0.4002 - val_accuracy: 0.8700 - 22s/epoch - 55ms/step\n",
      "Epoch 10/200\n",
      "390/390 - 20s - loss: 0.2878 - accuracy: 0.9003 - val_loss: 0.4956 - val_accuracy: 0.8411 - 20s/epoch - 52ms/step\n",
      "Epoch 11/200\n",
      "390/390 - 21s - loss: 0.2669 - accuracy: 0.9083 - val_loss: 0.4925 - val_accuracy: 0.8400 - 21s/epoch - 55ms/step\n",
      "Epoch 12/200\n",
      "390/390 - 21s - loss: 0.2579 - accuracy: 0.9110 - val_loss: 0.4383 - val_accuracy: 0.8634 - 21s/epoch - 54ms/step\n",
      "Epoch 13/200\n",
      "390/390 - 21s - loss: 0.2440 - accuracy: 0.9164 - val_loss: 0.3547 - val_accuracy: 0.8821 - 21s/epoch - 54ms/step\n",
      "Epoch 14/200\n",
      "390/390 - 22s - loss: 0.2341 - accuracy: 0.9176 - val_loss: 0.4272 - val_accuracy: 0.8546 - 22s/epoch - 57ms/step\n",
      "Epoch 15/200\n",
      "390/390 - 21s - loss: 0.2258 - accuracy: 0.9217 - val_loss: 0.4480 - val_accuracy: 0.8570 - 21s/epoch - 55ms/step\n",
      "Epoch 16/200\n",
      "390/390 - 22s - loss: 0.2202 - accuracy: 0.9227 - val_loss: 0.4911 - val_accuracy: 0.8464 - 22s/epoch - 57ms/step\n",
      "Epoch 17/200\n",
      "390/390 - 21s - loss: 0.2145 - accuracy: 0.9260 - val_loss: 0.4720 - val_accuracy: 0.8506 - 21s/epoch - 55ms/step\n",
      "Epoch 18/200\n",
      "390/390 - 20s - loss: 0.2058 - accuracy: 0.9281 - val_loss: 0.4369 - val_accuracy: 0.8623 - 20s/epoch - 51ms/step\n",
      "Epoch 19/200\n",
      "390/390 - 21s - loss: 0.2011 - accuracy: 0.9289 - val_loss: 0.3834 - val_accuracy: 0.8747 - 21s/epoch - 54ms/step\n",
      "Epoch 20/200\n",
      "390/390 - 21s - loss: 0.1950 - accuracy: 0.9323 - val_loss: 0.5224 - val_accuracy: 0.8508 - 21s/epoch - 55ms/step\n",
      "Epoch 21/200\n",
      "390/390 - 21s - loss: 0.1900 - accuracy: 0.9333 - val_loss: 0.6024 - val_accuracy: 0.8342 - 21s/epoch - 54ms/step\n",
      "Epoch 22/200\n",
      "390/390 - 20s - loss: 0.1850 - accuracy: 0.9356 - val_loss: 0.4177 - val_accuracy: 0.8756 - 20s/epoch - 52ms/step\n",
      "Epoch 23/200\n",
      "390/390 - 20s - loss: 0.1766 - accuracy: 0.9385 - val_loss: 0.4115 - val_accuracy: 0.8775 - 20s/epoch - 52ms/step\n",
      "Epoch 24/200\n",
      "390/390 - 20s - loss: 0.1794 - accuracy: 0.9378 - val_loss: 0.3594 - val_accuracy: 0.8870 - 20s/epoch - 52ms/step\n",
      "Epoch 25/200\n",
      "390/390 - 21s - loss: 0.1768 - accuracy: 0.9378 - val_loss: 0.4258 - val_accuracy: 0.8741 - 21s/epoch - 54ms/step\n",
      "Epoch 26/200\n",
      "390/390 - 20s - loss: 0.1699 - accuracy: 0.9411 - val_loss: 0.3421 - val_accuracy: 0.8908 - 20s/epoch - 52ms/step\n",
      "Epoch 27/200\n",
      "390/390 - 21s - loss: 0.1678 - accuracy: 0.9414 - val_loss: 0.3506 - val_accuracy: 0.8899 - 21s/epoch - 54ms/step\n",
      "Epoch 28/200\n",
      "390/390 - 20s - loss: 0.1615 - accuracy: 0.9437 - val_loss: 0.4822 - val_accuracy: 0.8586 - 20s/epoch - 52ms/step\n",
      "Epoch 29/200\n",
      "390/390 - 20s - loss: 0.1629 - accuracy: 0.9435 - val_loss: 0.4121 - val_accuracy: 0.8719 - 20s/epoch - 52ms/step\n",
      "Epoch 30/200\n",
      "390/390 - 21s - loss: 0.1590 - accuracy: 0.9450 - val_loss: 0.4365 - val_accuracy: 0.8689 - 21s/epoch - 54ms/step\n",
      "Epoch 31/200\n",
      "390/390 - 22s - loss: 0.1595 - accuracy: 0.9436 - val_loss: 0.4250 - val_accuracy: 0.8793 - 22s/epoch - 55ms/step\n",
      "Epoch 32/200\n",
      "390/390 - 20s - loss: 0.1574 - accuracy: 0.9456 - val_loss: 0.4532 - val_accuracy: 0.8588 - 20s/epoch - 52ms/step\n",
      "Epoch 33/200\n",
      "390/390 - 21s - loss: 0.1541 - accuracy: 0.9451 - val_loss: 0.4572 - val_accuracy: 0.8609 - 21s/epoch - 55ms/step\n",
      "Epoch 34/200\n",
      "390/390 - 21s - loss: 0.1509 - accuracy: 0.9476 - val_loss: 0.4581 - val_accuracy: 0.8670 - 21s/epoch - 54ms/step\n",
      "Epoch 35/200\n",
      "390/390 - 21s - loss: 0.1458 - accuracy: 0.9502 - val_loss: 0.4944 - val_accuracy: 0.8546 - 21s/epoch - 55ms/step\n",
      "Epoch 36/200\n",
      "390/390 - 21s - loss: 0.1458 - accuracy: 0.9487 - val_loss: 0.3508 - val_accuracy: 0.8928 - 21s/epoch - 54ms/step\n",
      "Epoch 37/200\n",
      "390/390 - 21s - loss: 0.1511 - accuracy: 0.9457 - val_loss: 0.3677 - val_accuracy: 0.8868 - 21s/epoch - 55ms/step\n",
      "Epoch 38/200\n",
      "390/390 - 20s - loss: 0.1438 - accuracy: 0.9500 - val_loss: 0.3316 - val_accuracy: 0.8991 - 20s/epoch - 53ms/step\n",
      "Epoch 39/200\n",
      "390/390 - 22s - loss: 0.1439 - accuracy: 0.9497 - val_loss: 0.5733 - val_accuracy: 0.8457 - 22s/epoch - 55ms/step\n",
      "Epoch 40/200\n",
      "390/390 - 21s - loss: 0.1429 - accuracy: 0.9497 - val_loss: 0.4210 - val_accuracy: 0.8802 - 21s/epoch - 54ms/step\n",
      "Epoch 41/200\n",
      "390/390 - 22s - loss: 0.1439 - accuracy: 0.9501 - val_loss: 0.4739 - val_accuracy: 0.8618 - 22s/epoch - 56ms/step\n",
      "Epoch 42/200\n",
      "390/390 - 21s - loss: 0.1416 - accuracy: 0.9497 - val_loss: 0.3312 - val_accuracy: 0.9018 - 21s/epoch - 54ms/step\n",
      "Epoch 43/200\n",
      "390/390 - 20s - loss: 0.1375 - accuracy: 0.9523 - val_loss: 0.3310 - val_accuracy: 0.8973 - 20s/epoch - 52ms/step\n",
      "Epoch 44/200\n",
      "390/390 - 20s - loss: 0.1336 - accuracy: 0.9530 - val_loss: 0.3502 - val_accuracy: 0.8942 - 20s/epoch - 52ms/step\n",
      "Epoch 45/200\n",
      "390/390 - 20s - loss: 0.1352 - accuracy: 0.9523 - val_loss: 0.3938 - val_accuracy: 0.8852 - 20s/epoch - 52ms/step\n",
      "Epoch 46/200\n",
      "390/390 - 21s - loss: 0.1310 - accuracy: 0.9532 - val_loss: 0.3886 - val_accuracy: 0.8897 - 21s/epoch - 55ms/step\n",
      "Epoch 47/200\n",
      "390/390 - 20s - loss: 0.1338 - accuracy: 0.9539 - val_loss: 0.5440 - val_accuracy: 0.8533 - 20s/epoch - 52ms/step\n",
      "Epoch 48/200\n",
      "390/390 - 22s - loss: 0.1300 - accuracy: 0.9545 - val_loss: 0.6296 - val_accuracy: 0.8386 - 22s/epoch - 57ms/step\n",
      "Epoch 49/200\n",
      "390/390 - 21s - loss: 0.1302 - accuracy: 0.9549 - val_loss: 0.5098 - val_accuracy: 0.8588 - 21s/epoch - 55ms/step\n",
      "Epoch 50/200\n",
      "390/390 - 20s - loss: 0.1296 - accuracy: 0.9556 - val_loss: 0.3271 - val_accuracy: 0.9021 - 20s/epoch - 52ms/step\n",
      "Epoch 51/200\n",
      "390/390 - 21s - loss: 0.1263 - accuracy: 0.9556 - val_loss: 0.3654 - val_accuracy: 0.8919 - 21s/epoch - 54ms/step\n",
      "Epoch 52/200\n",
      "390/390 - 20s - loss: 0.1257 - accuracy: 0.9563 - val_loss: 0.5388 - val_accuracy: 0.8496 - 20s/epoch - 52ms/step\n",
      "Epoch 53/200\n",
      "390/390 - 22s - loss: 0.1255 - accuracy: 0.9558 - val_loss: 0.3782 - val_accuracy: 0.8864 - 22s/epoch - 57ms/step\n",
      "Epoch 54/200\n",
      "390/390 - 21s - loss: 0.1274 - accuracy: 0.9556 - val_loss: 0.4019 - val_accuracy: 0.8865 - 21s/epoch - 55ms/step\n",
      "Epoch 55/200\n",
      "390/390 - 21s - loss: 0.1251 - accuracy: 0.9563 - val_loss: 0.4140 - val_accuracy: 0.8790 - 21s/epoch - 55ms/step\n",
      "Epoch 56/200\n",
      "390/390 - 22s - loss: 0.1234 - accuracy: 0.9567 - val_loss: 0.3724 - val_accuracy: 0.8936 - 22s/epoch - 56ms/step\n",
      "Epoch 57/200\n",
      "390/390 - 22s - loss: 0.1217 - accuracy: 0.9569 - val_loss: 0.5044 - val_accuracy: 0.8668 - 22s/epoch - 57ms/step\n",
      "Epoch 58/200\n",
      "390/390 - 22s - loss: 0.1231 - accuracy: 0.9568 - val_loss: 0.4016 - val_accuracy: 0.8822 - 22s/epoch - 56ms/step\n",
      "Epoch 59/200\n",
      "390/390 - 20s - loss: 0.1230 - accuracy: 0.9562 - val_loss: 0.5730 - val_accuracy: 0.8420 - 20s/epoch - 52ms/step\n",
      "Epoch 60/200\n",
      "390/390 - 21s - loss: 0.1195 - accuracy: 0.9585 - val_loss: 0.3870 - val_accuracy: 0.8870 - 21s/epoch - 54ms/step\n",
      "Epoch 61/200\n",
      "390/390 - 21s - loss: 0.1190 - accuracy: 0.9592 - val_loss: 0.4101 - val_accuracy: 0.8771 - 21s/epoch - 55ms/step\n",
      "Epoch 62/200\n",
      "390/390 - 22s - loss: 0.1190 - accuracy: 0.9592 - val_loss: 0.4710 - val_accuracy: 0.8658 - 22s/epoch - 56ms/step\n",
      "Epoch 63/200\n",
      "390/390 - 21s - loss: 0.1200 - accuracy: 0.9581 - val_loss: 0.3980 - val_accuracy: 0.8910 - 21s/epoch - 55ms/step\n",
      "Epoch 64/200\n",
      "390/390 - 20s - loss: 0.1148 - accuracy: 0.9607 - val_loss: 0.4939 - val_accuracy: 0.8643 - 20s/epoch - 52ms/step\n",
      "Epoch 65/200\n",
      "390/390 - 20s - loss: 0.1176 - accuracy: 0.9585 - val_loss: 0.4543 - val_accuracy: 0.8794 - 20s/epoch - 52ms/step\n",
      "Epoch 66/200\n",
      "390/390 - 20s - loss: 0.1146 - accuracy: 0.9597 - val_loss: 0.4238 - val_accuracy: 0.8782 - 20s/epoch - 52ms/step\n",
      "Epoch 67/200\n",
      "390/390 - 22s - loss: 0.1170 - accuracy: 0.9582 - val_loss: 0.4319 - val_accuracy: 0.8765 - 22s/epoch - 57ms/step\n",
      "Epoch 68/200\n",
      "390/390 - 22s - loss: 0.1093 - accuracy: 0.9615 - val_loss: 0.3825 - val_accuracy: 0.8930 - 22s/epoch - 56ms/step\n",
      "Epoch 69/200\n",
      "390/390 - 21s - loss: 0.1172 - accuracy: 0.9594 - val_loss: 0.4068 - val_accuracy: 0.8829 - 21s/epoch - 54ms/step\n",
      "Epoch 70/200\n",
      "390/390 - 21s - loss: 0.1118 - accuracy: 0.9619 - val_loss: 0.4712 - val_accuracy: 0.8696 - 21s/epoch - 55ms/step\n",
      "Epoch 71/200\n",
      "390/390 - 20s - loss: 0.1119 - accuracy: 0.9611 - val_loss: 0.3775 - val_accuracy: 0.8928 - 20s/epoch - 52ms/step\n",
      "Epoch 72/200\n",
      "390/390 - 20s - loss: 0.1135 - accuracy: 0.9605 - val_loss: 0.4088 - val_accuracy: 0.8802 - 20s/epoch - 52ms/step\n",
      "Epoch 73/200\n",
      "390/390 - 20s - loss: 0.1120 - accuracy: 0.9600 - val_loss: 0.4828 - val_accuracy: 0.8693 - 20s/epoch - 52ms/step\n",
      "Epoch 74/200\n",
      "390/390 - 20s - loss: 0.1102 - accuracy: 0.9616 - val_loss: 0.3761 - val_accuracy: 0.8980 - 20s/epoch - 52ms/step\n",
      "Epoch 75/200\n",
      "390/390 - 20s - loss: 0.1091 - accuracy: 0.9607 - val_loss: 0.5068 - val_accuracy: 0.8676 - 20s/epoch - 52ms/step\n",
      "Epoch 76/200\n",
      "390/390 - 21s - loss: 0.1113 - accuracy: 0.9613 - val_loss: 0.4254 - val_accuracy: 0.8803 - 21s/epoch - 53ms/step\n",
      "Epoch 77/200\n",
      "390/390 - 21s - loss: 0.1079 - accuracy: 0.9622 - val_loss: 0.5410 - val_accuracy: 0.8563 - 21s/epoch - 55ms/step\n",
      "Epoch 78/200\n",
      "390/390 - 20s - loss: 0.1078 - accuracy: 0.9614 - val_loss: 0.4513 - val_accuracy: 0.8778 - 20s/epoch - 52ms/step\n",
      "Epoch 79/200\n",
      "390/390 - 20s - loss: 0.1049 - accuracy: 0.9638 - val_loss: 0.4281 - val_accuracy: 0.8741 - 20s/epoch - 52ms/step\n",
      "Epoch 80/200\n",
      "390/390 - 20s - loss: 0.1116 - accuracy: 0.9604 - val_loss: 0.4572 - val_accuracy: 0.8771 - 20s/epoch - 52ms/step\n",
      "Epoch 81/200\n",
      "390/390 - 20s - loss: 0.1040 - accuracy: 0.9638 - val_loss: 0.4069 - val_accuracy: 0.8849 - 20s/epoch - 52ms/step\n",
      "Epoch 82/200\n",
      "390/390 - 22s - loss: 0.1084 - accuracy: 0.9632 - val_loss: 0.4007 - val_accuracy: 0.8890 - 22s/epoch - 55ms/step\n",
      "Epoch 83/200\n",
      "390/390 - 20s - loss: 0.1066 - accuracy: 0.9627 - val_loss: 0.3290 - val_accuracy: 0.9056 - 20s/epoch - 52ms/step\n",
      "Epoch 84/200\n",
      "390/390 - 20s - loss: 0.1066 - accuracy: 0.9630 - val_loss: 0.3631 - val_accuracy: 0.8981 - 20s/epoch - 52ms/step\n",
      "Epoch 85/200\n",
      "390/390 - 21s - loss: 0.1023 - accuracy: 0.9637 - val_loss: 0.3107 - val_accuracy: 0.9039 - 21s/epoch - 54ms/step\n",
      "Epoch 86/200\n",
      "390/390 - 20s - loss: 0.0968 - accuracy: 0.9659 - val_loss: 0.4427 - val_accuracy: 0.8849 - 20s/epoch - 52ms/step\n",
      "Epoch 87/200\n",
      "390/390 - 21s - loss: 0.1036 - accuracy: 0.9639 - val_loss: 0.4330 - val_accuracy: 0.8808 - 21s/epoch - 55ms/step\n",
      "Epoch 88/200\n",
      "390/390 - 21s - loss: 0.1006 - accuracy: 0.9657 - val_loss: 0.3585 - val_accuracy: 0.8968 - 21s/epoch - 55ms/step\n",
      "Epoch 89/200\n",
      "390/390 - 20s - loss: 0.1031 - accuracy: 0.9635 - val_loss: 0.4246 - val_accuracy: 0.8886 - 20s/epoch - 52ms/step\n",
      "Epoch 90/200\n",
      "390/390 - 21s - loss: 0.1020 - accuracy: 0.9647 - val_loss: 0.4401 - val_accuracy: 0.8867 - 21s/epoch - 55ms/step\n",
      "Epoch 91/200\n",
      "390/390 - 21s - loss: 0.1003 - accuracy: 0.9646 - val_loss: 0.4056 - val_accuracy: 0.8854 - 21s/epoch - 54ms/step\n",
      "Epoch 92/200\n",
      "390/390 - 20s - loss: 0.1065 - accuracy: 0.9627 - val_loss: 0.5772 - val_accuracy: 0.8491 - 20s/epoch - 52ms/step\n",
      "Epoch 93/200\n",
      "390/390 - 20s - loss: 0.0973 - accuracy: 0.9660 - val_loss: 0.3770 - val_accuracy: 0.8940 - 20s/epoch - 52ms/step\n",
      "Epoch 94/200\n",
      "390/390 - 22s - loss: 0.0958 - accuracy: 0.9671 - val_loss: 0.3934 - val_accuracy: 0.8880 - 22s/epoch - 55ms/step\n",
      "Epoch 95/200\n",
      "390/390 - 20s - loss: 0.1000 - accuracy: 0.9651 - val_loss: 0.4011 - val_accuracy: 0.8848 - 20s/epoch - 52ms/step\n",
      "Epoch 96/200\n",
      "390/390 - 21s - loss: 0.0992 - accuracy: 0.9652 - val_loss: 0.4166 - val_accuracy: 0.8823 - 21s/epoch - 54ms/step\n",
      "Epoch 97/200\n",
      "390/390 - 20s - loss: 0.0971 - accuracy: 0.9652 - val_loss: 0.3735 - val_accuracy: 0.8880 - 20s/epoch - 52ms/step\n",
      "Epoch 98/200\n",
      "390/390 - 20s - loss: 0.0943 - accuracy: 0.9672 - val_loss: 0.3370 - val_accuracy: 0.9027 - 20s/epoch - 52ms/step\n",
      "Epoch 99/200\n",
      "390/390 - 21s - loss: 0.0988 - accuracy: 0.9656 - val_loss: 0.4322 - val_accuracy: 0.8814 - 21s/epoch - 55ms/step\n",
      "Epoch 100/200\n",
      "390/390 - 22s - loss: 0.0964 - accuracy: 0.9674 - val_loss: 0.4839 - val_accuracy: 0.8738 - 22s/epoch - 56ms/step\n",
      "Epoch 101/200\n",
      "390/390 - 21s - loss: 0.0966 - accuracy: 0.9666 - val_loss: 0.5393 - val_accuracy: 0.8579 - 21s/epoch - 55ms/step\n",
      "Epoch 102/200\n",
      "390/390 - 22s - loss: 0.0942 - accuracy: 0.9671 - val_loss: 0.4522 - val_accuracy: 0.8765 - 22s/epoch - 56ms/step\n",
      "Epoch 103/200\n",
      "390/390 - 20s - loss: 0.0965 - accuracy: 0.9662 - val_loss: 0.5009 - val_accuracy: 0.8656 - 20s/epoch - 52ms/step\n",
      "Epoch 104/200\n",
      "390/390 - 20s - loss: 0.0921 - accuracy: 0.9674 - val_loss: 0.4338 - val_accuracy: 0.8799 - 20s/epoch - 52ms/step\n",
      "Epoch 105/200\n",
      "390/390 - 20s - loss: 0.0924 - accuracy: 0.9677 - val_loss: 0.4683 - val_accuracy: 0.8787 - 20s/epoch - 52ms/step\n",
      "Epoch 106/200\n",
      "390/390 - 22s - loss: 0.0952 - accuracy: 0.9663 - val_loss: 0.3962 - val_accuracy: 0.8898 - 22s/epoch - 55ms/step\n",
      "Epoch 107/200\n",
      "390/390 - 21s - loss: 0.0944 - accuracy: 0.9676 - val_loss: 0.3633 - val_accuracy: 0.8963 - 21s/epoch - 54ms/step\n",
      "Epoch 108/200\n",
      "390/390 - 20s - loss: 0.0891 - accuracy: 0.9689 - val_loss: 0.4171 - val_accuracy: 0.8871 - 20s/epoch - 52ms/step\n",
      "Epoch 109/200\n",
      "390/390 - 21s - loss: 0.0881 - accuracy: 0.9696 - val_loss: 0.4359 - val_accuracy: 0.8784 - 21s/epoch - 53ms/step\n",
      "Epoch 110/200\n",
      "390/390 - 22s - loss: 0.0901 - accuracy: 0.9681 - val_loss: 0.4357 - val_accuracy: 0.8876 - 22s/epoch - 56ms/step\n",
      "Epoch 111/200\n",
      "390/390 - 20s - loss: 0.0904 - accuracy: 0.9679 - val_loss: 0.3608 - val_accuracy: 0.8959 - 20s/epoch - 52ms/step\n",
      "Epoch 112/200\n",
      "390/390 - 21s - loss: 0.0888 - accuracy: 0.9689 - val_loss: 0.3738 - val_accuracy: 0.8978 - 21s/epoch - 54ms/step\n",
      "Epoch 113/200\n",
      "390/390 - 21s - loss: 0.0864 - accuracy: 0.9698 - val_loss: 0.3967 - val_accuracy: 0.8905 - 21s/epoch - 53ms/step\n",
      "Epoch 114/200\n",
      "390/390 - 20s - loss: 0.0868 - accuracy: 0.9697 - val_loss: 0.4368 - val_accuracy: 0.8819 - 20s/epoch - 52ms/step\n",
      "Epoch 115/200\n",
      "390/390 - 21s - loss: 0.0861 - accuracy: 0.9699 - val_loss: 0.4914 - val_accuracy: 0.8682 - 21s/epoch - 55ms/step\n",
      "Epoch 116/200\n",
      "390/390 - 20s - loss: 0.0903 - accuracy: 0.9682 - val_loss: 0.3915 - val_accuracy: 0.8907 - 20s/epoch - 52ms/step\n",
      "Epoch 117/200\n",
      "390/390 - 21s - loss: 0.0894 - accuracy: 0.9688 - val_loss: 0.3274 - val_accuracy: 0.9074 - 21s/epoch - 54ms/step\n",
      "Epoch 118/200\n",
      "390/390 - 21s - loss: 0.0899 - accuracy: 0.9688 - val_loss: 0.3811 - val_accuracy: 0.8910 - 21s/epoch - 55ms/step\n",
      "Epoch 119/200\n",
      "390/390 - 20s - loss: 0.0865 - accuracy: 0.9706 - val_loss: 0.3920 - val_accuracy: 0.8940 - 20s/epoch - 52ms/step\n",
      "Epoch 120/200\n",
      "390/390 - 21s - loss: 0.0860 - accuracy: 0.9698 - val_loss: 0.3638 - val_accuracy: 0.8998 - 21s/epoch - 54ms/step\n",
      "Epoch 121/200\n",
      "390/390 - 20s - loss: 0.0809 - accuracy: 0.9716 - val_loss: 0.3533 - val_accuracy: 0.9046 - 20s/epoch - 52ms/step\n",
      "Epoch 122/200\n",
      "390/390 - 20s - loss: 0.0878 - accuracy: 0.9701 - val_loss: 0.4131 - val_accuracy: 0.8869 - 20s/epoch - 52ms/step\n",
      "Epoch 123/200\n",
      "390/390 - 21s - loss: 0.0782 - accuracy: 0.9735 - val_loss: 0.3455 - val_accuracy: 0.9078 - 21s/epoch - 54ms/step\n",
      "Epoch 124/200\n",
      "390/390 - 22s - loss: 0.0824 - accuracy: 0.9713 - val_loss: 0.4910 - val_accuracy: 0.8702 - 22s/epoch - 56ms/step\n",
      "Epoch 125/200\n",
      "390/390 - 20s - loss: 0.0848 - accuracy: 0.9710 - val_loss: 0.3980 - val_accuracy: 0.8896 - 20s/epoch - 52ms/step\n",
      "Epoch 126/200\n",
      "390/390 - 21s - loss: 0.0827 - accuracy: 0.9714 - val_loss: 0.3678 - val_accuracy: 0.8971 - 21s/epoch - 53ms/step\n",
      "Epoch 127/200\n",
      "390/390 - 20s - loss: 0.0802 - accuracy: 0.9717 - val_loss: 0.3899 - val_accuracy: 0.8937 - 20s/epoch - 53ms/step\n",
      "Epoch 128/200\n",
      "390/390 - 21s - loss: 0.0830 - accuracy: 0.9719 - val_loss: 0.4283 - val_accuracy: 0.8824 - 21s/epoch - 54ms/step\n",
      "Epoch 129/200\n",
      "390/390 - 21s - loss: 0.0821 - accuracy: 0.9717 - val_loss: 0.7382 - val_accuracy: 0.8135 - 21s/epoch - 54ms/step\n",
      "Epoch 130/200\n",
      "390/390 - 21s - loss: 0.0773 - accuracy: 0.9731 - val_loss: 0.3682 - val_accuracy: 0.8970 - 21s/epoch - 55ms/step\n",
      "Epoch 131/200\n",
      "390/390 - 20s - loss: 0.0759 - accuracy: 0.9738 - val_loss: 0.3334 - val_accuracy: 0.9068 - 20s/epoch - 51ms/step\n",
      "Epoch 132/200\n",
      "390/390 - 21s - loss: 0.0814 - accuracy: 0.9713 - val_loss: 0.3526 - val_accuracy: 0.8979 - 21s/epoch - 53ms/step\n",
      "Epoch 133/200\n",
      "390/390 - 21s - loss: 0.0727 - accuracy: 0.9756 - val_loss: 0.4830 - val_accuracy: 0.8716 - 21s/epoch - 53ms/step\n",
      "Epoch 134/200\n",
      "390/390 - 21s - loss: 0.0772 - accuracy: 0.9734 - val_loss: 0.4120 - val_accuracy: 0.8879 - 21s/epoch - 53ms/step\n",
      "Epoch 135/200\n",
      "390/390 - 22s - loss: 0.0788 - accuracy: 0.9722 - val_loss: 0.3574 - val_accuracy: 0.8966 - 22s/epoch - 56ms/step\n",
      "Epoch 136/200\n",
      "390/390 - 21s - loss: 0.0811 - accuracy: 0.9712 - val_loss: 0.3111 - val_accuracy: 0.9140 - 21s/epoch - 54ms/step\n",
      "Epoch 137/200\n",
      "390/390 - 21s - loss: 0.0752 - accuracy: 0.9739 - val_loss: 0.3669 - val_accuracy: 0.8980 - 21s/epoch - 54ms/step\n",
      "Epoch 138/200\n",
      "390/390 - 21s - loss: 0.0757 - accuracy: 0.9736 - val_loss: 0.4236 - val_accuracy: 0.8828 - 21s/epoch - 54ms/step\n",
      "Epoch 139/200\n",
      "390/390 - 21s - loss: 0.0769 - accuracy: 0.9728 - val_loss: 0.4119 - val_accuracy: 0.8902 - 21s/epoch - 53ms/step\n",
      "Epoch 140/200\n",
      "390/390 - 21s - loss: 0.0725 - accuracy: 0.9752 - val_loss: 0.4054 - val_accuracy: 0.8838 - 21s/epoch - 53ms/step\n",
      "Epoch 141/200\n",
      "390/390 - 20s - loss: 0.0727 - accuracy: 0.9747 - val_loss: 0.3393 - val_accuracy: 0.9034 - 20s/epoch - 52ms/step\n",
      "Epoch 142/200\n",
      "390/390 - 22s - loss: 0.0710 - accuracy: 0.9762 - val_loss: 0.3717 - val_accuracy: 0.8956 - 22s/epoch - 55ms/step\n",
      "Epoch 143/200\n",
      "390/390 - 21s - loss: 0.0689 - accuracy: 0.9766 - val_loss: 0.3873 - val_accuracy: 0.9007 - 21s/epoch - 53ms/step\n",
      "Epoch 144/200\n",
      "390/390 - 21s - loss: 0.0691 - accuracy: 0.9762 - val_loss: 0.3664 - val_accuracy: 0.9050 - 21s/epoch - 54ms/step\n",
      "Epoch 145/200\n",
      "390/390 - 21s - loss: 0.0724 - accuracy: 0.9753 - val_loss: 0.3226 - val_accuracy: 0.9127 - 21s/epoch - 54ms/step\n",
      "Epoch 146/200\n",
      "390/390 - 21s - loss: 0.0688 - accuracy: 0.9759 - val_loss: 0.4077 - val_accuracy: 0.8862 - 21s/epoch - 54ms/step\n",
      "Epoch 147/200\n",
      "390/390 - 20s - loss: 0.0668 - accuracy: 0.9768 - val_loss: 0.3193 - val_accuracy: 0.9122 - 20s/epoch - 51ms/step\n",
      "Epoch 148/200\n",
      "390/390 - 20s - loss: 0.0652 - accuracy: 0.9772 - val_loss: 0.2966 - val_accuracy: 0.9160 - 20s/epoch - 52ms/step\n",
      "Epoch 149/200\n",
      "390/390 - 21s - loss: 0.0665 - accuracy: 0.9775 - val_loss: 0.3509 - val_accuracy: 0.9067 - 21s/epoch - 53ms/step\n",
      "Epoch 150/200\n",
      "390/390 - 20s - loss: 0.0668 - accuracy: 0.9771 - val_loss: 0.4016 - val_accuracy: 0.8954 - 20s/epoch - 52ms/step\n",
      "Epoch 151/200\n",
      "390/390 - 21s - loss: 0.0637 - accuracy: 0.9781 - val_loss: 0.3796 - val_accuracy: 0.8986 - 21s/epoch - 53ms/step\n",
      "Epoch 152/200\n",
      "390/390 - 21s - loss: 0.0650 - accuracy: 0.9774 - val_loss: 0.3251 - val_accuracy: 0.9100 - 21s/epoch - 53ms/step\n",
      "Epoch 153/200\n",
      "390/390 - 20s - loss: 0.0616 - accuracy: 0.9793 - val_loss: 0.3435 - val_accuracy: 0.9059 - 20s/epoch - 52ms/step\n",
      "Epoch 154/200\n",
      "390/390 - 21s - loss: 0.0582 - accuracy: 0.9798 - val_loss: 0.3633 - val_accuracy: 0.8986 - 21s/epoch - 53ms/step\n",
      "Epoch 155/200\n",
      "390/390 - 21s - loss: 0.0599 - accuracy: 0.9785 - val_loss: 0.3624 - val_accuracy: 0.9029 - 21s/epoch - 53ms/step\n",
      "Epoch 156/200\n",
      "390/390 - 21s - loss: 0.0625 - accuracy: 0.9786 - val_loss: 0.3565 - val_accuracy: 0.9028 - 21s/epoch - 53ms/step\n",
      "Epoch 157/200\n",
      "390/390 - 22s - loss: 0.0584 - accuracy: 0.9803 - val_loss: 0.3398 - val_accuracy: 0.9095 - 22s/epoch - 55ms/step\n",
      "Epoch 158/200\n",
      "390/390 - 20s - loss: 0.0530 - accuracy: 0.9822 - val_loss: 0.3600 - val_accuracy: 0.9036 - 20s/epoch - 51ms/step\n",
      "Epoch 159/200\n",
      "390/390 - 21s - loss: 0.0548 - accuracy: 0.9815 - val_loss: 0.3896 - val_accuracy: 0.8990 - 21s/epoch - 54ms/step\n",
      "Epoch 160/200\n",
      "390/390 - 21s - loss: 0.0569 - accuracy: 0.9804 - val_loss: 0.3412 - val_accuracy: 0.9035 - 21s/epoch - 54ms/step\n",
      "Epoch 161/200\n",
      "390/390 - 21s - loss: 0.0545 - accuracy: 0.9815 - val_loss: 0.3729 - val_accuracy: 0.9006 - 21s/epoch - 54ms/step\n",
      "Epoch 162/200\n",
      "390/390 - 20s - loss: 0.0517 - accuracy: 0.9825 - val_loss: 0.2912 - val_accuracy: 0.9193 - 20s/epoch - 51ms/step\n",
      "Epoch 163/200\n",
      "390/390 - 22s - loss: 0.0510 - accuracy: 0.9832 - val_loss: 0.3344 - val_accuracy: 0.9066 - 22s/epoch - 55ms/step\n",
      "Epoch 164/200\n",
      "390/390 - 21s - loss: 0.0532 - accuracy: 0.9819 - val_loss: 0.2969 - val_accuracy: 0.9181 - 21s/epoch - 54ms/step\n",
      "Epoch 165/200\n",
      "390/390 - 21s - loss: 0.0507 - accuracy: 0.9831 - val_loss: 0.3006 - val_accuracy: 0.9185 - 21s/epoch - 55ms/step\n",
      "Epoch 166/200\n",
      "390/390 - 20s - loss: 0.0490 - accuracy: 0.9839 - val_loss: 0.3109 - val_accuracy: 0.9133 - 20s/epoch - 51ms/step\n",
      "Epoch 167/200\n",
      "390/390 - 21s - loss: 0.0497 - accuracy: 0.9834 - val_loss: 0.3203 - val_accuracy: 0.9117 - 21s/epoch - 53ms/step\n",
      "Epoch 168/200\n",
      "390/390 - 20s - loss: 0.0460 - accuracy: 0.9845 - val_loss: 0.3232 - val_accuracy: 0.9130 - 20s/epoch - 51ms/step\n",
      "Epoch 169/200\n",
      "390/390 - 21s - loss: 0.0443 - accuracy: 0.9853 - val_loss: 0.3983 - val_accuracy: 0.8956 - 21s/epoch - 54ms/step\n",
      "Epoch 170/200\n",
      "390/390 - 21s - loss: 0.0416 - accuracy: 0.9862 - val_loss: 0.3452 - val_accuracy: 0.9047 - 21s/epoch - 54ms/step\n",
      "Epoch 171/200\n",
      "390/390 - 20s - loss: 0.0395 - accuracy: 0.9871 - val_loss: 0.2904 - val_accuracy: 0.9191 - 20s/epoch - 51ms/step\n",
      "Epoch 172/200\n",
      "390/390 - 21s - loss: 0.0400 - accuracy: 0.9869 - val_loss: 0.3235 - val_accuracy: 0.9143 - 21s/epoch - 54ms/step\n",
      "Epoch 173/200\n",
      "390/390 - 21s - loss: 0.0390 - accuracy: 0.9870 - val_loss: 0.2736 - val_accuracy: 0.9278 - 21s/epoch - 54ms/step\n",
      "Epoch 174/200\n",
      "390/390 - 20s - loss: 0.0362 - accuracy: 0.9883 - val_loss: 0.3071 - val_accuracy: 0.9179 - 20s/epoch - 51ms/step\n",
      "Epoch 175/200\n",
      "390/390 - 20s - loss: 0.0338 - accuracy: 0.9889 - val_loss: 0.3109 - val_accuracy: 0.9172 - 20s/epoch - 52ms/step\n",
      "Epoch 176/200\n",
      "390/390 - 21s - loss: 0.0348 - accuracy: 0.9890 - val_loss: 0.2939 - val_accuracy: 0.9190 - 21s/epoch - 54ms/step\n",
      "Epoch 177/200\n",
      "390/390 - 21s - loss: 0.0304 - accuracy: 0.9903 - val_loss: 0.3025 - val_accuracy: 0.9173 - 21s/epoch - 54ms/step\n",
      "Epoch 178/200\n",
      "390/390 - 21s - loss: 0.0293 - accuracy: 0.9909 - val_loss: 0.3113 - val_accuracy: 0.9162 - 21s/epoch - 54ms/step\n",
      "Epoch 179/200\n",
      "390/390 - 21s - loss: 0.0247 - accuracy: 0.9925 - val_loss: 0.2621 - val_accuracy: 0.9291 - 21s/epoch - 53ms/step\n",
      "Epoch 180/200\n",
      "390/390 - 20s - loss: 0.0257 - accuracy: 0.9921 - val_loss: 0.2413 - val_accuracy: 0.9326 - 20s/epoch - 51ms/step\n",
      "Epoch 181/200\n",
      "390/390 - 21s - loss: 0.0217 - accuracy: 0.9939 - val_loss: 0.2663 - val_accuracy: 0.9286 - 21s/epoch - 53ms/step\n",
      "Epoch 182/200\n",
      "390/390 - 21s - loss: 0.0200 - accuracy: 0.9946 - val_loss: 0.2381 - val_accuracy: 0.9352 - 21s/epoch - 54ms/step\n",
      "Epoch 183/200\n",
      "390/390 - 21s - loss: 0.0183 - accuracy: 0.9950 - val_loss: 0.2460 - val_accuracy: 0.9313 - 21s/epoch - 55ms/step\n",
      "Epoch 184/200\n",
      "390/390 - 21s - loss: 0.0163 - accuracy: 0.9959 - val_loss: 0.2686 - val_accuracy: 0.9265 - 21s/epoch - 54ms/step\n",
      "Epoch 185/200\n",
      "390/390 - 20s - loss: 0.0152 - accuracy: 0.9963 - val_loss: 0.2323 - val_accuracy: 0.9351 - 20s/epoch - 51ms/step\n",
      "Epoch 186/200\n",
      "390/390 - 21s - loss: 0.0135 - accuracy: 0.9971 - val_loss: 0.2153 - val_accuracy: 0.9386 - 21s/epoch - 54ms/step\n",
      "Epoch 187/200\n",
      "390/390 - 21s - loss: 0.0134 - accuracy: 0.9967 - val_loss: 0.2235 - val_accuracy: 0.9377 - 21s/epoch - 54ms/step\n",
      "Epoch 188/200\n",
      "390/390 - 20s - loss: 0.0119 - accuracy: 0.9975 - val_loss: 0.2204 - val_accuracy: 0.9357 - 20s/epoch - 52ms/step\n",
      "Epoch 189/200\n",
      "390/390 - 20s - loss: 0.0124 - accuracy: 0.9975 - val_loss: 0.2186 - val_accuracy: 0.9354 - 20s/epoch - 51ms/step\n",
      "Epoch 190/200\n",
      "390/390 - 21s - loss: 0.0108 - accuracy: 0.9983 - val_loss: 0.2062 - val_accuracy: 0.9410 - 21s/epoch - 53ms/step\n",
      "Epoch 191/200\n",
      "390/390 - 20s - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.2052 - val_accuracy: 0.9403 - 20s/epoch - 52ms/step\n",
      "Epoch 192/200\n",
      "390/390 - 21s - loss: 0.0114 - accuracy: 0.9983 - val_loss: 0.2010 - val_accuracy: 0.9395 - 21s/epoch - 53ms/step\n",
      "Epoch 193/200\n",
      "390/390 - 21s - loss: 0.0116 - accuracy: 0.9985 - val_loss: 0.1951 - val_accuracy: 0.9409 - 21s/epoch - 53ms/step\n",
      "Epoch 194/200\n",
      "390/390 - 20s - loss: 0.0118 - accuracy: 0.9986 - val_loss: 0.2004 - val_accuracy: 0.9398 - 20s/epoch - 52ms/step\n",
      "Epoch 195/200\n",
      "390/390 - 20s - loss: 0.0129 - accuracy: 0.9987 - val_loss: 0.1912 - val_accuracy: 0.9413 - 20s/epoch - 51ms/step\n",
      "Epoch 196/200\n",
      "390/390 - 20s - loss: 0.0142 - accuracy: 0.9988 - val_loss: 0.1900 - val_accuracy: 0.9409 - 20s/epoch - 52ms/step\n",
      "Epoch 197/200\n",
      "390/390 - 20s - loss: 0.0163 - accuracy: 0.9989 - val_loss: 0.1864 - val_accuracy: 0.9408 - 20s/epoch - 51ms/step\n",
      "Epoch 198/200\n",
      "390/390 - 21s - loss: 0.0194 - accuracy: 0.9988 - val_loss: 0.1863 - val_accuracy: 0.9416 - 21s/epoch - 54ms/step\n",
      "Epoch 199/200\n",
      "390/390 - 21s - loss: 0.0229 - accuracy: 0.9988 - val_loss: 0.1865 - val_accuracy: 0.9415 - 21s/epoch - 53ms/step\n",
      "Epoch 200/200\n",
      "390/390 - 21s - loss: 0.0291 - accuracy: 0.9987 - val_loss: 0.1885 - val_accuracy: 0.9421 - 21s/epoch - 54ms/step\n",
      " Using best val_acc=0.9421 from last 20 epochs\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.1885 - accuracy: 0.9421\n",
      " Finished: Lam=0.9, Repeat=1, Acc=0.9421\n",
      "\n",
      " lambda: Lam=0.9, Repeat=2/3\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "199343756 561714176\n",
      "Epoch 1/200\n",
      "390/390 - 23s - loss: 0.8686 - accuracy: 0.7118 - val_loss: 3.2333 - val_accuracy: 0.3133 - 23s/epoch - 59ms/step\n",
      "Epoch 2/200\n",
      "390/390 - 20s - loss: 0.6570 - accuracy: 0.7741 - val_loss: 3.2853 - val_accuracy: 0.3896 - 20s/epoch - 51ms/step\n",
      "Epoch 3/200\n",
      "390/390 - 21s - loss: 0.5670 - accuracy: 0.8089 - val_loss: 1.0850 - val_accuracy: 0.6772 - 21s/epoch - 53ms/step\n",
      "Epoch 4/200\n",
      "390/390 - 22s - loss: 0.4969 - accuracy: 0.8304 - val_loss: 0.9860 - val_accuracy: 0.6903 - 22s/epoch - 55ms/step\n",
      "Epoch 5/200\n",
      "390/390 - 20s - loss: 0.4497 - accuracy: 0.8471 - val_loss: 0.5600 - val_accuracy: 0.8195 - 20s/epoch - 51ms/step\n",
      "Epoch 6/200\n",
      "390/390 - 22s - loss: 0.4099 - accuracy: 0.8596 - val_loss: 0.5851 - val_accuracy: 0.8079 - 22s/epoch - 56ms/step\n",
      "Epoch 7/200\n",
      "390/390 - 20s - loss: 0.3599 - accuracy: 0.8782 - val_loss: 0.5493 - val_accuracy: 0.8217 - 20s/epoch - 51ms/step\n",
      "Epoch 8/200\n",
      "390/390 - 20s - loss: 0.3263 - accuracy: 0.8877 - val_loss: 0.5684 - val_accuracy: 0.8243 - 20s/epoch - 51ms/step\n",
      "Epoch 9/200\n",
      "390/390 - 20s - loss: 0.3024 - accuracy: 0.8953 - val_loss: 0.6059 - val_accuracy: 0.8171 - 20s/epoch - 51ms/step\n",
      "Epoch 10/200\n",
      "390/390 - 21s - loss: 0.2879 - accuracy: 0.9003 - val_loss: 0.5094 - val_accuracy: 0.8398 - 21s/epoch - 53ms/step\n",
      "Epoch 11/200\n",
      "390/390 - 20s - loss: 0.2665 - accuracy: 0.9079 - val_loss: 0.4978 - val_accuracy: 0.8401 - 20s/epoch - 52ms/step\n",
      "Epoch 12/200\n",
      "390/390 - 21s - loss: 0.2572 - accuracy: 0.9094 - val_loss: 0.4203 - val_accuracy: 0.8603 - 21s/epoch - 53ms/step\n",
      "Epoch 13/200\n",
      "390/390 - 21s - loss: 0.2452 - accuracy: 0.9156 - val_loss: 0.4197 - val_accuracy: 0.8653 - 21s/epoch - 54ms/step\n",
      "Epoch 14/200\n",
      "390/390 - 22s - loss: 0.2317 - accuracy: 0.9186 - val_loss: 0.4827 - val_accuracy: 0.8502 - 22s/epoch - 56ms/step\n",
      "Epoch 15/200\n",
      "390/390 - 21s - loss: 0.2250 - accuracy: 0.9215 - val_loss: 0.4262 - val_accuracy: 0.8651 - 21s/epoch - 54ms/step\n",
      "Epoch 16/200\n",
      "390/390 - 21s - loss: 0.2190 - accuracy: 0.9229 - val_loss: 0.4788 - val_accuracy: 0.8602 - 21s/epoch - 53ms/step\n",
      "Epoch 17/200\n",
      "390/390 - 21s - loss: 0.2102 - accuracy: 0.9274 - val_loss: 0.4112 - val_accuracy: 0.8699 - 21s/epoch - 53ms/step\n",
      "Epoch 18/200\n",
      "390/390 - 22s - loss: 0.2040 - accuracy: 0.9300 - val_loss: 0.4931 - val_accuracy: 0.8483 - 22s/epoch - 55ms/step\n",
      "Epoch 19/200\n",
      "390/390 - 22s - loss: 0.2021 - accuracy: 0.9300 - val_loss: 0.4004 - val_accuracy: 0.8736 - 22s/epoch - 55ms/step\n",
      "Epoch 20/200\n",
      "390/390 - 20s - loss: 0.1926 - accuracy: 0.9321 - val_loss: 0.4551 - val_accuracy: 0.8562 - 20s/epoch - 51ms/step\n",
      "Epoch 21/200\n",
      "390/390 - 20s - loss: 0.1871 - accuracy: 0.9344 - val_loss: 0.3734 - val_accuracy: 0.8831 - 20s/epoch - 52ms/step\n",
      "Epoch 22/200\n",
      "390/390 - 21s - loss: 0.1862 - accuracy: 0.9357 - val_loss: 0.3461 - val_accuracy: 0.8871 - 21s/epoch - 53ms/step\n",
      "Epoch 23/200\n",
      "390/390 - 21s - loss: 0.1826 - accuracy: 0.9360 - val_loss: 0.3254 - val_accuracy: 0.8936 - 21s/epoch - 54ms/step\n",
      "Epoch 24/200\n",
      "390/390 - 20s - loss: 0.1762 - accuracy: 0.9386 - val_loss: 0.4567 - val_accuracy: 0.8617 - 20s/epoch - 51ms/step\n",
      "Epoch 25/200\n",
      "390/390 - 20s - loss: 0.1697 - accuracy: 0.9405 - val_loss: 0.4668 - val_accuracy: 0.8605 - 20s/epoch - 52ms/step\n",
      "Epoch 26/200\n",
      "390/390 - 20s - loss: 0.1683 - accuracy: 0.9412 - val_loss: 0.4052 - val_accuracy: 0.8791 - 20s/epoch - 51ms/step\n",
      "Epoch 27/200\n",
      "390/390 - 20s - loss: 0.1687 - accuracy: 0.9411 - val_loss: 0.5148 - val_accuracy: 0.8487 - 20s/epoch - 52ms/step\n",
      "Epoch 28/200\n",
      "390/390 - 21s - loss: 0.1664 - accuracy: 0.9422 - val_loss: 0.5600 - val_accuracy: 0.8408 - 21s/epoch - 54ms/step\n",
      "Epoch 29/200\n",
      "390/390 - 20s - loss: 0.1610 - accuracy: 0.9432 - val_loss: 0.4569 - val_accuracy: 0.8607 - 20s/epoch - 51ms/step\n",
      "Epoch 30/200\n",
      "390/390 - 21s - loss: 0.1539 - accuracy: 0.9462 - val_loss: 0.4887 - val_accuracy: 0.8592 - 21s/epoch - 54ms/step\n",
      "Epoch 31/200\n",
      "390/390 - 21s - loss: 0.1557 - accuracy: 0.9454 - val_loss: 0.4560 - val_accuracy: 0.8696 - 21s/epoch - 53ms/step\n",
      "Epoch 32/200\n",
      "390/390 - 21s - loss: 0.1534 - accuracy: 0.9466 - val_loss: 0.3228 - val_accuracy: 0.8944 - 21s/epoch - 53ms/step\n",
      "Epoch 33/200\n",
      "390/390 - 21s - loss: 0.1569 - accuracy: 0.9449 - val_loss: 0.3924 - val_accuracy: 0.8797 - 21s/epoch - 54ms/step\n",
      "Epoch 34/200\n",
      "390/390 - 21s - loss: 0.1539 - accuracy: 0.9451 - val_loss: 0.4325 - val_accuracy: 0.8680 - 21s/epoch - 54ms/step\n",
      "Epoch 35/200\n",
      "390/390 - 20s - loss: 0.1512 - accuracy: 0.9475 - val_loss: 0.4298 - val_accuracy: 0.8672 - 20s/epoch - 51ms/step\n",
      "Epoch 36/200\n",
      "390/390 - 20s - loss: 0.1486 - accuracy: 0.9479 - val_loss: 0.3198 - val_accuracy: 0.9013 - 20s/epoch - 51ms/step\n",
      "Epoch 37/200\n",
      "390/390 - 20s - loss: 0.1462 - accuracy: 0.9483 - val_loss: 0.3736 - val_accuracy: 0.8859 - 20s/epoch - 51ms/step\n",
      "Epoch 38/200\n",
      "390/390 - 20s - loss: 0.1432 - accuracy: 0.9498 - val_loss: 0.4954 - val_accuracy: 0.8623 - 20s/epoch - 51ms/step\n",
      "Epoch 39/200\n",
      "390/390 - 20s - loss: 0.1432 - accuracy: 0.9496 - val_loss: 0.4091 - val_accuracy: 0.8843 - 20s/epoch - 51ms/step\n",
      "Epoch 40/200\n",
      "390/390 - 20s - loss: 0.1440 - accuracy: 0.9503 - val_loss: 0.4116 - val_accuracy: 0.8764 - 20s/epoch - 51ms/step\n",
      "Epoch 41/200\n",
      "390/390 - 20s - loss: 0.1343 - accuracy: 0.9542 - val_loss: 0.3605 - val_accuracy: 0.8847 - 20s/epoch - 52ms/step\n",
      "Epoch 42/200\n",
      "390/390 - 21s - loss: 0.1364 - accuracy: 0.9519 - val_loss: 0.3925 - val_accuracy: 0.8834 - 21s/epoch - 55ms/step\n",
      "Epoch 43/200\n",
      "390/390 - 21s - loss: 0.1359 - accuracy: 0.9520 - val_loss: 0.5289 - val_accuracy: 0.8528 - 21s/epoch - 54ms/step\n",
      "Epoch 44/200\n",
      "390/390 - 21s - loss: 0.1344 - accuracy: 0.9532 - val_loss: 0.4195 - val_accuracy: 0.8724 - 21s/epoch - 53ms/step\n",
      "Epoch 45/200\n",
      "390/390 - 20s - loss: 0.1327 - accuracy: 0.9535 - val_loss: 0.5194 - val_accuracy: 0.8630 - 20s/epoch - 52ms/step\n",
      "Epoch 46/200\n",
      "390/390 - 22s - loss: 0.1352 - accuracy: 0.9523 - val_loss: 0.4634 - val_accuracy: 0.8660 - 22s/epoch - 55ms/step\n",
      "Epoch 47/200\n",
      "390/390 - 20s - loss: 0.1286 - accuracy: 0.9552 - val_loss: 0.4039 - val_accuracy: 0.8809 - 20s/epoch - 51ms/step\n",
      "Epoch 48/200\n",
      "390/390 - 21s - loss: 0.1285 - accuracy: 0.9543 - val_loss: 0.3664 - val_accuracy: 0.8911 - 21s/epoch - 55ms/step\n",
      "Epoch 49/200\n",
      "390/390 - 21s - loss: 0.1282 - accuracy: 0.9557 - val_loss: 0.6037 - val_accuracy: 0.8457 - 21s/epoch - 55ms/step\n",
      "Epoch 50/200\n",
      "390/390 - 21s - loss: 0.1339 - accuracy: 0.9529 - val_loss: 0.3405 - val_accuracy: 0.8972 - 21s/epoch - 54ms/step\n",
      "Epoch 51/200\n",
      "390/390 - 21s - loss: 0.1254 - accuracy: 0.9563 - val_loss: 0.3762 - val_accuracy: 0.8895 - 21s/epoch - 54ms/step\n",
      "Epoch 52/200\n",
      "390/390 - 21s - loss: 0.1262 - accuracy: 0.9561 - val_loss: 0.4148 - val_accuracy: 0.8841 - 21s/epoch - 53ms/step\n",
      "Epoch 53/200\n",
      "390/390 - 21s - loss: 0.1286 - accuracy: 0.9552 - val_loss: 0.3727 - val_accuracy: 0.8863 - 21s/epoch - 54ms/step\n",
      "Epoch 54/200\n",
      "390/390 - 20s - loss: 0.1212 - accuracy: 0.9570 - val_loss: 0.4171 - val_accuracy: 0.8821 - 20s/epoch - 51ms/step\n",
      "Epoch 55/200\n",
      "390/390 - 20s - loss: 0.1209 - accuracy: 0.9575 - val_loss: 0.4261 - val_accuracy: 0.8727 - 20s/epoch - 51ms/step\n",
      "Epoch 56/200\n",
      "390/390 - 20s - loss: 0.1231 - accuracy: 0.9576 - val_loss: 0.4886 - val_accuracy: 0.8701 - 20s/epoch - 51ms/step\n",
      "Epoch 57/200\n",
      "390/390 - 22s - loss: 0.1221 - accuracy: 0.9573 - val_loss: 0.4265 - val_accuracy: 0.8777 - 22s/epoch - 55ms/step\n",
      "Epoch 58/200\n",
      "390/390 - 21s - loss: 0.1199 - accuracy: 0.9585 - val_loss: 0.3870 - val_accuracy: 0.8864 - 21s/epoch - 54ms/step\n",
      "Epoch 59/200\n",
      "390/390 - 20s - loss: 0.1218 - accuracy: 0.9574 - val_loss: 0.4541 - val_accuracy: 0.8705 - 20s/epoch - 51ms/step\n",
      "Epoch 60/200\n",
      "390/390 - 21s - loss: 0.1258 - accuracy: 0.9554 - val_loss: 0.4972 - val_accuracy: 0.8593 - 21s/epoch - 55ms/step\n",
      "Epoch 61/200\n",
      "390/390 - 21s - loss: 0.1160 - accuracy: 0.9597 - val_loss: 0.5493 - val_accuracy: 0.8584 - 21s/epoch - 54ms/step\n",
      "Epoch 62/200\n",
      "390/390 - 20s - loss: 0.1205 - accuracy: 0.9574 - val_loss: 0.3923 - val_accuracy: 0.8820 - 20s/epoch - 52ms/step\n",
      "Epoch 63/200\n",
      "390/390 - 20s - loss: 0.1190 - accuracy: 0.9585 - val_loss: 0.4323 - val_accuracy: 0.8779 - 20s/epoch - 51ms/step\n",
      "Epoch 64/200\n",
      "390/390 - 21s - loss: 0.1168 - accuracy: 0.9587 - val_loss: 0.3800 - val_accuracy: 0.8900 - 21s/epoch - 54ms/step\n",
      "Epoch 65/200\n",
      "390/390 - 20s - loss: 0.1204 - accuracy: 0.9573 - val_loss: 0.4093 - val_accuracy: 0.8855 - 20s/epoch - 52ms/step\n",
      "Epoch 66/200\n",
      "390/390 - 21s - loss: 0.1128 - accuracy: 0.9612 - val_loss: 0.4002 - val_accuracy: 0.8819 - 21s/epoch - 53ms/step\n",
      "Epoch 67/200\n",
      "390/390 - 21s - loss: 0.1157 - accuracy: 0.9604 - val_loss: 0.4386 - val_accuracy: 0.8787 - 21s/epoch - 54ms/step\n",
      "Epoch 68/200\n",
      "390/390 - 20s - loss: 0.1149 - accuracy: 0.9596 - val_loss: 0.3837 - val_accuracy: 0.8889 - 20s/epoch - 51ms/step\n",
      "Epoch 69/200\n",
      "390/390 - 20s - loss: 0.1126 - accuracy: 0.9606 - val_loss: 0.3268 - val_accuracy: 0.9015 - 20s/epoch - 51ms/step\n",
      "Epoch 70/200\n",
      "390/390 - 21s - loss: 0.1103 - accuracy: 0.9611 - val_loss: 0.3573 - val_accuracy: 0.8965 - 21s/epoch - 55ms/step\n",
      "Epoch 71/200\n",
      "390/390 - 20s - loss: 0.1116 - accuracy: 0.9607 - val_loss: 0.6905 - val_accuracy: 0.8337 - 20s/epoch - 51ms/step\n",
      "Epoch 72/200\n",
      "390/390 - 21s - loss: 0.1110 - accuracy: 0.9614 - val_loss: 0.6098 - val_accuracy: 0.8404 - 21s/epoch - 55ms/step\n",
      "Epoch 73/200\n",
      "390/390 - 21s - loss: 0.1113 - accuracy: 0.9605 - val_loss: 0.3647 - val_accuracy: 0.9005 - 21s/epoch - 54ms/step\n",
      "Epoch 74/200\n",
      "390/390 - 20s - loss: 0.1167 - accuracy: 0.9586 - val_loss: 0.3642 - val_accuracy: 0.8985 - 20s/epoch - 53ms/step\n",
      "Epoch 75/200\n",
      "390/390 - 22s - loss: 0.1089 - accuracy: 0.9616 - val_loss: 0.3800 - val_accuracy: 0.8910 - 22s/epoch - 55ms/step\n",
      "Epoch 76/200\n",
      "390/390 - 21s - loss: 0.1094 - accuracy: 0.9608 - val_loss: 0.4005 - val_accuracy: 0.8906 - 21s/epoch - 53ms/step\n",
      "Epoch 77/200\n",
      "390/390 - 21s - loss: 0.1063 - accuracy: 0.9634 - val_loss: 0.4191 - val_accuracy: 0.8808 - 21s/epoch - 54ms/step\n",
      "Epoch 78/200\n",
      "390/390 - 20s - loss: 0.1086 - accuracy: 0.9616 - val_loss: 0.4287 - val_accuracy: 0.8829 - 20s/epoch - 51ms/step\n",
      "Epoch 79/200\n",
      "390/390 - 20s - loss: 0.1062 - accuracy: 0.9630 - val_loss: 0.3367 - val_accuracy: 0.9018 - 20s/epoch - 51ms/step\n",
      "Epoch 80/200\n",
      "390/390 - 20s - loss: 0.1072 - accuracy: 0.9629 - val_loss: 0.3368 - val_accuracy: 0.9016 - 20s/epoch - 51ms/step\n",
      "Epoch 81/200\n",
      "390/390 - 21s - loss: 0.1027 - accuracy: 0.9648 - val_loss: 0.3676 - val_accuracy: 0.8978 - 21s/epoch - 54ms/step\n",
      "Epoch 82/200\n",
      "390/390 - 21s - loss: 0.1074 - accuracy: 0.9626 - val_loss: 0.4087 - val_accuracy: 0.8845 - 21s/epoch - 54ms/step\n",
      "Epoch 83/200\n",
      "390/390 - 21s - loss: 0.1062 - accuracy: 0.9628 - val_loss: 0.4353 - val_accuracy: 0.8755 - 21s/epoch - 53ms/step\n",
      "Epoch 84/200\n",
      "390/390 - 21s - loss: 0.1033 - accuracy: 0.9641 - val_loss: 0.4038 - val_accuracy: 0.8917 - 21s/epoch - 53ms/step\n",
      "Epoch 85/200\n",
      "390/390 - 21s - loss: 0.0979 - accuracy: 0.9659 - val_loss: 0.3335 - val_accuracy: 0.9017 - 21s/epoch - 54ms/step\n",
      "Epoch 86/200\n",
      "390/390 - 20s - loss: 0.1054 - accuracy: 0.9630 - val_loss: 0.4124 - val_accuracy: 0.8806 - 20s/epoch - 51ms/step\n",
      "Epoch 87/200\n",
      "390/390 - 20s - loss: 0.1044 - accuracy: 0.9632 - val_loss: 0.3732 - val_accuracy: 0.8924 - 20s/epoch - 51ms/step\n",
      "Epoch 88/200\n",
      "390/390 - 21s - loss: 0.0996 - accuracy: 0.9659 - val_loss: 0.3624 - val_accuracy: 0.8948 - 21s/epoch - 53ms/step\n",
      "Epoch 89/200\n",
      "390/390 - 21s - loss: 0.0999 - accuracy: 0.9643 - val_loss: 0.3732 - val_accuracy: 0.8928 - 21s/epoch - 54ms/step\n",
      "Epoch 90/200\n",
      "390/390 - 21s - loss: 0.1008 - accuracy: 0.9651 - val_loss: 0.3750 - val_accuracy: 0.8942 - 21s/epoch - 54ms/step\n",
      "Epoch 91/200\n",
      "390/390 - 22s - loss: 0.1029 - accuracy: 0.9637 - val_loss: 0.3793 - val_accuracy: 0.8988 - 22s/epoch - 57ms/step\n",
      "Epoch 92/200\n",
      "390/390 - 20s - loss: 0.0981 - accuracy: 0.9654 - val_loss: 0.3757 - val_accuracy: 0.8921 - 20s/epoch - 51ms/step\n",
      "Epoch 93/200\n",
      "390/390 - 20s - loss: 0.0982 - accuracy: 0.9650 - val_loss: 0.3501 - val_accuracy: 0.8956 - 20s/epoch - 51ms/step\n",
      "Epoch 94/200\n",
      "390/390 - 20s - loss: 0.0999 - accuracy: 0.9652 - val_loss: 0.4078 - val_accuracy: 0.8882 - 20s/epoch - 51ms/step\n",
      "Epoch 95/200\n",
      "390/390 - 20s - loss: 0.0984 - accuracy: 0.9663 - val_loss: 0.3517 - val_accuracy: 0.8974 - 20s/epoch - 52ms/step\n",
      "Epoch 96/200\n",
      "390/390 - 20s - loss: 0.0946 - accuracy: 0.9669 - val_loss: 0.4479 - val_accuracy: 0.8782 - 20s/epoch - 51ms/step\n",
      "Epoch 97/200\n",
      "390/390 - 22s - loss: 0.0981 - accuracy: 0.9649 - val_loss: 0.3610 - val_accuracy: 0.8949 - 22s/epoch - 56ms/step\n",
      "Epoch 98/200\n",
      "390/390 - 21s - loss: 0.0949 - accuracy: 0.9669 - val_loss: 0.3797 - val_accuracy: 0.8902 - 21s/epoch - 54ms/step\n",
      "Epoch 99/200\n",
      "390/390 - 20s - loss: 0.0954 - accuracy: 0.9669 - val_loss: 0.3416 - val_accuracy: 0.9022 - 20s/epoch - 51ms/step\n",
      "Epoch 100/200\n",
      "390/390 - 20s - loss: 0.1009 - accuracy: 0.9646 - val_loss: 0.3441 - val_accuracy: 0.9028 - 20s/epoch - 52ms/step\n",
      "Epoch 101/200\n",
      "390/390 - 20s - loss: 0.0942 - accuracy: 0.9668 - val_loss: 0.3972 - val_accuracy: 0.8828 - 20s/epoch - 51ms/step\n",
      "Epoch 102/200\n",
      "390/390 - 21s - loss: 0.0933 - accuracy: 0.9674 - val_loss: 0.3233 - val_accuracy: 0.9068 - 21s/epoch - 55ms/step\n",
      "Epoch 103/200\n",
      "390/390 - 20s - loss: 0.0912 - accuracy: 0.9680 - val_loss: 0.3910 - val_accuracy: 0.8912 - 20s/epoch - 52ms/step\n",
      "Epoch 104/200\n",
      "390/390 - 20s - loss: 0.0941 - accuracy: 0.9677 - val_loss: 0.4091 - val_accuracy: 0.8919 - 20s/epoch - 51ms/step\n",
      "Epoch 105/200\n",
      "390/390 - 20s - loss: 0.0946 - accuracy: 0.9666 - val_loss: 0.3450 - val_accuracy: 0.9027 - 20s/epoch - 51ms/step\n",
      "Epoch 106/200\n",
      "390/390 - 22s - loss: 0.0945 - accuracy: 0.9673 - val_loss: 0.5719 - val_accuracy: 0.8467 - 22s/epoch - 56ms/step\n",
      "Epoch 107/200\n",
      "390/390 - 20s - loss: 0.0918 - accuracy: 0.9687 - val_loss: 0.4495 - val_accuracy: 0.8811 - 20s/epoch - 51ms/step\n",
      "Epoch 108/200\n",
      "390/390 - 20s - loss: 0.0891 - accuracy: 0.9691 - val_loss: 0.3561 - val_accuracy: 0.8966 - 20s/epoch - 51ms/step\n",
      "Epoch 109/200\n",
      "390/390 - 21s - loss: 0.0906 - accuracy: 0.9687 - val_loss: 0.4113 - val_accuracy: 0.8883 - 21s/epoch - 53ms/step\n",
      "Epoch 110/200\n",
      "390/390 - 20s - loss: 0.0919 - accuracy: 0.9679 - val_loss: 0.3538 - val_accuracy: 0.8966 - 20s/epoch - 52ms/step\n",
      "Epoch 111/200\n",
      "390/390 - 20s - loss: 0.0909 - accuracy: 0.9681 - val_loss: 0.3183 - val_accuracy: 0.9046 - 20s/epoch - 51ms/step\n",
      "Epoch 112/200\n",
      "390/390 - 21s - loss: 0.0907 - accuracy: 0.9681 - val_loss: 0.4142 - val_accuracy: 0.8856 - 21s/epoch - 53ms/step\n",
      "Epoch 113/200\n",
      "390/390 - 20s - loss: 0.0864 - accuracy: 0.9707 - val_loss: 0.3366 - val_accuracy: 0.9053 - 20s/epoch - 51ms/step\n",
      "Epoch 114/200\n",
      "390/390 - 20s - loss: 0.0841 - accuracy: 0.9709 - val_loss: 0.3774 - val_accuracy: 0.8967 - 20s/epoch - 51ms/step\n",
      "Epoch 115/200\n",
      "390/390 - 21s - loss: 0.0878 - accuracy: 0.9684 - val_loss: 0.3984 - val_accuracy: 0.8916 - 21s/epoch - 54ms/step\n",
      "Epoch 116/200\n",
      "390/390 - 20s - loss: 0.0912 - accuracy: 0.9688 - val_loss: 0.3272 - val_accuracy: 0.9048 - 20s/epoch - 51ms/step\n",
      "Epoch 117/200\n",
      "390/390 - 20s - loss: 0.0859 - accuracy: 0.9702 - val_loss: 0.3980 - val_accuracy: 0.8908 - 20s/epoch - 51ms/step\n",
      "Epoch 118/200\n",
      "390/390 - 21s - loss: 0.0862 - accuracy: 0.9700 - val_loss: 0.4627 - val_accuracy: 0.8776 - 21s/epoch - 53ms/step\n",
      "Epoch 119/200\n",
      "390/390 - 20s - loss: 0.0864 - accuracy: 0.9703 - val_loss: 0.5083 - val_accuracy: 0.8748 - 20s/epoch - 51ms/step\n",
      "Epoch 120/200\n",
      "390/390 - 21s - loss: 0.0839 - accuracy: 0.9712 - val_loss: 0.4229 - val_accuracy: 0.8829 - 21s/epoch - 54ms/step\n",
      "Epoch 121/200\n",
      "390/390 - 21s - loss: 0.0847 - accuracy: 0.9704 - val_loss: 0.3608 - val_accuracy: 0.9011 - 21s/epoch - 54ms/step\n",
      "Epoch 122/200\n",
      "390/390 - 20s - loss: 0.0846 - accuracy: 0.9709 - val_loss: 0.5217 - val_accuracy: 0.8637 - 20s/epoch - 52ms/step\n",
      "Epoch 123/200\n",
      "390/390 - 20s - loss: 0.0845 - accuracy: 0.9701 - val_loss: 0.5657 - val_accuracy: 0.8488 - 20s/epoch - 51ms/step\n",
      "Epoch 124/200\n",
      "390/390 - 21s - loss: 0.0872 - accuracy: 0.9699 - val_loss: 0.3267 - val_accuracy: 0.9049 - 21s/epoch - 54ms/step\n",
      "Epoch 125/200\n",
      "390/390 - 21s - loss: 0.0828 - accuracy: 0.9710 - val_loss: 0.3177 - val_accuracy: 0.9115 - 21s/epoch - 54ms/step\n",
      "Epoch 126/200\n",
      "390/390 - 21s - loss: 0.0765 - accuracy: 0.9733 - val_loss: 0.3736 - val_accuracy: 0.8946 - 21s/epoch - 55ms/step\n",
      "Epoch 127/200\n",
      "390/390 - 21s - loss: 0.0809 - accuracy: 0.9720 - val_loss: 0.3802 - val_accuracy: 0.8933 - 21s/epoch - 54ms/step\n",
      "Epoch 128/200\n",
      "390/390 - 20s - loss: 0.0832 - accuracy: 0.9713 - val_loss: 0.4285 - val_accuracy: 0.8854 - 20s/epoch - 51ms/step\n",
      "Epoch 129/200\n",
      "390/390 - 21s - loss: 0.0796 - accuracy: 0.9725 - val_loss: 0.3792 - val_accuracy: 0.8946 - 21s/epoch - 55ms/step\n",
      "Epoch 130/200\n",
      "390/390 - 21s - loss: 0.0782 - accuracy: 0.9733 - val_loss: 0.3149 - val_accuracy: 0.9065 - 21s/epoch - 54ms/step\n",
      "Epoch 131/200\n",
      "390/390 - 20s - loss: 0.0796 - accuracy: 0.9726 - val_loss: 0.3334 - val_accuracy: 0.9062 - 20s/epoch - 51ms/step\n",
      "Epoch 132/200\n",
      "390/390 - 21s - loss: 0.0772 - accuracy: 0.9735 - val_loss: 0.4608 - val_accuracy: 0.8755 - 21s/epoch - 53ms/step\n",
      "Epoch 133/200\n",
      "390/390 - 21s - loss: 0.0785 - accuracy: 0.9729 - val_loss: 0.3695 - val_accuracy: 0.8995 - 21s/epoch - 53ms/step\n",
      "Epoch 134/200\n",
      "390/390 - 21s - loss: 0.0731 - accuracy: 0.9748 - val_loss: 0.4729 - val_accuracy: 0.8789 - 21s/epoch - 54ms/step\n",
      "Epoch 135/200\n",
      "390/390 - 21s - loss: 0.0787 - accuracy: 0.9723 - val_loss: 0.4591 - val_accuracy: 0.8760 - 21s/epoch - 54ms/step\n",
      "Epoch 136/200\n",
      "390/390 - 20s - loss: 0.0782 - accuracy: 0.9727 - val_loss: 0.4478 - val_accuracy: 0.8848 - 20s/epoch - 51ms/step\n",
      "Epoch 137/200\n",
      "390/390 - 22s - loss: 0.0776 - accuracy: 0.9732 - val_loss: 0.3413 - val_accuracy: 0.9023 - 22s/epoch - 55ms/step\n",
      "Epoch 138/200\n",
      "390/390 - 21s - loss: 0.0754 - accuracy: 0.9737 - val_loss: 0.3771 - val_accuracy: 0.8960 - 21s/epoch - 54ms/step\n",
      "Epoch 139/200\n",
      "390/390 - 21s - loss: 0.0735 - accuracy: 0.9742 - val_loss: 0.3282 - val_accuracy: 0.9103 - 21s/epoch - 54ms/step\n",
      "Epoch 140/200\n",
      "390/390 - 20s - loss: 0.0700 - accuracy: 0.9760 - val_loss: 0.3439 - val_accuracy: 0.9038 - 20s/epoch - 51ms/step\n",
      "Epoch 141/200\n",
      "390/390 - 21s - loss: 0.0719 - accuracy: 0.9753 - val_loss: 0.3545 - val_accuracy: 0.8982 - 21s/epoch - 53ms/step\n",
      "Epoch 142/200\n",
      "390/390 - 21s - loss: 0.0680 - accuracy: 0.9761 - val_loss: 0.3837 - val_accuracy: 0.8978 - 21s/epoch - 53ms/step\n",
      "Epoch 143/200\n",
      "390/390 - 20s - loss: 0.0686 - accuracy: 0.9754 - val_loss: 0.4520 - val_accuracy: 0.8814 - 20s/epoch - 51ms/step\n",
      "Epoch 144/200\n",
      "390/390 - 22s - loss: 0.0732 - accuracy: 0.9744 - val_loss: 0.3739 - val_accuracy: 0.9002 - 22s/epoch - 55ms/step\n",
      "Epoch 145/200\n",
      "390/390 - 21s - loss: 0.0655 - accuracy: 0.9763 - val_loss: 0.3427 - val_accuracy: 0.9035 - 21s/epoch - 55ms/step\n",
      "Epoch 146/200\n",
      "390/390 - 21s - loss: 0.0727 - accuracy: 0.9745 - val_loss: 0.4485 - val_accuracy: 0.8836 - 21s/epoch - 54ms/step\n",
      "Epoch 147/200\n",
      "390/390 - 20s - loss: 0.0651 - accuracy: 0.9774 - val_loss: 0.4838 - val_accuracy: 0.8731 - 20s/epoch - 51ms/step\n",
      "Epoch 148/200\n",
      "390/390 - 21s - loss: 0.0682 - accuracy: 0.9762 - val_loss: 0.3684 - val_accuracy: 0.8951 - 21s/epoch - 54ms/step\n",
      "Epoch 149/200\n",
      "390/390 - 20s - loss: 0.0696 - accuracy: 0.9763 - val_loss: 0.3507 - val_accuracy: 0.9024 - 20s/epoch - 51ms/step\n",
      "Epoch 150/200\n",
      "390/390 - 22s - loss: 0.0628 - accuracy: 0.9783 - val_loss: 0.3421 - val_accuracy: 0.9069 - 22s/epoch - 55ms/step\n",
      "Epoch 151/200\n",
      "390/390 - 20s - loss: 0.0654 - accuracy: 0.9770 - val_loss: 0.3208 - val_accuracy: 0.9120 - 20s/epoch - 51ms/step\n",
      "Epoch 152/200\n",
      "390/390 - 20s - loss: 0.0670 - accuracy: 0.9767 - val_loss: 0.3257 - val_accuracy: 0.9102 - 20s/epoch - 51ms/step\n",
      "Epoch 153/200\n",
      "390/390 - 20s - loss: 0.0577 - accuracy: 0.9805 - val_loss: 0.3516 - val_accuracy: 0.9017 - 20s/epoch - 51ms/step\n",
      "Epoch 154/200\n",
      "390/390 - 20s - loss: 0.0617 - accuracy: 0.9790 - val_loss: 0.3648 - val_accuracy: 0.9011 - 20s/epoch - 51ms/step\n",
      "Epoch 155/200\n",
      "390/390 - 20s - loss: 0.0578 - accuracy: 0.9810 - val_loss: 0.3292 - val_accuracy: 0.9093 - 20s/epoch - 51ms/step\n",
      "Epoch 156/200\n",
      "390/390 - 22s - loss: 0.0610 - accuracy: 0.9788 - val_loss: 0.4292 - val_accuracy: 0.8847 - 22s/epoch - 56ms/step\n",
      "Epoch 157/200\n",
      "390/390 - 21s - loss: 0.0575 - accuracy: 0.9800 - val_loss: 0.3088 - val_accuracy: 0.9178 - 21s/epoch - 54ms/step\n",
      "Epoch 158/200\n",
      "390/390 - 21s - loss: 0.0524 - accuracy: 0.9821 - val_loss: 0.3331 - val_accuracy: 0.9091 - 21s/epoch - 53ms/step\n",
      "Epoch 159/200\n",
      "390/390 - 21s - loss: 0.0608 - accuracy: 0.9795 - val_loss: 0.4164 - val_accuracy: 0.8900 - 21s/epoch - 53ms/step\n",
      "Epoch 160/200\n",
      "390/390 - 22s - loss: 0.0581 - accuracy: 0.9803 - val_loss: 0.3942 - val_accuracy: 0.8931 - 22s/epoch - 57ms/step\n",
      "Epoch 161/200\n",
      "390/390 - 21s - loss: 0.0517 - accuracy: 0.9826 - val_loss: 0.3757 - val_accuracy: 0.9014 - 21s/epoch - 54ms/step\n",
      "Epoch 162/200\n",
      "390/390 - 20s - loss: 0.0508 - accuracy: 0.9829 - val_loss: 0.3300 - val_accuracy: 0.9101 - 20s/epoch - 52ms/step\n",
      "Epoch 163/200\n",
      "390/390 - 20s - loss: 0.0497 - accuracy: 0.9830 - val_loss: 0.2935 - val_accuracy: 0.9177 - 20s/epoch - 52ms/step\n",
      "Epoch 164/200\n",
      "390/390 - 22s - loss: 0.0480 - accuracy: 0.9837 - val_loss: 0.3697 - val_accuracy: 0.8992 - 22s/epoch - 55ms/step\n",
      "Epoch 165/200\n",
      "390/390 - 21s - loss: 0.0491 - accuracy: 0.9833 - val_loss: 0.3486 - val_accuracy: 0.9041 - 21s/epoch - 54ms/step\n",
      "Epoch 166/200\n",
      "390/390 - 22s - loss: 0.0471 - accuracy: 0.9843 - val_loss: 0.3760 - val_accuracy: 0.9013 - 22s/epoch - 55ms/step\n",
      "Epoch 167/200\n",
      "390/390 - 20s - loss: 0.0475 - accuracy: 0.9838 - val_loss: 0.3055 - val_accuracy: 0.9157 - 20s/epoch - 51ms/step\n",
      "Epoch 168/200\n",
      "390/390 - 20s - loss: 0.0460 - accuracy: 0.9848 - val_loss: 0.2819 - val_accuracy: 0.9210 - 20s/epoch - 51ms/step\n",
      "Epoch 169/200\n",
      "390/390 - 20s - loss: 0.0468 - accuracy: 0.9843 - val_loss: 0.3226 - val_accuracy: 0.9104 - 20s/epoch - 51ms/step\n",
      "Epoch 170/200\n",
      "390/390 - 21s - loss: 0.0410 - accuracy: 0.9866 - val_loss: 0.2794 - val_accuracy: 0.9214 - 21s/epoch - 55ms/step\n",
      "Epoch 171/200\n",
      "390/390 - 20s - loss: 0.0379 - accuracy: 0.9875 - val_loss: 0.2688 - val_accuracy: 0.9259 - 20s/epoch - 51ms/step\n",
      "Epoch 172/200\n",
      "390/390 - 21s - loss: 0.0391 - accuracy: 0.9872 - val_loss: 0.3583 - val_accuracy: 0.9038 - 21s/epoch - 54ms/step\n",
      "Epoch 173/200\n",
      "390/390 - 20s - loss: 0.0363 - accuracy: 0.9886 - val_loss: 0.3520 - val_accuracy: 0.9066 - 20s/epoch - 51ms/step\n",
      "Epoch 174/200\n",
      "390/390 - 20s - loss: 0.0336 - accuracy: 0.9892 - val_loss: 0.3265 - val_accuracy: 0.9120 - 20s/epoch - 50ms/step\n",
      "Epoch 175/200\n",
      "390/390 - 21s - loss: 0.0341 - accuracy: 0.9890 - val_loss: 0.3274 - val_accuracy: 0.9072 - 21s/epoch - 54ms/step\n",
      "Epoch 176/200\n",
      "390/390 - 21s - loss: 0.0318 - accuracy: 0.9898 - val_loss: 0.2982 - val_accuracy: 0.9197 - 21s/epoch - 54ms/step\n",
      "Epoch 177/200\n",
      "390/390 - 20s - loss: 0.0277 - accuracy: 0.9910 - val_loss: 0.2772 - val_accuracy: 0.9251 - 20s/epoch - 51ms/step\n",
      "Epoch 178/200\n",
      "390/390 - 21s - loss: 0.0278 - accuracy: 0.9913 - val_loss: 0.2595 - val_accuracy: 0.9292 - 21s/epoch - 53ms/step\n",
      "Epoch 179/200\n",
      "390/390 - 20s - loss: 0.0262 - accuracy: 0.9921 - val_loss: 0.2901 - val_accuracy: 0.9231 - 20s/epoch - 52ms/step\n",
      "Epoch 180/200\n",
      "390/390 - 21s - loss: 0.0265 - accuracy: 0.9916 - val_loss: 0.2591 - val_accuracy: 0.9274 - 21s/epoch - 55ms/step\n",
      "Epoch 181/200\n",
      "390/390 - 20s - loss: 0.0227 - accuracy: 0.9935 - val_loss: 0.2270 - val_accuracy: 0.9358 - 20s/epoch - 50ms/step\n",
      "Epoch 182/200\n",
      "390/390 - 21s - loss: 0.0193 - accuracy: 0.9946 - val_loss: 0.2315 - val_accuracy: 0.9353 - 21s/epoch - 54ms/step\n",
      "Epoch 183/200\n",
      "390/390 - 21s - loss: 0.0191 - accuracy: 0.9946 - val_loss: 0.2467 - val_accuracy: 0.9313 - 21s/epoch - 53ms/step\n",
      "Epoch 184/200\n",
      "390/390 - 20s - loss: 0.0173 - accuracy: 0.9952 - val_loss: 0.2477 - val_accuracy: 0.9314 - 20s/epoch - 52ms/step\n",
      "Epoch 185/200\n",
      "390/390 - 20s - loss: 0.0155 - accuracy: 0.9961 - val_loss: 0.2219 - val_accuracy: 0.9365 - 20s/epoch - 51ms/step\n",
      "Epoch 186/200\n",
      "390/390 - 21s - loss: 0.0145 - accuracy: 0.9966 - val_loss: 0.2368 - val_accuracy: 0.9356 - 21s/epoch - 54ms/step\n",
      "Epoch 187/200\n",
      "390/390 - 21s - loss: 0.0142 - accuracy: 0.9965 - val_loss: 0.2293 - val_accuracy: 0.9364 - 21s/epoch - 54ms/step\n",
      "Epoch 188/200\n",
      "390/390 - 20s - loss: 0.0121 - accuracy: 0.9974 - val_loss: 0.2140 - val_accuracy: 0.9413 - 20s/epoch - 51ms/step\n",
      "Epoch 189/200\n",
      "390/390 - 21s - loss: 0.0114 - accuracy: 0.9978 - val_loss: 0.2148 - val_accuracy: 0.9401 - 21s/epoch - 54ms/step\n",
      "Epoch 190/200\n",
      "390/390 - 21s - loss: 0.0105 - accuracy: 0.9983 - val_loss: 0.2132 - val_accuracy: 0.9399 - 21s/epoch - 53ms/step\n",
      "Epoch 191/200\n",
      "390/390 - 21s - loss: 0.0107 - accuracy: 0.9983 - val_loss: 0.2019 - val_accuracy: 0.9430 - 21s/epoch - 54ms/step\n",
      "Epoch 192/200\n",
      "390/390 - 22s - loss: 0.0107 - accuracy: 0.9986 - val_loss: 0.2041 - val_accuracy: 0.9404 - 22s/epoch - 56ms/step\n",
      "Epoch 193/200\n",
      "390/390 - 21s - loss: 0.0115 - accuracy: 0.9986 - val_loss: 0.1935 - val_accuracy: 0.9428 - 21s/epoch - 54ms/step\n",
      "Epoch 194/200\n",
      "390/390 - 21s - loss: 0.0117 - accuracy: 0.9988 - val_loss: 0.1912 - val_accuracy: 0.9442 - 21s/epoch - 53ms/step\n",
      "Epoch 195/200\n",
      "390/390 - 20s - loss: 0.0132 - accuracy: 0.9986 - val_loss: 0.1901 - val_accuracy: 0.9434 - 20s/epoch - 51ms/step\n",
      "Epoch 196/200\n",
      "390/390 - 21s - loss: 0.0140 - accuracy: 0.9989 - val_loss: 0.1854 - val_accuracy: 0.9433 - 21s/epoch - 54ms/step\n",
      "Epoch 197/200\n",
      "390/390 - 20s - loss: 0.0168 - accuracy: 0.9987 - val_loss: 0.1832 - val_accuracy: 0.9446 - 20s/epoch - 51ms/step\n",
      "Epoch 198/200\n",
      "390/390 - 21s - loss: 0.0192 - accuracy: 0.9988 - val_loss: 0.1827 - val_accuracy: 0.9449 - 21s/epoch - 54ms/step\n",
      "Epoch 199/200\n",
      "390/390 - 21s - loss: 0.0228 - accuracy: 0.9988 - val_loss: 0.1823 - val_accuracy: 0.9445 - 21s/epoch - 54ms/step\n",
      "Epoch 200/200\n",
      "390/390 - 20s - loss: 0.0287 - accuracy: 0.9989 - val_loss: 0.1843 - val_accuracy: 0.9447 - 20s/epoch - 51ms/step\n",
      " Using best val_acc=0.9449 from last 20 epochs\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.1827 - accuracy: 0.9449\n",
      " Finished: Lam=0.9, Repeat=2, Acc=0.9449\n",
      "\n",
      " lambda: Lam=0.9, Repeat=3/3\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "199343756 561714176\n",
      "Epoch 1/200\n",
      "390/390 - 23s - loss: 0.8653 - accuracy: 0.7141 - val_loss: 3.3833 - val_accuracy: 0.2304 - 23s/epoch - 58ms/step\n",
      "Epoch 2/200\n",
      "390/390 - 20s - loss: 0.6568 - accuracy: 0.7782 - val_loss: 2.7040 - val_accuracy: 0.4323 - 20s/epoch - 51ms/step\n",
      "Epoch 3/200\n",
      "390/390 - 20s - loss: 0.5551 - accuracy: 0.8100 - val_loss: 0.9713 - val_accuracy: 0.7034 - 20s/epoch - 51ms/step\n",
      "Epoch 4/200\n",
      "390/390 - 21s - loss: 0.4966 - accuracy: 0.8306 - val_loss: 1.0217 - val_accuracy: 0.6923 - 21s/epoch - 53ms/step\n",
      "Epoch 5/200\n",
      "390/390 - 20s - loss: 0.4530 - accuracy: 0.8456 - val_loss: 1.4290 - val_accuracy: 0.6203 - 20s/epoch - 51ms/step\n",
      "Epoch 6/200\n",
      "390/390 - 21s - loss: 0.4024 - accuracy: 0.8623 - val_loss: 0.4890 - val_accuracy: 0.8397 - 21s/epoch - 53ms/step\n",
      "Epoch 7/200\n",
      "390/390 - 21s - loss: 0.3585 - accuracy: 0.8761 - val_loss: 0.5345 - val_accuracy: 0.8334 - 21s/epoch - 55ms/step\n",
      "Epoch 8/200\n",
      "390/390 - 20s - loss: 0.3266 - accuracy: 0.8869 - val_loss: 0.5282 - val_accuracy: 0.8256 - 20s/epoch - 51ms/step\n",
      "Epoch 9/200\n",
      "390/390 - 22s - loss: 0.3026 - accuracy: 0.8957 - val_loss: 0.4192 - val_accuracy: 0.8632 - 22s/epoch - 56ms/step\n",
      "Epoch 10/200\n",
      "390/390 - 21s - loss: 0.2802 - accuracy: 0.9037 - val_loss: 0.5596 - val_accuracy: 0.8184 - 21s/epoch - 54ms/step\n",
      "Epoch 11/200\n",
      "390/390 - 21s - loss: 0.2704 - accuracy: 0.9051 - val_loss: 0.4649 - val_accuracy: 0.8526 - 21s/epoch - 54ms/step\n",
      "Epoch 12/200\n",
      "390/390 - 20s - loss: 0.2611 - accuracy: 0.9089 - val_loss: 0.4668 - val_accuracy: 0.8483 - 20s/epoch - 50ms/step\n",
      "Epoch 13/200\n",
      "390/390 - 22s - loss: 0.2449 - accuracy: 0.9158 - val_loss: 0.3887 - val_accuracy: 0.8740 - 22s/epoch - 55ms/step\n",
      "Epoch 14/200\n",
      "390/390 - 20s - loss: 0.2327 - accuracy: 0.9178 - val_loss: 0.4648 - val_accuracy: 0.8507 - 20s/epoch - 51ms/step\n",
      "Epoch 15/200\n",
      "390/390 - 21s - loss: 0.2227 - accuracy: 0.9224 - val_loss: 0.4350 - val_accuracy: 0.8594 - 21s/epoch - 54ms/step\n",
      "Epoch 16/200\n",
      "390/390 - 21s - loss: 0.2149 - accuracy: 0.9269 - val_loss: 0.4841 - val_accuracy: 0.8520 - 21s/epoch - 54ms/step\n",
      "Epoch 17/200\n",
      "390/390 - 20s - loss: 0.2119 - accuracy: 0.9257 - val_loss: 0.4818 - val_accuracy: 0.8536 - 20s/epoch - 51ms/step\n",
      "Epoch 18/200\n",
      "390/390 - 21s - loss: 0.1990 - accuracy: 0.9313 - val_loss: 0.3776 - val_accuracy: 0.8780 - 21s/epoch - 53ms/step\n",
      "Epoch 19/200\n",
      "390/390 - 22s - loss: 0.1977 - accuracy: 0.9310 - val_loss: 0.3840 - val_accuracy: 0.8776 - 22s/epoch - 55ms/step\n",
      "Epoch 20/200\n",
      "390/390 - 21s - loss: 0.1890 - accuracy: 0.9330 - val_loss: 0.4239 - val_accuracy: 0.8682 - 21s/epoch - 53ms/step\n",
      "Epoch 21/200\n",
      "390/390 - 20s - loss: 0.1876 - accuracy: 0.9339 - val_loss: 0.4464 - val_accuracy: 0.8663 - 20s/epoch - 51ms/step\n",
      "Epoch 22/200\n",
      "390/390 - 21s - loss: 0.1850 - accuracy: 0.9353 - val_loss: 0.4154 - val_accuracy: 0.8710 - 21s/epoch - 54ms/step\n",
      "Epoch 23/200\n",
      "390/390 - 20s - loss: 0.1785 - accuracy: 0.9379 - val_loss: 0.4589 - val_accuracy: 0.8576 - 20s/epoch - 51ms/step\n",
      "Epoch 24/200\n",
      "390/390 - 21s - loss: 0.1782 - accuracy: 0.9381 - val_loss: 0.3899 - val_accuracy: 0.8765 - 21s/epoch - 54ms/step\n",
      "Epoch 25/200\n",
      "390/390 - 22s - loss: 0.1698 - accuracy: 0.9410 - val_loss: 0.3679 - val_accuracy: 0.8833 - 22s/epoch - 56ms/step\n",
      "Epoch 26/200\n",
      "390/390 - 20s - loss: 0.1694 - accuracy: 0.9411 - val_loss: 0.3456 - val_accuracy: 0.8923 - 20s/epoch - 51ms/step\n",
      "Epoch 27/200\n",
      "390/390 - 21s - loss: 0.1656 - accuracy: 0.9408 - val_loss: 0.3551 - val_accuracy: 0.8897 - 21s/epoch - 54ms/step\n",
      "Epoch 28/200\n",
      "390/390 - 20s - loss: 0.1646 - accuracy: 0.9423 - val_loss: 0.3541 - val_accuracy: 0.8917 - 20s/epoch - 51ms/step\n",
      "Epoch 29/200\n",
      "390/390 - 20s - loss: 0.1628 - accuracy: 0.9427 - val_loss: 0.3006 - val_accuracy: 0.9025 - 20s/epoch - 51ms/step\n",
      "Epoch 30/200\n",
      "390/390 - 21s - loss: 0.1523 - accuracy: 0.9472 - val_loss: 0.3949 - val_accuracy: 0.8819 - 21s/epoch - 54ms/step\n",
      "Epoch 31/200\n",
      "390/390 - 21s - loss: 0.1609 - accuracy: 0.9451 - val_loss: 0.4363 - val_accuracy: 0.8715 - 21s/epoch - 54ms/step\n",
      "Epoch 32/200\n",
      "390/390 - 21s - loss: 0.1504 - accuracy: 0.9471 - val_loss: 0.3780 - val_accuracy: 0.8903 - 21s/epoch - 53ms/step\n",
      "Epoch 33/200\n",
      "390/390 - 20s - loss: 0.1543 - accuracy: 0.9443 - val_loss: 0.3475 - val_accuracy: 0.8884 - 20s/epoch - 51ms/step\n",
      "Epoch 34/200\n",
      "390/390 - 21s - loss: 0.1497 - accuracy: 0.9472 - val_loss: 0.3725 - val_accuracy: 0.8895 - 21s/epoch - 55ms/step\n",
      "Epoch 35/200\n",
      "390/390 - 21s - loss: 0.1483 - accuracy: 0.9489 - val_loss: 0.3629 - val_accuracy: 0.8847 - 21s/epoch - 54ms/step\n",
      "Epoch 36/200\n",
      "390/390 - 21s - loss: 0.1461 - accuracy: 0.9483 - val_loss: 0.3989 - val_accuracy: 0.8841 - 21s/epoch - 53ms/step\n",
      "Epoch 37/200\n",
      "390/390 - 21s - loss: 0.1464 - accuracy: 0.9483 - val_loss: 0.5026 - val_accuracy: 0.8490 - 21s/epoch - 54ms/step\n",
      "Epoch 38/200\n",
      "390/390 - 21s - loss: 0.1455 - accuracy: 0.9484 - val_loss: 0.5235 - val_accuracy: 0.8530 - 21s/epoch - 54ms/step\n",
      "Epoch 39/200\n",
      "390/390 - 21s - loss: 0.1379 - accuracy: 0.9520 - val_loss: 0.3977 - val_accuracy: 0.8821 - 21s/epoch - 53ms/step\n",
      "Epoch 40/200\n",
      "390/390 - 20s - loss: 0.1421 - accuracy: 0.9503 - val_loss: 0.3984 - val_accuracy: 0.8846 - 20s/epoch - 51ms/step\n",
      "Epoch 41/200\n",
      "390/390 - 20s - loss: 0.1381 - accuracy: 0.9513 - val_loss: 0.3110 - val_accuracy: 0.9060 - 20s/epoch - 52ms/step\n",
      "Epoch 42/200\n",
      "390/390 - 21s - loss: 0.1388 - accuracy: 0.9515 - val_loss: 0.3862 - val_accuracy: 0.8858 - 21s/epoch - 53ms/step\n",
      "Epoch 43/200\n",
      "390/390 - 20s - loss: 0.1351 - accuracy: 0.9523 - val_loss: 0.6406 - val_accuracy: 0.8242 - 20s/epoch - 51ms/step\n",
      "Epoch 44/200\n",
      "390/390 - 22s - loss: 0.1400 - accuracy: 0.9511 - val_loss: 0.3936 - val_accuracy: 0.8858 - 22s/epoch - 56ms/step\n",
      "Epoch 45/200\n",
      "390/390 - 21s - loss: 0.1325 - accuracy: 0.9530 - val_loss: 0.3336 - val_accuracy: 0.9000 - 21s/epoch - 54ms/step\n",
      "Epoch 46/200\n",
      "390/390 - 20s - loss: 0.1338 - accuracy: 0.9539 - val_loss: 0.3112 - val_accuracy: 0.9050 - 20s/epoch - 51ms/step\n",
      "Epoch 47/200\n",
      "390/390 - 21s - loss: 0.1326 - accuracy: 0.9522 - val_loss: 0.4855 - val_accuracy: 0.8659 - 21s/epoch - 53ms/step\n",
      "Epoch 48/200\n",
      "390/390 - 20s - loss: 0.1331 - accuracy: 0.9532 - val_loss: 0.3511 - val_accuracy: 0.8926 - 20s/epoch - 51ms/step\n",
      "Epoch 49/200\n",
      "390/390 - 21s - loss: 0.1286 - accuracy: 0.9555 - val_loss: 0.4419 - val_accuracy: 0.8736 - 21s/epoch - 53ms/step\n",
      "Epoch 50/200\n",
      "390/390 - 20s - loss: 0.1255 - accuracy: 0.9566 - val_loss: 0.4165 - val_accuracy: 0.8762 - 20s/epoch - 51ms/step\n",
      "Epoch 51/200\n",
      "390/390 - 21s - loss: 0.1294 - accuracy: 0.9535 - val_loss: 0.3617 - val_accuracy: 0.8892 - 21s/epoch - 54ms/step\n",
      "Epoch 52/200\n",
      "390/390 - 21s - loss: 0.1274 - accuracy: 0.9554 - val_loss: 0.3580 - val_accuracy: 0.8901 - 21s/epoch - 54ms/step\n",
      "Epoch 53/200\n",
      "390/390 - 20s - loss: 0.1279 - accuracy: 0.9549 - val_loss: 0.3305 - val_accuracy: 0.9000 - 20s/epoch - 51ms/step\n",
      "Epoch 54/200\n",
      "390/390 - 21s - loss: 0.1230 - accuracy: 0.9571 - val_loss: 0.3654 - val_accuracy: 0.8910 - 21s/epoch - 53ms/step\n",
      "Epoch 55/200\n",
      "390/390 - 21s - loss: 0.1266 - accuracy: 0.9562 - val_loss: 0.4307 - val_accuracy: 0.8787 - 21s/epoch - 53ms/step\n",
      "Epoch 56/200\n",
      "390/390 - 20s - loss: 0.1231 - accuracy: 0.9563 - val_loss: 0.4580 - val_accuracy: 0.8731 - 20s/epoch - 51ms/step\n",
      "Epoch 57/200\n",
      "390/390 - 20s - loss: 0.1230 - accuracy: 0.9566 - val_loss: 0.4943 - val_accuracy: 0.8631 - 20s/epoch - 51ms/step\n",
      "Epoch 58/200\n",
      "390/390 - 21s - loss: 0.1160 - accuracy: 0.9589 - val_loss: 0.3723 - val_accuracy: 0.8962 - 21s/epoch - 55ms/step\n",
      "Epoch 59/200\n",
      "390/390 - 20s - loss: 0.1206 - accuracy: 0.9573 - val_loss: 0.4101 - val_accuracy: 0.8830 - 20s/epoch - 51ms/step\n",
      "Epoch 60/200\n",
      "390/390 - 21s - loss: 0.1202 - accuracy: 0.9578 - val_loss: 0.5642 - val_accuracy: 0.8449 - 21s/epoch - 53ms/step\n",
      "Epoch 61/200\n",
      "390/390 - 20s - loss: 0.1184 - accuracy: 0.9585 - val_loss: 0.4398 - val_accuracy: 0.8718 - 20s/epoch - 52ms/step\n",
      "Epoch 62/200\n",
      "390/390 - 20s - loss: 0.1179 - accuracy: 0.9592 - val_loss: 0.3442 - val_accuracy: 0.8958 - 20s/epoch - 51ms/step\n",
      "Epoch 63/200\n",
      "390/390 - 21s - loss: 0.1176 - accuracy: 0.9589 - val_loss: 0.4678 - val_accuracy: 0.8754 - 21s/epoch - 54ms/step\n",
      "Epoch 64/200\n",
      "390/390 - 21s - loss: 0.1166 - accuracy: 0.9588 - val_loss: 0.4913 - val_accuracy: 0.8621 - 21s/epoch - 55ms/step\n",
      "Epoch 65/200\n",
      "390/390 - 20s - loss: 0.1135 - accuracy: 0.9605 - val_loss: 0.4441 - val_accuracy: 0.8786 - 20s/epoch - 51ms/step\n",
      "Epoch 66/200\n",
      "390/390 - 21s - loss: 0.1147 - accuracy: 0.9598 - val_loss: 0.3551 - val_accuracy: 0.8972 - 21s/epoch - 55ms/step\n",
      "Epoch 67/200\n",
      "390/390 - 21s - loss: 0.1124 - accuracy: 0.9609 - val_loss: 0.3904 - val_accuracy: 0.8872 - 21s/epoch - 53ms/step\n",
      "Epoch 68/200\n",
      "390/390 - 21s - loss: 0.1147 - accuracy: 0.9605 - val_loss: 0.3612 - val_accuracy: 0.8922 - 21s/epoch - 54ms/step\n",
      "Epoch 69/200\n",
      "390/390 - 21s - loss: 0.1160 - accuracy: 0.9590 - val_loss: 1.1613 - val_accuracy: 0.7389 - 21s/epoch - 53ms/step\n",
      "Epoch 70/200\n",
      "390/390 - 20s - loss: 0.1125 - accuracy: 0.9613 - val_loss: 0.4165 - val_accuracy: 0.8826 - 20s/epoch - 51ms/step\n",
      "Epoch 71/200\n",
      "390/390 - 20s - loss: 0.1105 - accuracy: 0.9616 - val_loss: 0.3619 - val_accuracy: 0.8940 - 20s/epoch - 52ms/step\n",
      "Epoch 72/200\n",
      "390/390 - 20s - loss: 0.1146 - accuracy: 0.9594 - val_loss: 0.4369 - val_accuracy: 0.8752 - 20s/epoch - 51ms/step\n",
      "Epoch 73/200\n",
      "390/390 - 21s - loss: 0.1082 - accuracy: 0.9626 - val_loss: 0.3778 - val_accuracy: 0.8965 - 21s/epoch - 54ms/step\n",
      "Epoch 74/200\n",
      "390/390 - 21s - loss: 0.1126 - accuracy: 0.9613 - val_loss: 0.3541 - val_accuracy: 0.8968 - 21s/epoch - 54ms/step\n",
      "Epoch 75/200\n",
      "390/390 - 20s - loss: 0.1097 - accuracy: 0.9615 - val_loss: 0.3213 - val_accuracy: 0.9025 - 20s/epoch - 51ms/step\n",
      "Epoch 76/200\n",
      "390/390 - 20s - loss: 0.1105 - accuracy: 0.9613 - val_loss: 0.3906 - val_accuracy: 0.8892 - 20s/epoch - 51ms/step\n",
      "Epoch 77/200\n",
      "390/390 - 20s - loss: 0.1088 - accuracy: 0.9626 - val_loss: 0.4030 - val_accuracy: 0.8929 - 20s/epoch - 51ms/step\n",
      "Epoch 78/200\n",
      "390/390 - 20s - loss: 0.1050 - accuracy: 0.9627 - val_loss: 0.3243 - val_accuracy: 0.9030 - 20s/epoch - 51ms/step\n",
      "Epoch 79/200\n",
      "390/390 - 21s - loss: 0.1069 - accuracy: 0.9631 - val_loss: 0.4240 - val_accuracy: 0.8775 - 21s/epoch - 54ms/step\n",
      "Epoch 80/200\n",
      "390/390 - 20s - loss: 0.1050 - accuracy: 0.9628 - val_loss: 0.4390 - val_accuracy: 0.8793 - 20s/epoch - 51ms/step\n",
      "Epoch 81/200\n",
      "390/390 - 21s - loss: 0.1059 - accuracy: 0.9634 - val_loss: 0.3919 - val_accuracy: 0.8811 - 21s/epoch - 54ms/step\n",
      "Epoch 82/200\n",
      "390/390 - 20s - loss: 0.1029 - accuracy: 0.9634 - val_loss: 0.3933 - val_accuracy: 0.8898 - 20s/epoch - 52ms/step\n",
      "Epoch 83/200\n",
      "390/390 - 20s - loss: 0.1052 - accuracy: 0.9637 - val_loss: 0.4477 - val_accuracy: 0.8776 - 20s/epoch - 51ms/step\n",
      "Epoch 84/200\n",
      "390/390 - 20s - loss: 0.1047 - accuracy: 0.9637 - val_loss: 0.3799 - val_accuracy: 0.8933 - 20s/epoch - 51ms/step\n",
      "Epoch 85/200\n",
      "390/390 - 21s - loss: 0.1050 - accuracy: 0.9638 - val_loss: 0.4677 - val_accuracy: 0.8724 - 21s/epoch - 53ms/step\n",
      "Epoch 86/200\n",
      "390/390 - 20s - loss: 0.1051 - accuracy: 0.9629 - val_loss: 0.4326 - val_accuracy: 0.8802 - 20s/epoch - 51ms/step\n",
      "Epoch 87/200\n",
      "390/390 - 21s - loss: 0.1021 - accuracy: 0.9639 - val_loss: 0.3645 - val_accuracy: 0.8940 - 21s/epoch - 55ms/step\n",
      "Epoch 88/200\n",
      "390/390 - 21s - loss: 0.1036 - accuracy: 0.9636 - val_loss: 0.4442 - val_accuracy: 0.8793 - 21s/epoch - 54ms/step\n",
      "Epoch 89/200\n",
      "390/390 - 21s - loss: 0.1015 - accuracy: 0.9646 - val_loss: 0.3494 - val_accuracy: 0.8966 - 21s/epoch - 54ms/step\n",
      "Epoch 90/200\n",
      "390/390 - 21s - loss: 0.0969 - accuracy: 0.9666 - val_loss: 0.4803 - val_accuracy: 0.8759 - 21s/epoch - 53ms/step\n",
      "Epoch 91/200\n",
      "390/390 - 20s - loss: 0.0985 - accuracy: 0.9650 - val_loss: 0.3508 - val_accuracy: 0.9006 - 20s/epoch - 52ms/step\n",
      "Epoch 92/200\n",
      "390/390 - 21s - loss: 0.1019 - accuracy: 0.9644 - val_loss: 0.3485 - val_accuracy: 0.9047 - 21s/epoch - 53ms/step\n",
      "Epoch 93/200\n",
      "390/390 - 21s - loss: 0.0966 - accuracy: 0.9668 - val_loss: 0.4251 - val_accuracy: 0.8839 - 21s/epoch - 53ms/step\n",
      "Epoch 94/200\n",
      "390/390 - 21s - loss: 0.0997 - accuracy: 0.9660 - val_loss: 0.4430 - val_accuracy: 0.8817 - 21s/epoch - 53ms/step\n",
      "Epoch 95/200\n",
      "390/390 - 20s - loss: 0.0998 - accuracy: 0.9650 - val_loss: 0.3471 - val_accuracy: 0.9020 - 20s/epoch - 51ms/step\n",
      "Epoch 96/200\n",
      "390/390 - 21s - loss: 0.0948 - accuracy: 0.9671 - val_loss: 0.4692 - val_accuracy: 0.8711 - 21s/epoch - 53ms/step\n",
      "Epoch 97/200\n",
      "390/390 - 21s - loss: 0.1001 - accuracy: 0.9651 - val_loss: 0.4106 - val_accuracy: 0.8920 - 21s/epoch - 54ms/step\n",
      "Epoch 98/200\n",
      "390/390 - 20s - loss: 0.0962 - accuracy: 0.9660 - val_loss: 0.4595 - val_accuracy: 0.8769 - 20s/epoch - 51ms/step\n",
      "Epoch 99/200\n",
      "390/390 - 21s - loss: 0.0975 - accuracy: 0.9664 - val_loss: 0.3538 - val_accuracy: 0.9032 - 21s/epoch - 54ms/step\n",
      "Epoch 100/200\n",
      "390/390 - 21s - loss: 0.0973 - accuracy: 0.9657 - val_loss: 0.3819 - val_accuracy: 0.8935 - 21s/epoch - 53ms/step\n",
      "Epoch 101/200\n",
      "390/390 - 21s - loss: 0.0983 - accuracy: 0.9656 - val_loss: 0.3538 - val_accuracy: 0.9014 - 21s/epoch - 54ms/step\n",
      "Epoch 102/200\n",
      "390/390 - 20s - loss: 0.0961 - accuracy: 0.9662 - val_loss: 0.3832 - val_accuracy: 0.8876 - 20s/epoch - 51ms/step\n",
      "Epoch 103/200\n",
      "390/390 - 21s - loss: 0.0930 - accuracy: 0.9677 - val_loss: 0.5304 - val_accuracy: 0.8572 - 21s/epoch - 54ms/step\n",
      "Epoch 104/200\n",
      "390/390 - 21s - loss: 0.0948 - accuracy: 0.9663 - val_loss: 0.4154 - val_accuracy: 0.8868 - 21s/epoch - 54ms/step\n",
      "Epoch 105/200\n",
      "390/390 - 20s - loss: 0.0924 - accuracy: 0.9675 - val_loss: 0.4010 - val_accuracy: 0.8911 - 20s/epoch - 51ms/step\n",
      "Epoch 106/200\n",
      "390/390 - 21s - loss: 0.0924 - accuracy: 0.9670 - val_loss: 0.4046 - val_accuracy: 0.8840 - 21s/epoch - 54ms/step\n",
      "Epoch 107/200\n",
      "390/390 - 20s - loss: 0.0881 - accuracy: 0.9687 - val_loss: 0.4177 - val_accuracy: 0.8838 - 20s/epoch - 52ms/step\n",
      "Epoch 108/200\n",
      "390/390 - 21s - loss: 0.0897 - accuracy: 0.9690 - val_loss: 0.3395 - val_accuracy: 0.9074 - 21s/epoch - 54ms/step\n",
      "Epoch 109/200\n",
      "390/390 - 21s - loss: 0.0929 - accuracy: 0.9672 - val_loss: 0.3394 - val_accuracy: 0.9017 - 21s/epoch - 54ms/step\n",
      "Epoch 110/200\n",
      "390/390 - 21s - loss: 0.0909 - accuracy: 0.9684 - val_loss: 0.4017 - val_accuracy: 0.8839 - 21s/epoch - 53ms/step\n",
      "Epoch 111/200\n",
      "390/390 - 21s - loss: 0.0871 - accuracy: 0.9699 - val_loss: 0.4011 - val_accuracy: 0.8903 - 21s/epoch - 55ms/step\n",
      "Epoch 112/200\n",
      "390/390 - 21s - loss: 0.0876 - accuracy: 0.9696 - val_loss: 0.4424 - val_accuracy: 0.8798 - 21s/epoch - 53ms/step\n",
      "Epoch 113/200\n",
      "390/390 - 20s - loss: 0.0897 - accuracy: 0.9689 - val_loss: 0.3746 - val_accuracy: 0.8942 - 20s/epoch - 51ms/step\n",
      "Epoch 114/200\n",
      "390/390 - 21s - loss: 0.0915 - accuracy: 0.9677 - val_loss: 0.4602 - val_accuracy: 0.8802 - 21s/epoch - 53ms/step\n",
      "Epoch 115/200\n",
      "390/390 - 21s - loss: 0.0853 - accuracy: 0.9710 - val_loss: 0.3096 - val_accuracy: 0.9130 - 21s/epoch - 54ms/step\n",
      "Epoch 116/200\n",
      "390/390 - 20s - loss: 0.0834 - accuracy: 0.9711 - val_loss: 0.3488 - val_accuracy: 0.9019 - 20s/epoch - 51ms/step\n",
      "Epoch 117/200\n",
      "390/390 - 21s - loss: 0.0860 - accuracy: 0.9696 - val_loss: 0.3600 - val_accuracy: 0.9036 - 21s/epoch - 54ms/step\n",
      "Epoch 118/200\n",
      "390/390 - 21s - loss: 0.0889 - accuracy: 0.9695 - val_loss: 0.3814 - val_accuracy: 0.8949 - 21s/epoch - 54ms/step\n",
      "Epoch 119/200\n",
      "390/390 - 20s - loss: 0.0887 - accuracy: 0.9694 - val_loss: 0.3505 - val_accuracy: 0.9000 - 20s/epoch - 51ms/step\n",
      "Epoch 120/200\n",
      "390/390 - 21s - loss: 0.0823 - accuracy: 0.9706 - val_loss: 0.3294 - val_accuracy: 0.9017 - 21s/epoch - 54ms/step\n",
      "Epoch 121/200\n",
      "390/390 - 21s - loss: 0.0819 - accuracy: 0.9719 - val_loss: 0.3817 - val_accuracy: 0.8963 - 21s/epoch - 54ms/step\n",
      "Epoch 122/200\n",
      "390/390 - 21s - loss: 0.0843 - accuracy: 0.9707 - val_loss: 0.3897 - val_accuracy: 0.8969 - 21s/epoch - 54ms/step\n",
      "Epoch 123/200\n",
      "390/390 - 21s - loss: 0.0852 - accuracy: 0.9708 - val_loss: 0.3244 - val_accuracy: 0.9089 - 21s/epoch - 54ms/step\n",
      "Epoch 124/200\n",
      "390/390 - 21s - loss: 0.0831 - accuracy: 0.9703 - val_loss: 0.4008 - val_accuracy: 0.8905 - 21s/epoch - 54ms/step\n",
      "Epoch 125/200\n",
      "390/390 - 20s - loss: 0.0830 - accuracy: 0.9709 - val_loss: 0.5139 - val_accuracy: 0.8640 - 20s/epoch - 51ms/step\n",
      "Epoch 126/200\n",
      "390/390 - 20s - loss: 0.0834 - accuracy: 0.9710 - val_loss: 0.4979 - val_accuracy: 0.8714 - 20s/epoch - 50ms/step\n",
      "Epoch 127/200\n",
      "390/390 - 20s - loss: 0.0792 - accuracy: 0.9720 - val_loss: 0.3242 - val_accuracy: 0.9068 - 20s/epoch - 51ms/step\n",
      "Epoch 128/200\n",
      "390/390 - 21s - loss: 0.0832 - accuracy: 0.9709 - val_loss: 0.3391 - val_accuracy: 0.9017 - 21s/epoch - 54ms/step\n",
      "Epoch 129/200\n",
      "390/390 - 21s - loss: 0.0813 - accuracy: 0.9716 - val_loss: 0.3255 - val_accuracy: 0.9126 - 21s/epoch - 54ms/step\n",
      "Epoch 130/200\n",
      "390/390 - 21s - loss: 0.0757 - accuracy: 0.9741 - val_loss: 0.4821 - val_accuracy: 0.8772 - 21s/epoch - 54ms/step\n",
      "Epoch 131/200\n",
      "390/390 - 20s - loss: 0.0785 - accuracy: 0.9726 - val_loss: 0.4559 - val_accuracy: 0.8790 - 20s/epoch - 51ms/step\n",
      "Epoch 132/200\n",
      "390/390 - 21s - loss: 0.0785 - accuracy: 0.9722 - val_loss: 0.4280 - val_accuracy: 0.8889 - 21s/epoch - 53ms/step\n",
      "Epoch 133/200\n",
      "390/390 - 21s - loss: 0.0800 - accuracy: 0.9716 - val_loss: 0.4263 - val_accuracy: 0.8809 - 21s/epoch - 54ms/step\n",
      "Epoch 134/200\n",
      "390/390 - 21s - loss: 0.0805 - accuracy: 0.9725 - val_loss: 0.3643 - val_accuracy: 0.8997 - 21s/epoch - 53ms/step\n",
      "Epoch 135/200\n",
      "390/390 - 20s - loss: 0.0739 - accuracy: 0.9746 - val_loss: 0.3760 - val_accuracy: 0.8970 - 20s/epoch - 50ms/step\n",
      "Epoch 136/200\n",
      "390/390 - 21s - loss: 0.0737 - accuracy: 0.9749 - val_loss: 0.3341 - val_accuracy: 0.9056 - 21s/epoch - 55ms/step\n",
      "Epoch 137/200\n",
      "390/390 - 20s - loss: 0.0762 - accuracy: 0.9732 - val_loss: 0.4128 - val_accuracy: 0.8868 - 20s/epoch - 51ms/step\n",
      "Epoch 138/200\n",
      "390/390 - 20s - loss: 0.0726 - accuracy: 0.9747 - val_loss: 0.3981 - val_accuracy: 0.8925 - 20s/epoch - 51ms/step\n",
      "Epoch 139/200\n",
      "390/390 - 21s - loss: 0.0705 - accuracy: 0.9752 - val_loss: 0.2806 - val_accuracy: 0.9180 - 21s/epoch - 53ms/step\n",
      "Epoch 140/200\n",
      "390/390 - 21s - loss: 0.0732 - accuracy: 0.9747 - val_loss: 0.3544 - val_accuracy: 0.9061 - 21s/epoch - 55ms/step\n",
      "Epoch 141/200\n",
      "390/390 - 21s - loss: 0.0694 - accuracy: 0.9758 - val_loss: 0.4486 - val_accuracy: 0.8841 - 21s/epoch - 54ms/step\n",
      "Epoch 142/200\n",
      "390/390 - 21s - loss: 0.0687 - accuracy: 0.9765 - val_loss: 0.3475 - val_accuracy: 0.9046 - 21s/epoch - 54ms/step\n",
      "Epoch 143/200\n",
      "390/390 - 20s - loss: 0.0698 - accuracy: 0.9761 - val_loss: 0.3798 - val_accuracy: 0.8918 - 20s/epoch - 51ms/step\n",
      "Epoch 144/200\n",
      "390/390 - 21s - loss: 0.0715 - accuracy: 0.9755 - val_loss: 0.3920 - val_accuracy: 0.8936 - 21s/epoch - 54ms/step\n",
      "Epoch 145/200\n",
      "390/390 - 21s - loss: 0.0653 - accuracy: 0.9777 - val_loss: 0.3582 - val_accuracy: 0.9013 - 21s/epoch - 54ms/step\n",
      "Epoch 146/200\n",
      "390/390 - 20s - loss: 0.0673 - accuracy: 0.9764 - val_loss: 0.3586 - val_accuracy: 0.9012 - 20s/epoch - 51ms/step\n",
      "Epoch 147/200\n",
      "390/390 - 21s - loss: 0.0661 - accuracy: 0.9775 - val_loss: 0.4849 - val_accuracy: 0.8713 - 21s/epoch - 54ms/step\n",
      "Epoch 148/200\n",
      "390/390 - 21s - loss: 0.0686 - accuracy: 0.9767 - val_loss: 0.3722 - val_accuracy: 0.9037 - 21s/epoch - 54ms/step\n",
      "Epoch 149/200\n",
      "390/390 - 20s - loss: 0.0641 - accuracy: 0.9778 - val_loss: 0.3295 - val_accuracy: 0.9101 - 20s/epoch - 51ms/step\n",
      "Epoch 150/200\n",
      "390/390 - 21s - loss: 0.0655 - accuracy: 0.9774 - val_loss: 0.4098 - val_accuracy: 0.8960 - 21s/epoch - 54ms/step\n",
      "Epoch 151/200\n",
      "390/390 - 21s - loss: 0.0628 - accuracy: 0.9784 - val_loss: 0.3530 - val_accuracy: 0.9029 - 21s/epoch - 55ms/step\n",
      "Epoch 152/200\n",
      "390/390 - 21s - loss: 0.0645 - accuracy: 0.9778 - val_loss: 0.3460 - val_accuracy: 0.9023 - 21s/epoch - 54ms/step\n",
      "Epoch 153/200\n",
      "390/390 - 20s - loss: 0.0614 - accuracy: 0.9791 - val_loss: 0.3757 - val_accuracy: 0.9008 - 20s/epoch - 51ms/step\n",
      "Epoch 154/200\n",
      "390/390 - 21s - loss: 0.0576 - accuracy: 0.9800 - val_loss: 0.3596 - val_accuracy: 0.9010 - 21s/epoch - 54ms/step\n",
      "Epoch 155/200\n",
      "390/390 - 21s - loss: 0.0613 - accuracy: 0.9792 - val_loss: 0.3611 - val_accuracy: 0.9020 - 21s/epoch - 53ms/step\n",
      "Epoch 156/200\n",
      "390/390 - 21s - loss: 0.0555 - accuracy: 0.9808 - val_loss: 0.4201 - val_accuracy: 0.8908 - 21s/epoch - 55ms/step\n",
      "Epoch 157/200\n",
      "390/390 - 20s - loss: 0.0593 - accuracy: 0.9795 - val_loss: 0.2925 - val_accuracy: 0.9205 - 20s/epoch - 51ms/step\n",
      "Epoch 158/200\n",
      "390/390 - 20s - loss: 0.0567 - accuracy: 0.9813 - val_loss: 0.3077 - val_accuracy: 0.9120 - 20s/epoch - 51ms/step\n",
      "Epoch 159/200\n",
      "390/390 - 21s - loss: 0.0535 - accuracy: 0.9816 - val_loss: 0.4024 - val_accuracy: 0.8954 - 21s/epoch - 54ms/step\n",
      "Epoch 160/200\n",
      "390/390 - 22s - loss: 0.0562 - accuracy: 0.9801 - val_loss: 0.3809 - val_accuracy: 0.8981 - 22s/epoch - 56ms/step\n",
      "Epoch 161/200\n",
      "390/390 - 20s - loss: 0.0571 - accuracy: 0.9803 - val_loss: 0.3004 - val_accuracy: 0.9197 - 20s/epoch - 52ms/step\n",
      "Epoch 162/200\n",
      "390/390 - 21s - loss: 0.0500 - accuracy: 0.9836 - val_loss: 0.3650 - val_accuracy: 0.9045 - 21s/epoch - 55ms/step\n",
      "Epoch 163/200\n",
      "390/390 - 20s - loss: 0.0491 - accuracy: 0.9836 - val_loss: 0.2990 - val_accuracy: 0.9172 - 20s/epoch - 51ms/step\n",
      "Epoch 164/200\n",
      "390/390 - 21s - loss: 0.0503 - accuracy: 0.9834 - val_loss: 0.2896 - val_accuracy: 0.9214 - 21s/epoch - 55ms/step\n",
      "Epoch 165/200\n",
      "390/390 - 21s - loss: 0.0480 - accuracy: 0.9831 - val_loss: 0.4012 - val_accuracy: 0.8980 - 21s/epoch - 54ms/step\n",
      "Epoch 166/200\n",
      "390/390 - 21s - loss: 0.0483 - accuracy: 0.9840 - val_loss: 0.3177 - val_accuracy: 0.9134 - 21s/epoch - 54ms/step\n",
      "Epoch 167/200\n",
      "390/390 - 20s - loss: 0.0473 - accuracy: 0.9846 - val_loss: 0.3317 - val_accuracy: 0.9127 - 20s/epoch - 52ms/step\n",
      "Epoch 168/200\n",
      "390/390 - 21s - loss: 0.0460 - accuracy: 0.9852 - val_loss: 0.3509 - val_accuracy: 0.9082 - 21s/epoch - 54ms/step\n",
      "Epoch 169/200\n",
      "390/390 - 20s - loss: 0.0424 - accuracy: 0.9860 - val_loss: 0.2868 - val_accuracy: 0.9224 - 20s/epoch - 51ms/step\n",
      "Epoch 170/200\n",
      "390/390 - 20s - loss: 0.0391 - accuracy: 0.9868 - val_loss: 0.3369 - val_accuracy: 0.9150 - 20s/epoch - 51ms/step\n",
      "Epoch 171/200\n",
      "390/390 - 22s - loss: 0.0426 - accuracy: 0.9860 - val_loss: 0.3448 - val_accuracy: 0.9080 - 22s/epoch - 56ms/step\n",
      "Epoch 172/200\n",
      "390/390 - 20s - loss: 0.0380 - accuracy: 0.9878 - val_loss: 0.2901 - val_accuracy: 0.9221 - 20s/epoch - 51ms/step\n",
      "Epoch 173/200\n",
      "390/390 - 20s - loss: 0.0383 - accuracy: 0.9870 - val_loss: 0.2752 - val_accuracy: 0.9247 - 20s/epoch - 52ms/step\n",
      "Epoch 174/200\n",
      "390/390 - 20s - loss: 0.0338 - accuracy: 0.9894 - val_loss: 0.3118 - val_accuracy: 0.9153 - 20s/epoch - 51ms/step\n",
      "Epoch 175/200\n",
      "390/390 - 21s - loss: 0.0340 - accuracy: 0.9889 - val_loss: 0.2708 - val_accuracy: 0.9270 - 21s/epoch - 53ms/step\n",
      "Epoch 176/200\n",
      "390/390 - 20s - loss: 0.0295 - accuracy: 0.9912 - val_loss: 0.2767 - val_accuracy: 0.9233 - 20s/epoch - 52ms/step\n",
      "Epoch 177/200\n",
      "390/390 - 21s - loss: 0.0279 - accuracy: 0.9910 - val_loss: 0.2683 - val_accuracy: 0.9283 - 21s/epoch - 54ms/step\n",
      "Epoch 178/200\n",
      "390/390 - 21s - loss: 0.0291 - accuracy: 0.9908 - val_loss: 0.2677 - val_accuracy: 0.9282 - 21s/epoch - 54ms/step\n",
      "Epoch 179/200\n",
      "390/390 - 22s - loss: 0.0278 - accuracy: 0.9914 - val_loss: 0.2567 - val_accuracy: 0.9299 - 22s/epoch - 56ms/step\n",
      "Epoch 180/200\n",
      "390/390 - 21s - loss: 0.0256 - accuracy: 0.9922 - val_loss: 0.2826 - val_accuracy: 0.9238 - 21s/epoch - 53ms/step\n",
      "Epoch 181/200\n",
      "390/390 - 21s - loss: 0.0225 - accuracy: 0.9934 - val_loss: 0.2492 - val_accuracy: 0.9304 - 21s/epoch - 54ms/step\n",
      "Epoch 182/200\n",
      "390/390 - 20s - loss: 0.0215 - accuracy: 0.9936 - val_loss: 0.2722 - val_accuracy: 0.9263 - 20s/epoch - 51ms/step\n",
      "Epoch 183/200\n",
      "390/390 - 21s - loss: 0.0200 - accuracy: 0.9947 - val_loss: 0.2953 - val_accuracy: 0.9215 - 21s/epoch - 54ms/step\n",
      "Epoch 184/200\n",
      "390/390 - 21s - loss: 0.0198 - accuracy: 0.9945 - val_loss: 0.2372 - val_accuracy: 0.9340 - 21s/epoch - 54ms/step\n",
      "Epoch 185/200\n",
      "390/390 - 21s - loss: 0.0158 - accuracy: 0.9960 - val_loss: 0.2518 - val_accuracy: 0.9309 - 21s/epoch - 54ms/step\n",
      "Epoch 186/200\n",
      "390/390 - 21s - loss: 0.0136 - accuracy: 0.9968 - val_loss: 0.2211 - val_accuracy: 0.9388 - 21s/epoch - 53ms/step\n",
      "Epoch 187/200\n",
      "390/390 - 20s - loss: 0.0133 - accuracy: 0.9969 - val_loss: 0.2219 - val_accuracy: 0.9367 - 20s/epoch - 51ms/step\n",
      "Epoch 188/200\n",
      "390/390 - 21s - loss: 0.0118 - accuracy: 0.9975 - val_loss: 0.2310 - val_accuracy: 0.9367 - 21s/epoch - 54ms/step\n",
      "Epoch 189/200\n",
      "390/390 - 20s - loss: 0.0119 - accuracy: 0.9977 - val_loss: 0.2215 - val_accuracy: 0.9382 - 20s/epoch - 51ms/step\n",
      "Epoch 190/200\n",
      "390/390 - 21s - loss: 0.0114 - accuracy: 0.9978 - val_loss: 0.2209 - val_accuracy: 0.9378 - 21s/epoch - 54ms/step\n",
      "Epoch 191/200\n",
      "390/390 - 20s - loss: 0.0109 - accuracy: 0.9982 - val_loss: 0.2125 - val_accuracy: 0.9405 - 20s/epoch - 52ms/step\n",
      "Epoch 192/200\n",
      "390/390 - 21s - loss: 0.0105 - accuracy: 0.9986 - val_loss: 0.2023 - val_accuracy: 0.9424 - 21s/epoch - 54ms/step\n",
      "Epoch 193/200\n",
      "390/390 - 21s - loss: 0.0109 - accuracy: 0.9986 - val_loss: 0.2011 - val_accuracy: 0.9420 - 21s/epoch - 54ms/step\n",
      "Epoch 194/200\n",
      "390/390 - 21s - loss: 0.0120 - accuracy: 0.9986 - val_loss: 0.1967 - val_accuracy: 0.9424 - 21s/epoch - 55ms/step\n",
      "Epoch 195/200\n",
      "390/390 - 21s - loss: 0.0124 - accuracy: 0.9987 - val_loss: 0.1938 - val_accuracy: 0.9431 - 21s/epoch - 54ms/step\n",
      "Epoch 196/200\n",
      "390/390 - 22s - loss: 0.0143 - accuracy: 0.9988 - val_loss: 0.1937 - val_accuracy: 0.9418 - 22s/epoch - 55ms/step\n",
      "Epoch 197/200\n",
      "390/390 - 20s - loss: 0.0159 - accuracy: 0.9989 - val_loss: 0.1904 - val_accuracy: 0.9422 - 20s/epoch - 51ms/step\n",
      "Epoch 198/200\n",
      "390/390 - 21s - loss: 0.0189 - accuracy: 0.9987 - val_loss: 0.1874 - val_accuracy: 0.9424 - 21s/epoch - 53ms/step\n",
      "Epoch 199/200\n",
      "390/390 - 20s - loss: 0.0231 - accuracy: 0.9987 - val_loss: 0.1873 - val_accuracy: 0.9427 - 20s/epoch - 51ms/step\n",
      "Epoch 200/200\n",
      "390/390 - 21s - loss: 0.0284 - accuracy: 0.9989 - val_loss: 0.1883 - val_accuracy: 0.9433 - 21s/epoch - 54ms/step\n",
      " Using best val_acc=0.9433 from last 20 epochs\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.1883 - accuracy: 0.9433\n",
      " Finished: Lam=0.9, Repeat=3, Acc=0.9433\n",
      "\n",
      " lambda: Lam=0.8, Repeat=1/3\n",
      "26 0\n",
      "51 1\n",
      "42 2\n",
      "89 0\n",
      "91 1\n",
      "96 2\n",
      "191 0\n",
      "175 1\n",
      "176 2\n",
      "336 0\n",
      "420 1\n",
      "351 2\n",
      "269001286 561714176\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 1/200\n",
      "390/390 - 24s - loss: 0.7575 - accuracy: 0.7509 - val_loss: 2.7183 - val_accuracy: 0.3828 - 24s/epoch - 62ms/step\n",
      "Epoch 2/200\n",
      "390/390 - 20s - loss: 0.6247 - accuracy: 0.7882 - val_loss: 1.5454 - val_accuracy: 0.5765 - 20s/epoch - 51ms/step\n",
      "Epoch 3/200\n",
      "390/390 - 22s - loss: 0.5277 - accuracy: 0.8205 - val_loss: 1.0275 - val_accuracy: 0.7122 - 22s/epoch - 55ms/step\n",
      "Epoch 4/200\n",
      "390/390 - 20s - loss: 0.4651 - accuracy: 0.8421 - val_loss: 0.6697 - val_accuracy: 0.7809 - 20s/epoch - 51ms/step\n",
      "Epoch 5/200\n",
      "390/390 - 21s - loss: 0.4226 - accuracy: 0.8562 - val_loss: 0.6544 - val_accuracy: 0.7995 - 21s/epoch - 55ms/step\n",
      "Epoch 6/200\n",
      "390/390 - 20s - loss: 0.3791 - accuracy: 0.8697 - val_loss: 0.6251 - val_accuracy: 0.8037 - 20s/epoch - 51ms/step\n",
      "Epoch 7/200\n",
      "390/390 - 21s - loss: 0.3269 - accuracy: 0.8881 - val_loss: 0.4122 - val_accuracy: 0.8589 - 21s/epoch - 54ms/step\n",
      "Epoch 8/200\n",
      "390/390 - 20s - loss: 0.2974 - accuracy: 0.8977 - val_loss: 0.4100 - val_accuracy: 0.8634 - 20s/epoch - 50ms/step\n",
      "Epoch 9/200\n",
      "390/390 - 20s - loss: 0.2768 - accuracy: 0.9043 - val_loss: 0.4597 - val_accuracy: 0.8435 - 20s/epoch - 51ms/step\n",
      "Epoch 10/200\n",
      "390/390 - 21s - loss: 0.2552 - accuracy: 0.9110 - val_loss: 0.3865 - val_accuracy: 0.8731 - 21s/epoch - 53ms/step\n",
      "Epoch 11/200\n",
      "390/390 - 20s - loss: 0.2442 - accuracy: 0.9151 - val_loss: 0.4918 - val_accuracy: 0.8464 - 20s/epoch - 51ms/step\n",
      "Epoch 12/200\n",
      "390/390 - 21s - loss: 0.2272 - accuracy: 0.9214 - val_loss: 0.4143 - val_accuracy: 0.8655 - 21s/epoch - 54ms/step\n",
      "Epoch 13/200\n",
      "390/390 - 20s - loss: 0.2166 - accuracy: 0.9249 - val_loss: 0.4575 - val_accuracy: 0.8570 - 20s/epoch - 51ms/step\n",
      "Epoch 14/200\n",
      "390/390 - 21s - loss: 0.2065 - accuracy: 0.9282 - val_loss: 0.4173 - val_accuracy: 0.8675 - 21s/epoch - 53ms/step\n",
      "Epoch 15/200\n",
      "390/390 - 21s - loss: 0.1995 - accuracy: 0.9297 - val_loss: 0.4845 - val_accuracy: 0.8551 - 21s/epoch - 53ms/step\n",
      "Epoch 16/200\n",
      "390/390 - 20s - loss: 0.1942 - accuracy: 0.9328 - val_loss: 0.3523 - val_accuracy: 0.8911 - 20s/epoch - 51ms/step\n",
      "Epoch 17/200\n",
      "390/390 - 21s - loss: 0.1846 - accuracy: 0.9361 - val_loss: 0.4145 - val_accuracy: 0.8710 - 21s/epoch - 54ms/step\n",
      "Epoch 18/200\n",
      "390/390 - 21s - loss: 0.1815 - accuracy: 0.9370 - val_loss: 0.3881 - val_accuracy: 0.8745 - 21s/epoch - 54ms/step\n",
      "Epoch 19/200\n",
      "390/390 - 20s - loss: 0.1785 - accuracy: 0.9368 - val_loss: 0.3568 - val_accuracy: 0.8873 - 20s/epoch - 51ms/step\n",
      "Epoch 20/200\n",
      "390/390 - 21s - loss: 0.1688 - accuracy: 0.9407 - val_loss: 0.4700 - val_accuracy: 0.8607 - 21s/epoch - 54ms/step\n",
      "Epoch 21/200\n",
      "390/390 - 21s - loss: 0.1650 - accuracy: 0.9422 - val_loss: 0.4744 - val_accuracy: 0.8618 - 21s/epoch - 53ms/step\n",
      "Epoch 22/200\n",
      "390/390 - 21s - loss: 0.1632 - accuracy: 0.9436 - val_loss: 0.5496 - val_accuracy: 0.8468 - 21s/epoch - 53ms/step\n",
      "Epoch 23/200\n",
      "390/390 - 21s - loss: 0.1607 - accuracy: 0.9446 - val_loss: 0.3482 - val_accuracy: 0.8912 - 21s/epoch - 54ms/step\n",
      "Epoch 24/200\n",
      "390/390 - 21s - loss: 0.1551 - accuracy: 0.9455 - val_loss: 0.4094 - val_accuracy: 0.8783 - 21s/epoch - 53ms/step\n",
      "Epoch 25/200\n",
      "390/390 - 21s - loss: 0.1535 - accuracy: 0.9462 - val_loss: 0.3286 - val_accuracy: 0.8987 - 21s/epoch - 54ms/step\n",
      "Epoch 26/200\n",
      "390/390 - 20s - loss: 0.1481 - accuracy: 0.9482 - val_loss: 0.3911 - val_accuracy: 0.8806 - 20s/epoch - 52ms/step\n",
      "Epoch 27/200\n",
      "390/390 - 21s - loss: 0.1454 - accuracy: 0.9487 - val_loss: 0.3663 - val_accuracy: 0.8817 - 21s/epoch - 53ms/step\n",
      "Epoch 28/200\n",
      "390/390 - 22s - loss: 0.1473 - accuracy: 0.9483 - val_loss: 0.3294 - val_accuracy: 0.8971 - 22s/epoch - 55ms/step\n",
      "Epoch 29/200\n",
      "390/390 - 21s - loss: 0.1391 - accuracy: 0.9516 - val_loss: 0.4661 - val_accuracy: 0.8661 - 21s/epoch - 54ms/step\n",
      "Epoch 30/200\n",
      "390/390 - 22s - loss: 0.1424 - accuracy: 0.9511 - val_loss: 0.3496 - val_accuracy: 0.8950 - 22s/epoch - 56ms/step\n",
      "Epoch 31/200\n",
      "390/390 - 22s - loss: 0.1332 - accuracy: 0.9542 - val_loss: 0.3606 - val_accuracy: 0.8951 - 22s/epoch - 55ms/step\n",
      "Epoch 32/200\n",
      "390/390 - 21s - loss: 0.1398 - accuracy: 0.9510 - val_loss: 0.4373 - val_accuracy: 0.8790 - 21s/epoch - 55ms/step\n",
      "Epoch 33/200\n",
      "390/390 - 21s - loss: 0.1349 - accuracy: 0.9521 - val_loss: 0.2985 - val_accuracy: 0.9004 - 21s/epoch - 53ms/step\n",
      "Epoch 34/200\n",
      "390/390 - 21s - loss: 0.1314 - accuracy: 0.9530 - val_loss: 0.3787 - val_accuracy: 0.8806 - 21s/epoch - 53ms/step\n",
      "Epoch 35/200\n",
      "390/390 - 20s - loss: 0.1360 - accuracy: 0.9522 - val_loss: 0.3285 - val_accuracy: 0.9032 - 20s/epoch - 51ms/step\n",
      "Epoch 36/200\n",
      "390/390 - 21s - loss: 0.1290 - accuracy: 0.9547 - val_loss: 0.4428 - val_accuracy: 0.8729 - 21s/epoch - 54ms/step\n",
      "Epoch 37/200\n",
      "390/390 - 21s - loss: 0.1234 - accuracy: 0.9563 - val_loss: 0.3358 - val_accuracy: 0.8987 - 21s/epoch - 54ms/step\n",
      "Epoch 38/200\n",
      "390/390 - 21s - loss: 0.1231 - accuracy: 0.9566 - val_loss: 0.4586 - val_accuracy: 0.8702 - 21s/epoch - 55ms/step\n",
      "Epoch 39/200\n",
      "390/390 - 21s - loss: 0.1229 - accuracy: 0.9578 - val_loss: 0.3408 - val_accuracy: 0.8970 - 21s/epoch - 53ms/step\n",
      "Epoch 40/200\n",
      "390/390 - 21s - loss: 0.1243 - accuracy: 0.9572 - val_loss: 0.5399 - val_accuracy: 0.8529 - 21s/epoch - 54ms/step\n",
      "Epoch 41/200\n",
      "390/390 - 21s - loss: 0.1200 - accuracy: 0.9580 - val_loss: 0.4870 - val_accuracy: 0.8679 - 21s/epoch - 53ms/step\n",
      "Epoch 42/200\n",
      "390/390 - 21s - loss: 0.1215 - accuracy: 0.9571 - val_loss: 0.4224 - val_accuracy: 0.8805 - 21s/epoch - 54ms/step\n",
      "Epoch 43/200\n",
      "390/390 - 21s - loss: 0.1183 - accuracy: 0.9590 - val_loss: 0.3177 - val_accuracy: 0.9064 - 21s/epoch - 55ms/step\n",
      "Epoch 44/200\n",
      "390/390 - 20s - loss: 0.1178 - accuracy: 0.9589 - val_loss: 0.4861 - val_accuracy: 0.8694 - 20s/epoch - 51ms/step\n",
      "Epoch 45/200\n",
      "390/390 - 21s - loss: 0.1150 - accuracy: 0.9593 - val_loss: 0.3814 - val_accuracy: 0.8887 - 21s/epoch - 53ms/step\n",
      "Epoch 46/200\n",
      "390/390 - 20s - loss: 0.1200 - accuracy: 0.9589 - val_loss: 0.4368 - val_accuracy: 0.8786 - 20s/epoch - 51ms/step\n",
      "Epoch 47/200\n",
      "390/390 - 22s - loss: 0.1165 - accuracy: 0.9590 - val_loss: 0.4129 - val_accuracy: 0.8823 - 22s/epoch - 56ms/step\n",
      "Epoch 48/200\n",
      "390/390 - 22s - loss: 0.1168 - accuracy: 0.9597 - val_loss: 0.4494 - val_accuracy: 0.8791 - 22s/epoch - 56ms/step\n",
      "Epoch 49/200\n",
      "390/390 - 21s - loss: 0.1137 - accuracy: 0.9606 - val_loss: 0.4776 - val_accuracy: 0.8671 - 21s/epoch - 54ms/step\n",
      "Epoch 50/200\n",
      "390/390 - 20s - loss: 0.1131 - accuracy: 0.9601 - val_loss: 0.3878 - val_accuracy: 0.8856 - 20s/epoch - 52ms/step\n",
      "Epoch 51/200\n",
      "390/390 - 21s - loss: 0.1120 - accuracy: 0.9613 - val_loss: 0.4228 - val_accuracy: 0.8777 - 21s/epoch - 54ms/step\n",
      "Epoch 52/200\n",
      "390/390 - 21s - loss: 0.1105 - accuracy: 0.9617 - val_loss: 0.5063 - val_accuracy: 0.8702 - 21s/epoch - 53ms/step\n",
      "Epoch 53/200\n",
      "390/390 - 21s - loss: 0.1082 - accuracy: 0.9617 - val_loss: 0.4363 - val_accuracy: 0.8798 - 21s/epoch - 53ms/step\n",
      "Epoch 54/200\n",
      "390/390 - 20s - loss: 0.1105 - accuracy: 0.9619 - val_loss: 0.4133 - val_accuracy: 0.8859 - 20s/epoch - 51ms/step\n",
      "Epoch 55/200\n",
      "390/390 - 20s - loss: 0.1096 - accuracy: 0.9612 - val_loss: 0.5269 - val_accuracy: 0.8662 - 20s/epoch - 51ms/step\n",
      "Epoch 56/200\n",
      "390/390 - 20s - loss: 0.1077 - accuracy: 0.9618 - val_loss: 0.3334 - val_accuracy: 0.9059 - 20s/epoch - 51ms/step\n",
      "Epoch 57/200\n",
      "390/390 - 21s - loss: 0.1080 - accuracy: 0.9619 - val_loss: 0.4030 - val_accuracy: 0.8811 - 21s/epoch - 54ms/step\n",
      "Epoch 58/200\n",
      "390/390 - 21s - loss: 0.1055 - accuracy: 0.9635 - val_loss: 0.3171 - val_accuracy: 0.9099 - 21s/epoch - 53ms/step\n",
      "Epoch 59/200\n",
      "390/390 - 21s - loss: 0.1078 - accuracy: 0.9618 - val_loss: 0.4203 - val_accuracy: 0.8852 - 21s/epoch - 53ms/step\n",
      "Epoch 60/200\n",
      "390/390 - 21s - loss: 0.1062 - accuracy: 0.9636 - val_loss: 0.4010 - val_accuracy: 0.8852 - 21s/epoch - 53ms/step\n",
      "Epoch 61/200\n",
      "390/390 - 21s - loss: 0.1030 - accuracy: 0.9639 - val_loss: 0.3661 - val_accuracy: 0.8911 - 21s/epoch - 54ms/step\n",
      "Epoch 62/200\n",
      "390/390 - 21s - loss: 0.1022 - accuracy: 0.9644 - val_loss: 0.3813 - val_accuracy: 0.8939 - 21s/epoch - 54ms/step\n",
      "Epoch 63/200\n",
      "390/390 - 20s - loss: 0.1047 - accuracy: 0.9640 - val_loss: 0.4257 - val_accuracy: 0.8861 - 20s/epoch - 51ms/step\n",
      "Epoch 64/200\n",
      "390/390 - 22s - loss: 0.1046 - accuracy: 0.9630 - val_loss: 0.3889 - val_accuracy: 0.8909 - 22s/epoch - 55ms/step\n",
      "Epoch 65/200\n",
      "390/390 - 20s - loss: 0.0998 - accuracy: 0.9665 - val_loss: 0.3311 - val_accuracy: 0.9038 - 20s/epoch - 51ms/step\n",
      "Epoch 66/200\n",
      "390/390 - 21s - loss: 0.1016 - accuracy: 0.9645 - val_loss: 0.3638 - val_accuracy: 0.8983 - 21s/epoch - 54ms/step\n",
      "Epoch 67/200\n",
      "390/390 - 22s - loss: 0.0987 - accuracy: 0.9656 - val_loss: 0.3584 - val_accuracy: 0.8986 - 22s/epoch - 55ms/step\n",
      "Epoch 68/200\n",
      "390/390 - 22s - loss: 0.0987 - accuracy: 0.9658 - val_loss: 0.3602 - val_accuracy: 0.9026 - 22s/epoch - 55ms/step\n",
      "Epoch 69/200\n",
      "390/390 - 21s - loss: 0.1007 - accuracy: 0.9642 - val_loss: 0.3753 - val_accuracy: 0.8955 - 21s/epoch - 54ms/step\n",
      "Epoch 70/200\n",
      "390/390 - 21s - loss: 0.0973 - accuracy: 0.9663 - val_loss: 0.3916 - val_accuracy: 0.8864 - 21s/epoch - 53ms/step\n",
      "Epoch 71/200\n",
      "390/390 - 20s - loss: 0.0988 - accuracy: 0.9651 - val_loss: 0.3682 - val_accuracy: 0.8983 - 20s/epoch - 52ms/step\n",
      "Epoch 72/200\n",
      "390/390 - 20s - loss: 0.0995 - accuracy: 0.9647 - val_loss: 0.3829 - val_accuracy: 0.8934 - 20s/epoch - 51ms/step\n",
      "Epoch 73/200\n",
      "390/390 - 21s - loss: 0.0987 - accuracy: 0.9649 - val_loss: 0.3636 - val_accuracy: 0.8931 - 21s/epoch - 54ms/step\n",
      "Epoch 74/200\n",
      "390/390 - 21s - loss: 0.0946 - accuracy: 0.9681 - val_loss: 0.2983 - val_accuracy: 0.9152 - 21s/epoch - 54ms/step\n",
      "Epoch 75/200\n",
      "390/390 - 20s - loss: 0.0961 - accuracy: 0.9671 - val_loss: 0.4332 - val_accuracy: 0.8873 - 20s/epoch - 51ms/step\n",
      "Epoch 76/200\n",
      "390/390 - 20s - loss: 0.1001 - accuracy: 0.9652 - val_loss: 0.4479 - val_accuracy: 0.8761 - 20s/epoch - 51ms/step\n",
      "Epoch 77/200\n",
      "390/390 - 21s - loss: 0.0959 - accuracy: 0.9668 - val_loss: 0.4418 - val_accuracy: 0.8793 - 21s/epoch - 53ms/step\n",
      "Epoch 78/200\n",
      "390/390 - 20s - loss: 0.0949 - accuracy: 0.9669 - val_loss: 0.3997 - val_accuracy: 0.8870 - 20s/epoch - 51ms/step\n",
      "Epoch 79/200\n",
      "390/390 - 20s - loss: 0.0989 - accuracy: 0.9657 - val_loss: 0.3287 - val_accuracy: 0.9032 - 20s/epoch - 51ms/step\n",
      "Epoch 80/200\n",
      "390/390 - 21s - loss: 0.0923 - accuracy: 0.9676 - val_loss: 0.3976 - val_accuracy: 0.8866 - 21s/epoch - 54ms/step\n",
      "Epoch 81/200\n",
      "390/390 - 21s - loss: 0.0948 - accuracy: 0.9676 - val_loss: 0.3403 - val_accuracy: 0.9001 - 21s/epoch - 53ms/step\n",
      "Epoch 82/200\n",
      "390/390 - 20s - loss: 0.0899 - accuracy: 0.9681 - val_loss: 0.3444 - val_accuracy: 0.9084 - 20s/epoch - 51ms/step\n",
      "Epoch 83/200\n",
      "390/390 - 20s - loss: 0.0947 - accuracy: 0.9673 - val_loss: 0.4492 - val_accuracy: 0.8733 - 20s/epoch - 52ms/step\n",
      "Epoch 84/200\n",
      "390/390 - 20s - loss: 0.0925 - accuracy: 0.9670 - val_loss: 0.4395 - val_accuracy: 0.8822 - 20s/epoch - 51ms/step\n",
      "Epoch 85/200\n",
      "390/390 - 21s - loss: 0.0903 - accuracy: 0.9686 - val_loss: 0.3774 - val_accuracy: 0.8965 - 21s/epoch - 53ms/step\n",
      "Epoch 86/200\n",
      "390/390 - 20s - loss: 0.0960 - accuracy: 0.9673 - val_loss: 0.3525 - val_accuracy: 0.9006 - 20s/epoch - 51ms/step\n",
      "Epoch 87/200\n",
      "390/390 - 21s - loss: 0.0919 - accuracy: 0.9675 - val_loss: 0.3074 - val_accuracy: 0.9107 - 21s/epoch - 53ms/step\n",
      "Epoch 88/200\n",
      "390/390 - 20s - loss: 0.0893 - accuracy: 0.9690 - val_loss: 0.4019 - val_accuracy: 0.8953 - 20s/epoch - 52ms/step\n",
      "Epoch 89/200\n",
      "390/390 - 20s - loss: 0.0869 - accuracy: 0.9703 - val_loss: 0.4462 - val_accuracy: 0.8799 - 20s/epoch - 52ms/step\n",
      "Epoch 90/200\n",
      "390/390 - 20s - loss: 0.0895 - accuracy: 0.9687 - val_loss: 0.3997 - val_accuracy: 0.8868 - 20s/epoch - 51ms/step\n",
      "Epoch 91/200\n",
      "390/390 - 20s - loss: 0.0881 - accuracy: 0.9702 - val_loss: 0.3915 - val_accuracy: 0.8932 - 20s/epoch - 52ms/step\n",
      "Epoch 92/200\n",
      "390/390 - 20s - loss: 0.0894 - accuracy: 0.9695 - val_loss: 0.3435 - val_accuracy: 0.9031 - 20s/epoch - 51ms/step\n",
      "Epoch 93/200\n",
      "390/390 - 20s - loss: 0.0828 - accuracy: 0.9717 - val_loss: 0.3403 - val_accuracy: 0.9054 - 20s/epoch - 51ms/step\n",
      "Epoch 94/200\n",
      "390/390 - 21s - loss: 0.0907 - accuracy: 0.9690 - val_loss: 0.4169 - val_accuracy: 0.8866 - 21s/epoch - 53ms/step\n",
      "Epoch 95/200\n",
      "390/390 - 21s - loss: 0.0866 - accuracy: 0.9696 - val_loss: 0.3989 - val_accuracy: 0.8916 - 21s/epoch - 55ms/step\n",
      "Epoch 96/200\n",
      "390/390 - 20s - loss: 0.0878 - accuracy: 0.9705 - val_loss: 0.3426 - val_accuracy: 0.9068 - 20s/epoch - 51ms/step\n",
      "Epoch 97/200\n",
      "390/390 - 21s - loss: 0.0861 - accuracy: 0.9698 - val_loss: 0.4395 - val_accuracy: 0.8829 - 21s/epoch - 54ms/step\n",
      "Epoch 98/200\n",
      "390/390 - 20s - loss: 0.0865 - accuracy: 0.9700 - val_loss: 0.4633 - val_accuracy: 0.8755 - 20s/epoch - 51ms/step\n",
      "Epoch 99/200\n",
      "390/390 - 21s - loss: 0.0866 - accuracy: 0.9695 - val_loss: 0.3736 - val_accuracy: 0.8956 - 21s/epoch - 53ms/step\n",
      "Epoch 100/200\n",
      "390/390 - 21s - loss: 0.0852 - accuracy: 0.9704 - val_loss: 0.3715 - val_accuracy: 0.8969 - 21s/epoch - 53ms/step\n",
      "Epoch 101/200\n",
      "390/390 - 20s - loss: 0.0864 - accuracy: 0.9699 - val_loss: 0.4538 - val_accuracy: 0.8783 - 20s/epoch - 51ms/step\n",
      "Epoch 102/200\n",
      "390/390 - 20s - loss: 0.0863 - accuracy: 0.9706 - val_loss: 0.3591 - val_accuracy: 0.9009 - 20s/epoch - 50ms/step\n",
      "Epoch 103/200\n",
      "390/390 - 20s - loss: 0.0798 - accuracy: 0.9724 - val_loss: 0.3921 - val_accuracy: 0.8973 - 20s/epoch - 51ms/step\n",
      "Epoch 104/200\n",
      "390/390 - 20s - loss: 0.0855 - accuracy: 0.9700 - val_loss: 0.3939 - val_accuracy: 0.8951 - 20s/epoch - 51ms/step\n",
      "Epoch 105/200\n",
      "390/390 - 21s - loss: 0.0805 - accuracy: 0.9724 - val_loss: 0.5018 - val_accuracy: 0.8688 - 21s/epoch - 53ms/step\n",
      "Epoch 106/200\n",
      "390/390 - 21s - loss: 0.0832 - accuracy: 0.9715 - val_loss: 0.3497 - val_accuracy: 0.9021 - 21s/epoch - 53ms/step\n",
      "Epoch 107/200\n",
      "390/390 - 20s - loss: 0.0808 - accuracy: 0.9723 - val_loss: 0.3475 - val_accuracy: 0.9023 - 20s/epoch - 51ms/step\n",
      "Epoch 108/200\n",
      "390/390 - 21s - loss: 0.0807 - accuracy: 0.9721 - val_loss: 0.4015 - val_accuracy: 0.8923 - 21s/epoch - 53ms/step\n",
      "Epoch 109/200\n",
      "390/390 - 21s - loss: 0.0808 - accuracy: 0.9721 - val_loss: 0.3347 - val_accuracy: 0.9090 - 21s/epoch - 54ms/step\n",
      "Epoch 110/200\n",
      "390/390 - 20s - loss: 0.0784 - accuracy: 0.9731 - val_loss: 0.3861 - val_accuracy: 0.8920 - 20s/epoch - 52ms/step\n",
      "Epoch 111/200\n",
      "390/390 - 20s - loss: 0.0823 - accuracy: 0.9720 - val_loss: 0.4122 - val_accuracy: 0.8839 - 20s/epoch - 52ms/step\n",
      "Epoch 112/200\n",
      "390/390 - 22s - loss: 0.0824 - accuracy: 0.9713 - val_loss: 0.6713 - val_accuracy: 0.8433 - 22s/epoch - 56ms/step\n",
      "Epoch 113/200\n",
      "390/390 - 20s - loss: 0.0779 - accuracy: 0.9730 - val_loss: 0.4549 - val_accuracy: 0.8778 - 20s/epoch - 51ms/step\n",
      "Epoch 114/200\n",
      "390/390 - 21s - loss: 0.0741 - accuracy: 0.9742 - val_loss: 0.3917 - val_accuracy: 0.8965 - 21s/epoch - 53ms/step\n",
      "Epoch 115/200\n",
      "390/390 - 21s - loss: 0.0767 - accuracy: 0.9738 - val_loss: 0.3194 - val_accuracy: 0.9115 - 21s/epoch - 54ms/step\n",
      "Epoch 116/200\n",
      "390/390 - 20s - loss: 0.0777 - accuracy: 0.9725 - val_loss: 0.4281 - val_accuracy: 0.8900 - 20s/epoch - 51ms/step\n",
      "Epoch 117/200\n",
      "390/390 - 20s - loss: 0.0730 - accuracy: 0.9753 - val_loss: 0.4099 - val_accuracy: 0.8976 - 20s/epoch - 51ms/step\n",
      "Epoch 118/200\n",
      "390/390 - 21s - loss: 0.0750 - accuracy: 0.9736 - val_loss: 0.3080 - val_accuracy: 0.9122 - 21s/epoch - 53ms/step\n",
      "Epoch 119/200\n",
      "390/390 - 20s - loss: 0.0850 - accuracy: 0.9708 - val_loss: 0.4080 - val_accuracy: 0.8893 - 20s/epoch - 52ms/step\n",
      "Epoch 120/200\n",
      "390/390 - 21s - loss: 0.0747 - accuracy: 0.9745 - val_loss: 0.3143 - val_accuracy: 0.9141 - 21s/epoch - 54ms/step\n",
      "Epoch 121/200\n",
      "390/390 - 21s - loss: 0.0713 - accuracy: 0.9750 - val_loss: 0.4012 - val_accuracy: 0.8864 - 21s/epoch - 54ms/step\n",
      "Epoch 122/200\n",
      "390/390 - 20s - loss: 0.0749 - accuracy: 0.9736 - val_loss: 0.3864 - val_accuracy: 0.8992 - 20s/epoch - 51ms/step\n",
      "Epoch 123/200\n",
      "390/390 - 21s - loss: 0.0773 - accuracy: 0.9740 - val_loss: 0.3434 - val_accuracy: 0.9038 - 21s/epoch - 53ms/step\n",
      "Epoch 124/200\n",
      "390/390 - 20s - loss: 0.0717 - accuracy: 0.9752 - val_loss: 0.3561 - val_accuracy: 0.9070 - 20s/epoch - 52ms/step\n",
      "Epoch 125/200\n",
      "390/390 - 21s - loss: 0.0669 - accuracy: 0.9767 - val_loss: 0.3456 - val_accuracy: 0.9053 - 21s/epoch - 53ms/step\n",
      "Epoch 126/200\n",
      "390/390 - 21s - loss: 0.0677 - accuracy: 0.9768 - val_loss: 0.3237 - val_accuracy: 0.9103 - 21s/epoch - 54ms/step\n",
      "Epoch 127/200\n",
      "390/390 - 21s - loss: 0.0746 - accuracy: 0.9741 - val_loss: 0.4166 - val_accuracy: 0.8871 - 21s/epoch - 55ms/step\n",
      "Epoch 128/200\n",
      "390/390 - 20s - loss: 0.0746 - accuracy: 0.9750 - val_loss: 0.3567 - val_accuracy: 0.8994 - 20s/epoch - 52ms/step\n",
      "Epoch 129/200\n",
      "390/390 - 21s - loss: 0.0697 - accuracy: 0.9769 - val_loss: 0.3383 - val_accuracy: 0.9078 - 21s/epoch - 53ms/step\n",
      "Epoch 130/200\n",
      "390/390 - 21s - loss: 0.0676 - accuracy: 0.9768 - val_loss: 0.4323 - val_accuracy: 0.8878 - 21s/epoch - 53ms/step\n",
      "Epoch 131/200\n",
      "390/390 - 21s - loss: 0.0698 - accuracy: 0.9756 - val_loss: 0.4145 - val_accuracy: 0.8923 - 21s/epoch - 54ms/step\n",
      "Epoch 132/200\n",
      "390/390 - 21s - loss: 0.0674 - accuracy: 0.9774 - val_loss: 0.3851 - val_accuracy: 0.8964 - 21s/epoch - 54ms/step\n",
      "Epoch 133/200\n",
      "390/390 - 21s - loss: 0.0674 - accuracy: 0.9764 - val_loss: 0.3475 - val_accuracy: 0.9062 - 21s/epoch - 54ms/step\n",
      "Epoch 134/200\n",
      "390/390 - 21s - loss: 0.0685 - accuracy: 0.9767 - val_loss: 0.4373 - val_accuracy: 0.8885 - 21s/epoch - 54ms/step\n",
      "Epoch 135/200\n",
      "390/390 - 21s - loss: 0.0648 - accuracy: 0.9778 - val_loss: 0.3483 - val_accuracy: 0.9081 - 21s/epoch - 54ms/step\n",
      "Epoch 136/200\n",
      "390/390 - 20s - loss: 0.0678 - accuracy: 0.9772 - val_loss: 0.3730 - val_accuracy: 0.9017 - 20s/epoch - 51ms/step\n",
      "Epoch 137/200\n",
      "390/390 - 20s - loss: 0.0651 - accuracy: 0.9775 - val_loss: 0.3703 - val_accuracy: 0.8980 - 20s/epoch - 51ms/step\n",
      "Epoch 138/200\n",
      "390/390 - 21s - loss: 0.0730 - accuracy: 0.9750 - val_loss: 0.4366 - val_accuracy: 0.8838 - 21s/epoch - 54ms/step\n",
      "Epoch 139/200\n",
      "390/390 - 21s - loss: 0.0624 - accuracy: 0.9781 - val_loss: 0.3420 - val_accuracy: 0.9087 - 21s/epoch - 54ms/step\n",
      "Epoch 140/200\n",
      "390/390 - 20s - loss: 0.0629 - accuracy: 0.9790 - val_loss: 0.3470 - val_accuracy: 0.9039 - 20s/epoch - 52ms/step\n",
      "Epoch 141/200\n",
      "390/390 - 21s - loss: 0.0620 - accuracy: 0.9785 - val_loss: 0.3605 - val_accuracy: 0.9028 - 21s/epoch - 55ms/step\n",
      "Epoch 142/200\n",
      "390/390 - 21s - loss: 0.0599 - accuracy: 0.9792 - val_loss: 0.3063 - val_accuracy: 0.9144 - 21s/epoch - 54ms/step\n",
      "Epoch 143/200\n",
      "390/390 - 21s - loss: 0.0612 - accuracy: 0.9789 - val_loss: 0.3000 - val_accuracy: 0.9150 - 21s/epoch - 53ms/step\n",
      "Epoch 144/200\n",
      "390/390 - 20s - loss: 0.0616 - accuracy: 0.9788 - val_loss: 0.3057 - val_accuracy: 0.9150 - 20s/epoch - 51ms/step\n",
      "Epoch 145/200\n",
      "390/390 - 21s - loss: 0.0604 - accuracy: 0.9796 - val_loss: 0.3669 - val_accuracy: 0.8982 - 21s/epoch - 54ms/step\n",
      "Epoch 146/200\n",
      "390/390 - 20s - loss: 0.0579 - accuracy: 0.9800 - val_loss: 0.3002 - val_accuracy: 0.9140 - 20s/epoch - 52ms/step\n",
      "Epoch 147/200\n",
      "390/390 - 21s - loss: 0.0612 - accuracy: 0.9800 - val_loss: 0.3383 - val_accuracy: 0.9064 - 21s/epoch - 54ms/step\n",
      "Epoch 148/200\n",
      "390/390 - 21s - loss: 0.0602 - accuracy: 0.9794 - val_loss: 0.3742 - val_accuracy: 0.9013 - 21s/epoch - 55ms/step\n",
      "Epoch 149/200\n",
      "390/390 - 21s - loss: 0.0585 - accuracy: 0.9797 - val_loss: 0.3643 - val_accuracy: 0.9007 - 21s/epoch - 54ms/step\n",
      "Epoch 150/200\n",
      "390/390 - 21s - loss: 0.0562 - accuracy: 0.9806 - val_loss: 0.3669 - val_accuracy: 0.9034 - 21s/epoch - 54ms/step\n",
      "Epoch 151/200\n",
      "390/390 - 21s - loss: 0.0568 - accuracy: 0.9813 - val_loss: 0.3405 - val_accuracy: 0.9092 - 21s/epoch - 54ms/step\n",
      "Epoch 152/200\n",
      "390/390 - 20s - loss: 0.0565 - accuracy: 0.9811 - val_loss: 0.3346 - val_accuracy: 0.9112 - 20s/epoch - 51ms/step\n",
      "Epoch 153/200\n",
      "390/390 - 20s - loss: 0.0566 - accuracy: 0.9803 - val_loss: 0.3080 - val_accuracy: 0.9147 - 20s/epoch - 51ms/step\n",
      "Epoch 154/200\n",
      "390/390 - 21s - loss: 0.0490 - accuracy: 0.9837 - val_loss: 0.3382 - val_accuracy: 0.9108 - 21s/epoch - 53ms/step\n",
      "Epoch 155/200\n",
      "390/390 - 20s - loss: 0.0503 - accuracy: 0.9828 - val_loss: 0.4573 - val_accuracy: 0.8828 - 20s/epoch - 52ms/step\n",
      "Epoch 156/200\n",
      "390/390 - 21s - loss: 0.0556 - accuracy: 0.9807 - val_loss: 0.4952 - val_accuracy: 0.8749 - 21s/epoch - 55ms/step\n",
      "Epoch 157/200\n",
      "390/390 - 21s - loss: 0.0473 - accuracy: 0.9839 - val_loss: 0.4050 - val_accuracy: 0.8952 - 21s/epoch - 54ms/step\n",
      "Epoch 158/200\n",
      "390/390 - 20s - loss: 0.0519 - accuracy: 0.9821 - val_loss: 0.3228 - val_accuracy: 0.9114 - 20s/epoch - 51ms/step\n",
      "Epoch 159/200\n",
      "390/390 - 21s - loss: 0.0507 - accuracy: 0.9833 - val_loss: 0.3967 - val_accuracy: 0.8939 - 21s/epoch - 54ms/step\n",
      "Epoch 160/200\n",
      "390/390 - 21s - loss: 0.0484 - accuracy: 0.9836 - val_loss: 0.2971 - val_accuracy: 0.9174 - 21s/epoch - 55ms/step\n",
      "Epoch 161/200\n",
      "390/390 - 20s - loss: 0.0466 - accuracy: 0.9846 - val_loss: 0.3699 - val_accuracy: 0.9023 - 20s/epoch - 51ms/step\n",
      "Epoch 162/200\n",
      "390/390 - 21s - loss: 0.0466 - accuracy: 0.9844 - val_loss: 0.3433 - val_accuracy: 0.9102 - 21s/epoch - 54ms/step\n",
      "Epoch 163/200\n",
      "390/390 - 21s - loss: 0.0419 - accuracy: 0.9860 - val_loss: 0.3247 - val_accuracy: 0.9158 - 21s/epoch - 53ms/step\n",
      "Epoch 164/200\n",
      "390/390 - 20s - loss: 0.0413 - accuracy: 0.9863 - val_loss: 0.3659 - val_accuracy: 0.9010 - 20s/epoch - 51ms/step\n",
      "Epoch 165/200\n",
      "390/390 - 21s - loss: 0.0410 - accuracy: 0.9864 - val_loss: 0.2851 - val_accuracy: 0.9229 - 21s/epoch - 54ms/step\n",
      "Epoch 166/200\n",
      "390/390 - 20s - loss: 0.0414 - accuracy: 0.9860 - val_loss: 0.3197 - val_accuracy: 0.9144 - 20s/epoch - 51ms/step\n",
      "Epoch 167/200\n",
      "390/390 - 21s - loss: 0.0384 - accuracy: 0.9876 - val_loss: 0.3007 - val_accuracy: 0.9184 - 21s/epoch - 55ms/step\n",
      "Epoch 168/200\n",
      "390/390 - 21s - loss: 0.0350 - accuracy: 0.9888 - val_loss: 0.2990 - val_accuracy: 0.9221 - 21s/epoch - 53ms/step\n",
      "Epoch 169/200\n",
      "390/390 - 21s - loss: 0.0335 - accuracy: 0.9888 - val_loss: 0.3360 - val_accuracy: 0.9150 - 21s/epoch - 53ms/step\n",
      "Epoch 170/200\n",
      "390/390 - 21s - loss: 0.0365 - accuracy: 0.9878 - val_loss: 0.2968 - val_accuracy: 0.9195 - 21s/epoch - 53ms/step\n",
      "Epoch 171/200\n",
      "390/390 - 21s - loss: 0.0338 - accuracy: 0.9889 - val_loss: 0.3035 - val_accuracy: 0.9222 - 21s/epoch - 53ms/step\n",
      "Epoch 172/200\n",
      "390/390 - 20s - loss: 0.0325 - accuracy: 0.9899 - val_loss: 0.2712 - val_accuracy: 0.9246 - 20s/epoch - 51ms/step\n",
      "Epoch 173/200\n",
      "390/390 - 20s - loss: 0.0301 - accuracy: 0.9905 - val_loss: 0.3030 - val_accuracy: 0.9194 - 20s/epoch - 51ms/step\n",
      "Epoch 174/200\n",
      "390/390 - 20s - loss: 0.0297 - accuracy: 0.9905 - val_loss: 0.3302 - val_accuracy: 0.9160 - 20s/epoch - 51ms/step\n",
      "Epoch 175/200\n",
      "390/390 - 21s - loss: 0.0299 - accuracy: 0.9905 - val_loss: 0.3016 - val_accuracy: 0.9230 - 21s/epoch - 54ms/step\n",
      "Epoch 176/200\n",
      "390/390 - 21s - loss: 0.0276 - accuracy: 0.9911 - val_loss: 0.2887 - val_accuracy: 0.9210 - 21s/epoch - 54ms/step\n",
      "Epoch 177/200\n",
      "390/390 - 21s - loss: 0.0246 - accuracy: 0.9926 - val_loss: 0.2593 - val_accuracy: 0.9293 - 21s/epoch - 54ms/step\n",
      "Epoch 178/200\n",
      "390/390 - 21s - loss: 0.0194 - accuracy: 0.9946 - val_loss: 0.2860 - val_accuracy: 0.9228 - 21s/epoch - 54ms/step\n",
      "Epoch 179/200\n",
      "390/390 - 22s - loss: 0.0208 - accuracy: 0.9938 - val_loss: 0.2541 - val_accuracy: 0.9325 - 22s/epoch - 56ms/step\n",
      "Epoch 180/200\n",
      "390/390 - 21s - loss: 0.0206 - accuracy: 0.9941 - val_loss: 0.2995 - val_accuracy: 0.9219 - 21s/epoch - 54ms/step\n",
      "Epoch 181/200\n",
      "390/390 - 21s - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.2382 - val_accuracy: 0.9360 - 21s/epoch - 54ms/step\n",
      "Epoch 182/200\n",
      "390/390 - 20s - loss: 0.0164 - accuracy: 0.9953 - val_loss: 0.2565 - val_accuracy: 0.9307 - 20s/epoch - 51ms/step\n",
      "Epoch 183/200\n",
      "390/390 - 21s - loss: 0.0152 - accuracy: 0.9960 - val_loss: 0.2726 - val_accuracy: 0.9268 - 21s/epoch - 53ms/step\n",
      "Epoch 184/200\n",
      "390/390 - 21s - loss: 0.0131 - accuracy: 0.9967 - val_loss: 0.2390 - val_accuracy: 0.9372 - 21s/epoch - 54ms/step\n",
      "Epoch 185/200\n",
      "390/390 - 21s - loss: 0.0121 - accuracy: 0.9971 - val_loss: 0.2327 - val_accuracy: 0.9372 - 21s/epoch - 54ms/step\n",
      "Epoch 186/200\n",
      "390/390 - 21s - loss: 0.0113 - accuracy: 0.9974 - val_loss: 0.2418 - val_accuracy: 0.9336 - 21s/epoch - 54ms/step\n",
      "Epoch 187/200\n",
      "390/390 - 21s - loss: 0.0109 - accuracy: 0.9978 - val_loss: 0.2244 - val_accuracy: 0.9387 - 21s/epoch - 53ms/step\n",
      "Epoch 188/200\n",
      "390/390 - 20s - loss: 0.0099 - accuracy: 0.9981 - val_loss: 0.2072 - val_accuracy: 0.9451 - 20s/epoch - 51ms/step\n",
      "Epoch 189/200\n",
      "390/390 - 20s - loss: 0.0101 - accuracy: 0.9980 - val_loss: 0.2157 - val_accuracy: 0.9402 - 20s/epoch - 51ms/step\n",
      "Epoch 190/200\n",
      "390/390 - 21s - loss: 0.0090 - accuracy: 0.9986 - val_loss: 0.2097 - val_accuracy: 0.9403 - 21s/epoch - 54ms/step\n",
      "Epoch 191/200\n",
      "390/390 - 21s - loss: 0.0091 - accuracy: 0.9987 - val_loss: 0.2054 - val_accuracy: 0.9415 - 21s/epoch - 53ms/step\n",
      "Epoch 192/200\n",
      "390/390 - 20s - loss: 0.0095 - accuracy: 0.9987 - val_loss: 0.2063 - val_accuracy: 0.9411 - 20s/epoch - 51ms/step\n",
      "Epoch 193/200\n",
      "390/390 - 20s - loss: 0.0095 - accuracy: 0.9990 - val_loss: 0.1999 - val_accuracy: 0.9422 - 20s/epoch - 52ms/step\n",
      "Epoch 194/200\n",
      "390/390 - 21s - loss: 0.0106 - accuracy: 0.9991 - val_loss: 0.1968 - val_accuracy: 0.9429 - 21s/epoch - 54ms/step\n",
      "Epoch 195/200\n",
      "390/390 - 21s - loss: 0.0112 - accuracy: 0.9993 - val_loss: 0.1939 - val_accuracy: 0.9431 - 21s/epoch - 54ms/step\n",
      "Epoch 196/200\n",
      "390/390 - 21s - loss: 0.0128 - accuracy: 0.9993 - val_loss: 0.1896 - val_accuracy: 0.9436 - 21s/epoch - 54ms/step\n",
      "Epoch 197/200\n",
      "390/390 - 20s - loss: 0.0149 - accuracy: 0.9991 - val_loss: 0.1880 - val_accuracy: 0.9437 - 20s/epoch - 51ms/step\n",
      "Epoch 198/200\n",
      "390/390 - 21s - loss: 0.0179 - accuracy: 0.9991 - val_loss: 0.1868 - val_accuracy: 0.9433 - 21s/epoch - 54ms/step\n",
      "Epoch 199/200\n",
      "390/390 - 21s - loss: 0.0221 - accuracy: 0.9991 - val_loss: 0.1864 - val_accuracy: 0.9435 - 21s/epoch - 53ms/step\n",
      "Epoch 200/200\n",
      "390/390 - 20s - loss: 0.0277 - accuracy: 0.9992 - val_loss: 0.1885 - val_accuracy: 0.9439 - 20s/epoch - 51ms/step\n",
      " Using best val_acc=0.9451 from last 20 epochs\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2072 - accuracy: 0.9451\n",
      " Finished: Lam=0.8, Repeat=1, Acc=0.9451\n",
      "\n",
      " lambda: Lam=0.8, Repeat=2/3\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "269001286 561714176\n",
      "Epoch 1/200\n",
      "390/390 - 23s - loss: 0.7688 - accuracy: 0.7462 - val_loss: 2.5300 - val_accuracy: 0.3311 - 23s/epoch - 59ms/step\n",
      "Epoch 2/200\n",
      "390/390 - 21s - loss: 0.6342 - accuracy: 0.7839 - val_loss: 1.2857 - val_accuracy: 0.6266 - 21s/epoch - 53ms/step\n",
      "Epoch 3/200\n",
      "390/390 - 21s - loss: 0.5224 - accuracy: 0.8222 - val_loss: 1.5992 - val_accuracy: 0.6022 - 21s/epoch - 54ms/step\n",
      "Epoch 4/200\n",
      "390/390 - 21s - loss: 0.4639 - accuracy: 0.8413 - val_loss: 0.8650 - val_accuracy: 0.7308 - 21s/epoch - 54ms/step\n",
      "Epoch 5/200\n",
      "390/390 - 20s - loss: 0.4255 - accuracy: 0.8559 - val_loss: 1.2814 - val_accuracy: 0.6673 - 20s/epoch - 51ms/step\n",
      "Epoch 6/200\n",
      "390/390 - 20s - loss: 0.3743 - accuracy: 0.8721 - val_loss: 0.6242 - val_accuracy: 0.7974 - 20s/epoch - 51ms/step\n",
      "Epoch 7/200\n",
      "390/390 - 20s - loss: 0.3277 - accuracy: 0.8873 - val_loss: 0.6372 - val_accuracy: 0.7986 - 20s/epoch - 51ms/step\n",
      "Epoch 8/200\n",
      "390/390 - 21s - loss: 0.3001 - accuracy: 0.8965 - val_loss: 0.4772 - val_accuracy: 0.8423 - 21s/epoch - 54ms/step\n",
      "Epoch 9/200\n",
      "390/390 - 21s - loss: 0.2739 - accuracy: 0.9061 - val_loss: 0.4475 - val_accuracy: 0.8566 - 21s/epoch - 53ms/step\n",
      "Epoch 10/200\n",
      "390/390 - 21s - loss: 0.2581 - accuracy: 0.9118 - val_loss: 0.3999 - val_accuracy: 0.8676 - 21s/epoch - 54ms/step\n",
      "Epoch 11/200\n",
      "390/390 - 21s - loss: 0.2427 - accuracy: 0.9168 - val_loss: 0.3855 - val_accuracy: 0.8726 - 21s/epoch - 54ms/step\n",
      "Epoch 12/200\n",
      "390/390 - 20s - loss: 0.2260 - accuracy: 0.9202 - val_loss: 0.3786 - val_accuracy: 0.8784 - 20s/epoch - 52ms/step\n",
      "Epoch 13/200\n",
      "390/390 - 21s - loss: 0.2149 - accuracy: 0.9252 - val_loss: 0.3648 - val_accuracy: 0.8778 - 21s/epoch - 54ms/step\n",
      "Epoch 14/200\n",
      "390/390 - 21s - loss: 0.2071 - accuracy: 0.9280 - val_loss: 0.3742 - val_accuracy: 0.8758 - 21s/epoch - 54ms/step\n",
      "Epoch 15/200\n",
      "390/390 - 21s - loss: 0.1999 - accuracy: 0.9304 - val_loss: 0.3371 - val_accuracy: 0.8896 - 21s/epoch - 54ms/step\n",
      "Epoch 16/200\n",
      "390/390 - 21s - loss: 0.1921 - accuracy: 0.9337 - val_loss: 0.3102 - val_accuracy: 0.8978 - 21s/epoch - 54ms/step\n",
      "Epoch 17/200\n",
      "390/390 - 20s - loss: 0.1880 - accuracy: 0.9333 - val_loss: 0.3540 - val_accuracy: 0.8896 - 20s/epoch - 51ms/step\n",
      "Epoch 18/200\n",
      "390/390 - 21s - loss: 0.1784 - accuracy: 0.9373 - val_loss: 0.4097 - val_accuracy: 0.8745 - 21s/epoch - 53ms/step\n",
      "Epoch 19/200\n",
      "390/390 - 21s - loss: 0.1756 - accuracy: 0.9393 - val_loss: 0.4144 - val_accuracy: 0.8727 - 21s/epoch - 55ms/step\n",
      "Epoch 20/200\n",
      "390/390 - 20s - loss: 0.1691 - accuracy: 0.9400 - val_loss: 0.3909 - val_accuracy: 0.8761 - 20s/epoch - 52ms/step\n",
      "Epoch 21/200\n",
      "390/390 - 21s - loss: 0.1641 - accuracy: 0.9429 - val_loss: 0.3858 - val_accuracy: 0.8791 - 21s/epoch - 54ms/step\n",
      "Epoch 22/200\n",
      "390/390 - 20s - loss: 0.1603 - accuracy: 0.9445 - val_loss: 0.3866 - val_accuracy: 0.8840 - 20s/epoch - 51ms/step\n",
      "Epoch 23/200\n",
      "390/390 - 20s - loss: 0.1556 - accuracy: 0.9461 - val_loss: 0.3028 - val_accuracy: 0.9053 - 20s/epoch - 51ms/step\n",
      "Epoch 24/200\n",
      "390/390 - 21s - loss: 0.1528 - accuracy: 0.9466 - val_loss: 0.3882 - val_accuracy: 0.8813 - 21s/epoch - 53ms/step\n",
      "Epoch 25/200\n",
      "390/390 - 21s - loss: 0.1522 - accuracy: 0.9459 - val_loss: 0.2907 - val_accuracy: 0.9089 - 21s/epoch - 54ms/step\n",
      "Epoch 26/200\n",
      "390/390 - 20s - loss: 0.1486 - accuracy: 0.9477 - val_loss: 0.2994 - val_accuracy: 0.9071 - 20s/epoch - 51ms/step\n",
      "Epoch 27/200\n",
      "390/390 - 21s - loss: 0.1440 - accuracy: 0.9498 - val_loss: 0.3907 - val_accuracy: 0.8818 - 21s/epoch - 54ms/step\n",
      "Epoch 28/200\n",
      "390/390 - 21s - loss: 0.1449 - accuracy: 0.9492 - val_loss: 0.3946 - val_accuracy: 0.8796 - 21s/epoch - 55ms/step\n",
      "Epoch 29/200\n",
      "390/390 - 20s - loss: 0.1412 - accuracy: 0.9513 - val_loss: 0.3839 - val_accuracy: 0.8819 - 20s/epoch - 51ms/step\n",
      "Epoch 30/200\n",
      "390/390 - 20s - loss: 0.1366 - accuracy: 0.9511 - val_loss: 0.3446 - val_accuracy: 0.8909 - 20s/epoch - 52ms/step\n",
      "Epoch 31/200\n",
      "390/390 - 21s - loss: 0.1351 - accuracy: 0.9521 - val_loss: 0.3787 - val_accuracy: 0.8853 - 21s/epoch - 53ms/step\n",
      "Epoch 32/200\n",
      "390/390 - 20s - loss: 0.1383 - accuracy: 0.9524 - val_loss: 0.4225 - val_accuracy: 0.8705 - 20s/epoch - 51ms/step\n",
      "Epoch 33/200\n",
      "390/390 - 20s - loss: 0.1323 - accuracy: 0.9538 - val_loss: 0.4048 - val_accuracy: 0.8819 - 20s/epoch - 52ms/step\n",
      "Epoch 34/200\n",
      "390/390 - 21s - loss: 0.1319 - accuracy: 0.9542 - val_loss: 0.3330 - val_accuracy: 0.8959 - 21s/epoch - 54ms/step\n",
      "Epoch 35/200\n",
      "390/390 - 20s - loss: 0.1302 - accuracy: 0.9539 - val_loss: 0.4183 - val_accuracy: 0.8725 - 20s/epoch - 51ms/step\n",
      "Epoch 36/200\n",
      "390/390 - 21s - loss: 0.1279 - accuracy: 0.9567 - val_loss: 0.4444 - val_accuracy: 0.8676 - 21s/epoch - 53ms/step\n",
      "Epoch 37/200\n",
      "390/390 - 21s - loss: 0.1257 - accuracy: 0.9558 - val_loss: 0.3879 - val_accuracy: 0.8876 - 21s/epoch - 54ms/step\n",
      "Epoch 38/200\n",
      "390/390 - 20s - loss: 0.1225 - accuracy: 0.9574 - val_loss: 0.4673 - val_accuracy: 0.8642 - 20s/epoch - 51ms/step\n",
      "Epoch 39/200\n",
      "390/390 - 21s - loss: 0.1233 - accuracy: 0.9568 - val_loss: 0.3408 - val_accuracy: 0.8984 - 21s/epoch - 54ms/step\n",
      "Epoch 40/200\n",
      "390/390 - 20s - loss: 0.1188 - accuracy: 0.9587 - val_loss: 0.3857 - val_accuracy: 0.8892 - 20s/epoch - 52ms/step\n",
      "Epoch 41/200\n",
      "390/390 - 21s - loss: 0.1211 - accuracy: 0.9572 - val_loss: 0.3526 - val_accuracy: 0.8938 - 21s/epoch - 53ms/step\n",
      "Epoch 42/200\n",
      "390/390 - 20s - loss: 0.1211 - accuracy: 0.9574 - val_loss: 0.5237 - val_accuracy: 0.8619 - 20s/epoch - 51ms/step\n",
      "Epoch 43/200\n",
      "390/390 - 21s - loss: 0.1192 - accuracy: 0.9581 - val_loss: 0.3217 - val_accuracy: 0.8998 - 21s/epoch - 53ms/step\n",
      "Epoch 44/200\n",
      "390/390 - 20s - loss: 0.1175 - accuracy: 0.9597 - val_loss: 0.4788 - val_accuracy: 0.8641 - 20s/epoch - 51ms/step\n",
      "Epoch 45/200\n",
      "390/390 - 21s - loss: 0.1158 - accuracy: 0.9596 - val_loss: 0.4641 - val_accuracy: 0.8733 - 21s/epoch - 54ms/step\n",
      "Epoch 46/200\n",
      "390/390 - 21s - loss: 0.1169 - accuracy: 0.9587 - val_loss: 0.3444 - val_accuracy: 0.9033 - 21s/epoch - 54ms/step\n",
      "Epoch 47/200\n",
      "390/390 - 20s - loss: 0.1137 - accuracy: 0.9600 - val_loss: 0.3437 - val_accuracy: 0.9027 - 20s/epoch - 50ms/step\n",
      "Epoch 48/200\n",
      "390/390 - 21s - loss: 0.1123 - accuracy: 0.9602 - val_loss: 0.5981 - val_accuracy: 0.8375 - 21s/epoch - 53ms/step\n",
      "Epoch 49/200\n",
      "390/390 - 21s - loss: 0.1117 - accuracy: 0.9613 - val_loss: 0.3756 - val_accuracy: 0.8926 - 21s/epoch - 54ms/step\n",
      "Epoch 50/200\n",
      "390/390 - 20s - loss: 0.1146 - accuracy: 0.9601 - val_loss: 0.3854 - val_accuracy: 0.8898 - 20s/epoch - 51ms/step\n",
      "Epoch 51/200\n",
      "390/390 - 22s - loss: 0.1100 - accuracy: 0.9617 - val_loss: 0.3623 - val_accuracy: 0.8950 - 22s/epoch - 56ms/step\n",
      "Epoch 52/200\n",
      "390/390 - 21s - loss: 0.1104 - accuracy: 0.9615 - val_loss: 0.3320 - val_accuracy: 0.9031 - 21s/epoch - 54ms/step\n",
      "Epoch 53/200\n",
      "390/390 - 20s - loss: 0.1071 - accuracy: 0.9608 - val_loss: 0.3873 - val_accuracy: 0.8879 - 20s/epoch - 52ms/step\n",
      "Epoch 54/200\n",
      "390/390 - 20s - loss: 0.1119 - accuracy: 0.9614 - val_loss: 0.3567 - val_accuracy: 0.8937 - 20s/epoch - 51ms/step\n",
      "Epoch 55/200\n",
      "390/390 - 21s - loss: 0.1070 - accuracy: 0.9625 - val_loss: 0.3928 - val_accuracy: 0.8853 - 21s/epoch - 53ms/step\n",
      "Epoch 56/200\n",
      "390/390 - 21s - loss: 0.1075 - accuracy: 0.9623 - val_loss: 0.5791 - val_accuracy: 0.8480 - 21s/epoch - 53ms/step\n",
      "Epoch 57/200\n",
      "390/390 - 21s - loss: 0.1081 - accuracy: 0.9621 - val_loss: 0.4500 - val_accuracy: 0.8735 - 21s/epoch - 54ms/step\n",
      "Epoch 58/200\n",
      "390/390 - 21s - loss: 0.1069 - accuracy: 0.9628 - val_loss: 0.3477 - val_accuracy: 0.8997 - 21s/epoch - 54ms/step\n",
      "Epoch 59/200\n",
      "390/390 - 20s - loss: 0.1023 - accuracy: 0.9640 - val_loss: 0.4461 - val_accuracy: 0.8812 - 20s/epoch - 51ms/step\n",
      "Epoch 60/200\n",
      "390/390 - 20s - loss: 0.1052 - accuracy: 0.9626 - val_loss: 0.5060 - val_accuracy: 0.8668 - 20s/epoch - 52ms/step\n",
      "Epoch 61/200\n",
      "390/390 - 20s - loss: 0.1069 - accuracy: 0.9619 - val_loss: 0.4418 - val_accuracy: 0.8743 - 20s/epoch - 51ms/step\n",
      "Epoch 62/200\n",
      "390/390 - 20s - loss: 0.1023 - accuracy: 0.9658 - val_loss: 0.3816 - val_accuracy: 0.8978 - 20s/epoch - 51ms/step\n",
      "Epoch 63/200\n",
      "390/390 - 20s - loss: 0.1041 - accuracy: 0.9640 - val_loss: 0.3616 - val_accuracy: 0.8965 - 20s/epoch - 51ms/step\n",
      "Epoch 64/200\n",
      "390/390 - 21s - loss: 0.1012 - accuracy: 0.9639 - val_loss: 0.4191 - val_accuracy: 0.8904 - 21s/epoch - 54ms/step\n",
      "Epoch 65/200\n",
      "390/390 - 21s - loss: 0.1047 - accuracy: 0.9634 - val_loss: 0.3618 - val_accuracy: 0.9010 - 21s/epoch - 54ms/step\n",
      "Epoch 66/200\n",
      "390/390 - 21s - loss: 0.1034 - accuracy: 0.9652 - val_loss: 0.4475 - val_accuracy: 0.8770 - 21s/epoch - 53ms/step\n",
      "Epoch 67/200\n",
      "390/390 - 22s - loss: 0.0972 - accuracy: 0.9657 - val_loss: 0.3745 - val_accuracy: 0.8982 - 22s/epoch - 55ms/step\n",
      "Epoch 68/200\n",
      "390/390 - 20s - loss: 0.1000 - accuracy: 0.9661 - val_loss: 0.3428 - val_accuracy: 0.8967 - 20s/epoch - 51ms/step\n",
      "Epoch 69/200\n",
      "390/390 - 21s - loss: 0.1025 - accuracy: 0.9640 - val_loss: 0.4290 - val_accuracy: 0.8744 - 21s/epoch - 54ms/step\n",
      "Epoch 70/200\n",
      "390/390 - 20s - loss: 0.0986 - accuracy: 0.9661 - val_loss: 0.4190 - val_accuracy: 0.8855 - 20s/epoch - 51ms/step\n",
      "Epoch 71/200\n",
      "390/390 - 20s - loss: 0.0955 - accuracy: 0.9666 - val_loss: 0.4010 - val_accuracy: 0.8881 - 20s/epoch - 51ms/step\n",
      "Epoch 72/200\n",
      "390/390 - 21s - loss: 0.1017 - accuracy: 0.9650 - val_loss: 0.3960 - val_accuracy: 0.8874 - 21s/epoch - 53ms/step\n",
      "Epoch 73/200\n",
      "390/390 - 21s - loss: 0.0980 - accuracy: 0.9665 - val_loss: 0.3643 - val_accuracy: 0.9009 - 21s/epoch - 53ms/step\n",
      "Epoch 74/200\n",
      "390/390 - 20s - loss: 0.0984 - accuracy: 0.9655 - val_loss: 0.4576 - val_accuracy: 0.8734 - 20s/epoch - 51ms/step\n",
      "Epoch 75/200\n",
      "390/390 - 20s - loss: 0.0972 - accuracy: 0.9668 - val_loss: 0.3373 - val_accuracy: 0.9029 - 20s/epoch - 51ms/step\n",
      "Epoch 76/200\n",
      "390/390 - 21s - loss: 0.0969 - accuracy: 0.9652 - val_loss: 0.3380 - val_accuracy: 0.8986 - 21s/epoch - 55ms/step\n",
      "Epoch 77/200\n",
      "390/390 - 21s - loss: 0.0970 - accuracy: 0.9658 - val_loss: 0.3790 - val_accuracy: 0.8923 - 21s/epoch - 53ms/step\n",
      "Epoch 78/200\n",
      "390/390 - 20s - loss: 0.0922 - accuracy: 0.9686 - val_loss: 0.4425 - val_accuracy: 0.8825 - 20s/epoch - 52ms/step\n",
      "Epoch 79/200\n",
      "390/390 - 21s - loss: 0.0932 - accuracy: 0.9676 - val_loss: 0.5977 - val_accuracy: 0.8508 - 21s/epoch - 53ms/step\n",
      "Epoch 80/200\n",
      "390/390 - 20s - loss: 0.0957 - accuracy: 0.9663 - val_loss: 0.3878 - val_accuracy: 0.8931 - 20s/epoch - 51ms/step\n",
      "Epoch 81/200\n",
      "390/390 - 21s - loss: 0.0920 - accuracy: 0.9673 - val_loss: 0.3867 - val_accuracy: 0.8941 - 21s/epoch - 54ms/step\n",
      "Epoch 82/200\n",
      "390/390 - 21s - loss: 0.0919 - accuracy: 0.9681 - val_loss: 0.4134 - val_accuracy: 0.8787 - 21s/epoch - 54ms/step\n",
      "Epoch 83/200\n",
      "390/390 - 20s - loss: 0.0922 - accuracy: 0.9677 - val_loss: 0.4198 - val_accuracy: 0.8844 - 20s/epoch - 51ms/step\n",
      "Epoch 84/200\n",
      "390/390 - 20s - loss: 0.0906 - accuracy: 0.9680 - val_loss: 0.4306 - val_accuracy: 0.8860 - 20s/epoch - 52ms/step\n",
      "Epoch 85/200\n",
      "390/390 - 20s - loss: 0.0907 - accuracy: 0.9681 - val_loss: 0.4539 - val_accuracy: 0.8777 - 20s/epoch - 52ms/step\n",
      "Epoch 86/200\n",
      "390/390 - 20s - loss: 0.0879 - accuracy: 0.9703 - val_loss: 0.5371 - val_accuracy: 0.8626 - 20s/epoch - 51ms/step\n",
      "Epoch 87/200\n",
      "390/390 - 21s - loss: 0.0921 - accuracy: 0.9679 - val_loss: 0.3850 - val_accuracy: 0.8922 - 21s/epoch - 55ms/step\n",
      "Epoch 88/200\n",
      "390/390 - 20s - loss: 0.0870 - accuracy: 0.9698 - val_loss: 0.3330 - val_accuracy: 0.9077 - 20s/epoch - 51ms/step\n",
      "Epoch 89/200\n",
      "390/390 - 21s - loss: 0.0936 - accuracy: 0.9669 - val_loss: 0.4321 - val_accuracy: 0.8837 - 21s/epoch - 53ms/step\n",
      "Epoch 90/200\n",
      "390/390 - 20s - loss: 0.0901 - accuracy: 0.9691 - val_loss: 0.3933 - val_accuracy: 0.8926 - 20s/epoch - 51ms/step\n",
      "Epoch 91/200\n",
      "390/390 - 21s - loss: 0.0882 - accuracy: 0.9698 - val_loss: 0.4017 - val_accuracy: 0.8878 - 21s/epoch - 53ms/step\n",
      "Epoch 92/200\n",
      "390/390 - 20s - loss: 0.0892 - accuracy: 0.9692 - val_loss: 0.4886 - val_accuracy: 0.8690 - 20s/epoch - 52ms/step\n",
      "Epoch 93/200\n",
      "390/390 - 21s - loss: 0.0873 - accuracy: 0.9690 - val_loss: 0.3947 - val_accuracy: 0.8931 - 21s/epoch - 54ms/step\n",
      "Epoch 94/200\n",
      "390/390 - 21s - loss: 0.0885 - accuracy: 0.9695 - val_loss: 0.3278 - val_accuracy: 0.9091 - 21s/epoch - 55ms/step\n",
      "Epoch 95/200\n",
      "390/390 - 20s - loss: 0.0876 - accuracy: 0.9696 - val_loss: 0.4359 - val_accuracy: 0.8791 - 20s/epoch - 51ms/step\n",
      "Epoch 96/200\n",
      "390/390 - 20s - loss: 0.0897 - accuracy: 0.9685 - val_loss: 0.3977 - val_accuracy: 0.8929 - 20s/epoch - 51ms/step\n",
      "Epoch 97/200\n",
      "390/390 - 20s - loss: 0.0821 - accuracy: 0.9713 - val_loss: 0.3953 - val_accuracy: 0.8915 - 20s/epoch - 51ms/step\n",
      "Epoch 98/200\n",
      "390/390 - 20s - loss: 0.0854 - accuracy: 0.9706 - val_loss: 0.3959 - val_accuracy: 0.8833 - 20s/epoch - 51ms/step\n",
      "Epoch 99/200\n",
      "390/390 - 21s - loss: 0.0840 - accuracy: 0.9703 - val_loss: 0.3707 - val_accuracy: 0.9003 - 21s/epoch - 54ms/step\n",
      "Epoch 100/200\n",
      "390/390 - 20s - loss: 0.0851 - accuracy: 0.9706 - val_loss: 0.3141 - val_accuracy: 0.9092 - 20s/epoch - 51ms/step\n",
      "Epoch 101/200\n",
      "390/390 - 21s - loss: 0.0835 - accuracy: 0.9709 - val_loss: 0.3409 - val_accuracy: 0.9044 - 21s/epoch - 53ms/step\n",
      "Epoch 102/200\n",
      "390/390 - 21s - loss: 0.0851 - accuracy: 0.9694 - val_loss: 0.3922 - val_accuracy: 0.8932 - 21s/epoch - 54ms/step\n",
      "Epoch 103/200\n",
      "390/390 - 21s - loss: 0.0843 - accuracy: 0.9705 - val_loss: 0.3941 - val_accuracy: 0.8911 - 21s/epoch - 53ms/step\n",
      "Epoch 104/200\n",
      "390/390 - 22s - loss: 0.0874 - accuracy: 0.9704 - val_loss: 0.3475 - val_accuracy: 0.9012 - 22s/epoch - 55ms/step\n",
      "Epoch 105/200\n",
      "390/390 - 21s - loss: 0.0760 - accuracy: 0.9741 - val_loss: 0.3837 - val_accuracy: 0.8949 - 21s/epoch - 53ms/step\n",
      "Epoch 106/200\n",
      "390/390 - 20s - loss: 0.0830 - accuracy: 0.9716 - val_loss: 0.3541 - val_accuracy: 0.9009 - 20s/epoch - 51ms/step\n",
      "Epoch 107/200\n",
      "390/390 - 20s - loss: 0.0802 - accuracy: 0.9722 - val_loss: 0.3320 - val_accuracy: 0.9045 - 20s/epoch - 51ms/step\n",
      "Epoch 108/200\n",
      "390/390 - 21s - loss: 0.0771 - accuracy: 0.9730 - val_loss: 0.3279 - val_accuracy: 0.9091 - 21s/epoch - 54ms/step\n",
      "Epoch 109/200\n",
      "390/390 - 21s - loss: 0.0815 - accuracy: 0.9716 - val_loss: 0.3505 - val_accuracy: 0.9070 - 21s/epoch - 54ms/step\n",
      "Epoch 110/200\n",
      "390/390 - 21s - loss: 0.0814 - accuracy: 0.9723 - val_loss: 0.3357 - val_accuracy: 0.9012 - 21s/epoch - 53ms/step\n",
      "Epoch 111/200\n",
      "390/390 - 21s - loss: 0.0751 - accuracy: 0.9739 - val_loss: 0.3233 - val_accuracy: 0.9121 - 21s/epoch - 54ms/step\n",
      "Epoch 112/200\n",
      "390/390 - 21s - loss: 0.0789 - accuracy: 0.9721 - val_loss: 0.3007 - val_accuracy: 0.9172 - 21s/epoch - 53ms/step\n",
      "Epoch 113/200\n",
      "390/390 - 20s - loss: 0.0762 - accuracy: 0.9737 - val_loss: 0.3294 - val_accuracy: 0.9102 - 20s/epoch - 51ms/step\n",
      "Epoch 114/200\n",
      "390/390 - 20s - loss: 0.0767 - accuracy: 0.9733 - val_loss: 0.4400 - val_accuracy: 0.8771 - 20s/epoch - 51ms/step\n",
      "Epoch 115/200\n",
      "390/390 - 21s - loss: 0.0798 - accuracy: 0.9726 - val_loss: 0.4480 - val_accuracy: 0.8850 - 21s/epoch - 54ms/step\n",
      "Epoch 116/200\n",
      "390/390 - 21s - loss: 0.0772 - accuracy: 0.9730 - val_loss: 0.4030 - val_accuracy: 0.8908 - 21s/epoch - 53ms/step\n",
      "Epoch 117/200\n",
      "390/390 - 20s - loss: 0.0772 - accuracy: 0.9730 - val_loss: 0.2993 - val_accuracy: 0.9145 - 20s/epoch - 51ms/step\n",
      "Epoch 118/200\n",
      "390/390 - 20s - loss: 0.0732 - accuracy: 0.9746 - val_loss: 0.3935 - val_accuracy: 0.8950 - 20s/epoch - 52ms/step\n",
      "Epoch 119/200\n",
      "390/390 - 20s - loss: 0.0752 - accuracy: 0.9735 - val_loss: 0.3654 - val_accuracy: 0.9049 - 20s/epoch - 51ms/step\n",
      "Epoch 120/200\n",
      "390/390 - 20s - loss: 0.0746 - accuracy: 0.9738 - val_loss: 0.4108 - val_accuracy: 0.8895 - 20s/epoch - 51ms/step\n",
      "Epoch 121/200\n",
      "390/390 - 20s - loss: 0.0755 - accuracy: 0.9737 - val_loss: 0.3656 - val_accuracy: 0.8989 - 20s/epoch - 51ms/step\n",
      "Epoch 122/200\n",
      "390/390 - 21s - loss: 0.0715 - accuracy: 0.9752 - val_loss: 0.3311 - val_accuracy: 0.9136 - 21s/epoch - 53ms/step\n",
      "Epoch 123/200\n",
      "390/390 - 20s - loss: 0.0710 - accuracy: 0.9751 - val_loss: 0.3585 - val_accuracy: 0.9009 - 20s/epoch - 51ms/step\n",
      "Epoch 124/200\n",
      "390/390 - 21s - loss: 0.0768 - accuracy: 0.9739 - val_loss: 0.4125 - val_accuracy: 0.8923 - 21s/epoch - 53ms/step\n",
      "Epoch 125/200\n",
      "390/390 - 21s - loss: 0.0737 - accuracy: 0.9740 - val_loss: 0.4516 - val_accuracy: 0.8828 - 21s/epoch - 54ms/step\n",
      "Epoch 126/200\n",
      "390/390 - 21s - loss: 0.0748 - accuracy: 0.9736 - val_loss: 0.3763 - val_accuracy: 0.8959 - 21s/epoch - 55ms/step\n",
      "Epoch 127/200\n",
      "390/390 - 22s - loss: 0.0718 - accuracy: 0.9752 - val_loss: 0.4560 - val_accuracy: 0.8833 - 22s/epoch - 55ms/step\n",
      "Epoch 128/200\n",
      "390/390 - 20s - loss: 0.0672 - accuracy: 0.9765 - val_loss: 0.3686 - val_accuracy: 0.9049 - 20s/epoch - 51ms/step\n",
      "Epoch 129/200\n",
      "390/390 - 20s - loss: 0.0714 - accuracy: 0.9752 - val_loss: 0.3739 - val_accuracy: 0.9006 - 20s/epoch - 51ms/step\n",
      "Epoch 130/200\n",
      "390/390 - 22s - loss: 0.0743 - accuracy: 0.9745 - val_loss: 0.3671 - val_accuracy: 0.9024 - 22s/epoch - 55ms/step\n",
      "Epoch 131/200\n",
      "390/390 - 20s - loss: 0.0720 - accuracy: 0.9745 - val_loss: 0.4304 - val_accuracy: 0.8880 - 20s/epoch - 51ms/step\n",
      "Epoch 132/200\n",
      "390/390 - 21s - loss: 0.0687 - accuracy: 0.9759 - val_loss: 0.4397 - val_accuracy: 0.8904 - 21s/epoch - 54ms/step\n",
      "Epoch 133/200\n",
      "390/390 - 21s - loss: 0.0672 - accuracy: 0.9770 - val_loss: 0.3815 - val_accuracy: 0.8960 - 21s/epoch - 53ms/step\n",
      "Epoch 134/200\n",
      "390/390 - 21s - loss: 0.0683 - accuracy: 0.9765 - val_loss: 0.3948 - val_accuracy: 0.9009 - 21s/epoch - 54ms/step\n",
      "Epoch 135/200\n",
      "390/390 - 20s - loss: 0.0693 - accuracy: 0.9764 - val_loss: 0.2696 - val_accuracy: 0.9225 - 20s/epoch - 51ms/step\n",
      "Epoch 136/200\n",
      "390/390 - 20s - loss: 0.0613 - accuracy: 0.9786 - val_loss: 0.3881 - val_accuracy: 0.8946 - 20s/epoch - 51ms/step\n",
      "Epoch 137/200\n",
      "390/390 - 21s - loss: 0.0612 - accuracy: 0.9790 - val_loss: 0.3159 - val_accuracy: 0.9119 - 21s/epoch - 53ms/step\n",
      "Epoch 138/200\n",
      "390/390 - 20s - loss: 0.0663 - accuracy: 0.9776 - val_loss: 0.3094 - val_accuracy: 0.9165 - 20s/epoch - 51ms/step\n",
      "Epoch 139/200\n",
      "390/390 - 20s - loss: 0.0665 - accuracy: 0.9774 - val_loss: 0.3251 - val_accuracy: 0.9081 - 20s/epoch - 51ms/step\n",
      "Epoch 140/200\n",
      "390/390 - 21s - loss: 0.0613 - accuracy: 0.9788 - val_loss: 0.5005 - val_accuracy: 0.8736 - 21s/epoch - 54ms/step\n",
      "Epoch 141/200\n",
      "390/390 - 20s - loss: 0.0618 - accuracy: 0.9786 - val_loss: 0.3725 - val_accuracy: 0.8947 - 20s/epoch - 51ms/step\n",
      "Epoch 142/200\n",
      "390/390 - 21s - loss: 0.0623 - accuracy: 0.9782 - val_loss: 0.3141 - val_accuracy: 0.9123 - 21s/epoch - 53ms/step\n",
      "Epoch 143/200\n",
      "390/390 - 21s - loss: 0.0631 - accuracy: 0.9782 - val_loss: 0.3299 - val_accuracy: 0.9157 - 21s/epoch - 54ms/step\n",
      "Epoch 144/200\n",
      "390/390 - 21s - loss: 0.0590 - accuracy: 0.9795 - val_loss: 0.3835 - val_accuracy: 0.9005 - 21s/epoch - 54ms/step\n",
      "Epoch 145/200\n",
      "390/390 - 22s - loss: 0.0594 - accuracy: 0.9793 - val_loss: 0.4160 - val_accuracy: 0.8899 - 22s/epoch - 56ms/step\n",
      "Epoch 146/200\n",
      "390/390 - 21s - loss: 0.0666 - accuracy: 0.9777 - val_loss: 0.3354 - val_accuracy: 0.9111 - 21s/epoch - 55ms/step\n",
      "Epoch 147/200\n",
      "390/390 - 22s - loss: 0.0597 - accuracy: 0.9794 - val_loss: 0.3637 - val_accuracy: 0.9018 - 22s/epoch - 55ms/step\n",
      "Epoch 148/200\n",
      "390/390 - 21s - loss: 0.0604 - accuracy: 0.9795 - val_loss: 0.3561 - val_accuracy: 0.9023 - 21s/epoch - 54ms/step\n",
      "Epoch 149/200\n",
      "390/390 - 21s - loss: 0.0578 - accuracy: 0.9800 - val_loss: 0.4011 - val_accuracy: 0.8938 - 21s/epoch - 53ms/step\n",
      "Epoch 150/200\n",
      "390/390 - 21s - loss: 0.0587 - accuracy: 0.9798 - val_loss: 0.4359 - val_accuracy: 0.8868 - 21s/epoch - 54ms/step\n",
      "Epoch 151/200\n",
      "390/390 - 21s - loss: 0.0572 - accuracy: 0.9816 - val_loss: 0.2916 - val_accuracy: 0.9190 - 21s/epoch - 54ms/step\n",
      "Epoch 152/200\n",
      "390/390 - 20s - loss: 0.0552 - accuracy: 0.9812 - val_loss: 0.3438 - val_accuracy: 0.9095 - 20s/epoch - 51ms/step\n",
      "Epoch 153/200\n",
      "390/390 - 21s - loss: 0.0482 - accuracy: 0.9843 - val_loss: 0.3086 - val_accuracy: 0.9162 - 21s/epoch - 54ms/step\n",
      "Epoch 154/200\n",
      "390/390 - 20s - loss: 0.0512 - accuracy: 0.9832 - val_loss: 0.3205 - val_accuracy: 0.9160 - 20s/epoch - 52ms/step\n",
      "Epoch 155/200\n",
      "390/390 - 20s - loss: 0.0493 - accuracy: 0.9831 - val_loss: 0.3143 - val_accuracy: 0.9142 - 20s/epoch - 51ms/step\n",
      "Epoch 156/200\n",
      "390/390 - 20s - loss: 0.0552 - accuracy: 0.9816 - val_loss: 0.3562 - val_accuracy: 0.9012 - 20s/epoch - 51ms/step\n",
      "Epoch 157/200\n",
      "390/390 - 21s - loss: 0.0507 - accuracy: 0.9832 - val_loss: 0.2861 - val_accuracy: 0.9203 - 21s/epoch - 55ms/step\n",
      "Epoch 158/200\n",
      "390/390 - 21s - loss: 0.0502 - accuracy: 0.9838 - val_loss: 0.2783 - val_accuracy: 0.9222 - 21s/epoch - 54ms/step\n",
      "Epoch 159/200\n",
      "390/390 - 21s - loss: 0.0483 - accuracy: 0.9837 - val_loss: 0.3656 - val_accuracy: 0.8995 - 21s/epoch - 55ms/step\n",
      "Epoch 160/200\n",
      "390/390 - 21s - loss: 0.0465 - accuracy: 0.9840 - val_loss: 0.3314 - val_accuracy: 0.9159 - 21s/epoch - 54ms/step\n",
      "Epoch 161/200\n",
      "390/390 - 21s - loss: 0.0453 - accuracy: 0.9842 - val_loss: 0.3551 - val_accuracy: 0.9088 - 21s/epoch - 53ms/step\n",
      "Epoch 162/200\n",
      "390/390 - 21s - loss: 0.0467 - accuracy: 0.9838 - val_loss: 0.3894 - val_accuracy: 0.8966 - 21s/epoch - 54ms/step\n",
      "Epoch 163/200\n",
      "390/390 - 22s - loss: 0.0434 - accuracy: 0.9860 - val_loss: 0.2714 - val_accuracy: 0.9246 - 22s/epoch - 55ms/step\n",
      "Epoch 164/200\n",
      "390/390 - 20s - loss: 0.0433 - accuracy: 0.9856 - val_loss: 0.3084 - val_accuracy: 0.9179 - 20s/epoch - 51ms/step\n",
      "Epoch 165/200\n",
      "390/390 - 21s - loss: 0.0402 - accuracy: 0.9866 - val_loss: 0.3300 - val_accuracy: 0.9101 - 21s/epoch - 53ms/step\n",
      "Epoch 166/200\n",
      "390/390 - 21s - loss: 0.0405 - accuracy: 0.9865 - val_loss: 0.3074 - val_accuracy: 0.9164 - 21s/epoch - 54ms/step\n",
      "Epoch 167/200\n",
      "390/390 - 20s - loss: 0.0411 - accuracy: 0.9862 - val_loss: 0.2801 - val_accuracy: 0.9237 - 20s/epoch - 51ms/step\n",
      "Epoch 168/200\n",
      "390/390 - 21s - loss: 0.0406 - accuracy: 0.9862 - val_loss: 0.2891 - val_accuracy: 0.9231 - 21s/epoch - 53ms/step\n",
      "Epoch 169/200\n",
      "390/390 - 21s - loss: 0.0389 - accuracy: 0.9871 - val_loss: 0.3437 - val_accuracy: 0.9084 - 21s/epoch - 53ms/step\n",
      "Epoch 170/200\n",
      "390/390 - 21s - loss: 0.0341 - accuracy: 0.9892 - val_loss: 0.2689 - val_accuracy: 0.9262 - 21s/epoch - 54ms/step\n",
      "Epoch 171/200\n",
      "390/390 - 20s - loss: 0.0341 - accuracy: 0.9890 - val_loss: 0.2976 - val_accuracy: 0.9188 - 20s/epoch - 51ms/step\n",
      "Epoch 172/200\n",
      "390/390 - 21s - loss: 0.0343 - accuracy: 0.9889 - val_loss: 0.2624 - val_accuracy: 0.9267 - 21s/epoch - 53ms/step\n",
      "Epoch 173/200\n",
      "390/390 - 21s - loss: 0.0299 - accuracy: 0.9904 - val_loss: 0.2914 - val_accuracy: 0.9202 - 21s/epoch - 54ms/step\n",
      "Epoch 174/200\n",
      "390/390 - 21s - loss: 0.0300 - accuracy: 0.9902 - val_loss: 0.3003 - val_accuracy: 0.9201 - 21s/epoch - 54ms/step\n",
      "Epoch 175/200\n",
      "390/390 - 21s - loss: 0.0293 - accuracy: 0.9906 - val_loss: 0.2787 - val_accuracy: 0.9221 - 21s/epoch - 53ms/step\n",
      "Epoch 176/200\n",
      "390/390 - 20s - loss: 0.0238 - accuracy: 0.9928 - val_loss: 0.2476 - val_accuracy: 0.9342 - 20s/epoch - 52ms/step\n",
      "Epoch 177/200\n",
      "390/390 - 20s - loss: 0.0239 - accuracy: 0.9925 - val_loss: 0.2791 - val_accuracy: 0.9244 - 20s/epoch - 51ms/step\n",
      "Epoch 178/200\n",
      "390/390 - 21s - loss: 0.0227 - accuracy: 0.9930 - val_loss: 0.2534 - val_accuracy: 0.9340 - 21s/epoch - 53ms/step\n",
      "Epoch 179/200\n",
      "390/390 - 20s - loss: 0.0187 - accuracy: 0.9947 - val_loss: 0.2751 - val_accuracy: 0.9272 - 20s/epoch - 51ms/step\n",
      "Epoch 180/200\n",
      "390/390 - 21s - loss: 0.0196 - accuracy: 0.9941 - val_loss: 0.2465 - val_accuracy: 0.9330 - 21s/epoch - 53ms/step\n",
      "Epoch 181/200\n",
      "390/390 - 21s - loss: 0.0178 - accuracy: 0.9951 - val_loss: 0.2386 - val_accuracy: 0.9349 - 21s/epoch - 54ms/step\n",
      "Epoch 182/200\n",
      "390/390 - 20s - loss: 0.0168 - accuracy: 0.9953 - val_loss: 0.2334 - val_accuracy: 0.9352 - 20s/epoch - 51ms/step\n",
      "Epoch 183/200\n",
      "390/390 - 20s - loss: 0.0158 - accuracy: 0.9958 - val_loss: 0.2291 - val_accuracy: 0.9395 - 20s/epoch - 51ms/step\n",
      "Epoch 184/200\n",
      "390/390 - 20s - loss: 0.0123 - accuracy: 0.9972 - val_loss: 0.2261 - val_accuracy: 0.9379 - 20s/epoch - 52ms/step\n",
      "Epoch 185/200\n",
      "390/390 - 20s - loss: 0.0132 - accuracy: 0.9966 - val_loss: 0.2255 - val_accuracy: 0.9396 - 20s/epoch - 52ms/step\n",
      "Epoch 186/200\n",
      "390/390 - 21s - loss: 0.0112 - accuracy: 0.9977 - val_loss: 0.2304 - val_accuracy: 0.9366 - 21s/epoch - 53ms/step\n",
      "Epoch 187/200\n",
      "390/390 - 21s - loss: 0.0112 - accuracy: 0.9975 - val_loss: 0.2079 - val_accuracy: 0.9414 - 21s/epoch - 53ms/step\n",
      "Epoch 188/200\n",
      "390/390 - 20s - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.2093 - val_accuracy: 0.9413 - 20s/epoch - 51ms/step\n",
      "Epoch 189/200\n",
      "390/390 - 21s - loss: 0.0093 - accuracy: 0.9983 - val_loss: 0.2002 - val_accuracy: 0.9448 - 21s/epoch - 53ms/step\n",
      "Epoch 190/200\n",
      "390/390 - 20s - loss: 0.0091 - accuracy: 0.9986 - val_loss: 0.2039 - val_accuracy: 0.9422 - 20s/epoch - 51ms/step\n",
      "Epoch 191/200\n",
      "390/390 - 20s - loss: 0.0086 - accuracy: 0.9990 - val_loss: 0.1955 - val_accuracy: 0.9446 - 20s/epoch - 51ms/step\n",
      "Epoch 192/200\n",
      "390/390 - 21s - loss: 0.0086 - accuracy: 0.9992 - val_loss: 0.1948 - val_accuracy: 0.9438 - 21s/epoch - 55ms/step\n",
      "Epoch 193/200\n",
      "390/390 - 20s - loss: 0.0098 - accuracy: 0.9989 - val_loss: 0.1877 - val_accuracy: 0.9455 - 20s/epoch - 51ms/step\n",
      "Epoch 194/200\n",
      "390/390 - 21s - loss: 0.0107 - accuracy: 0.9988 - val_loss: 0.1846 - val_accuracy: 0.9455 - 21s/epoch - 54ms/step\n",
      "Epoch 195/200\n",
      "390/390 - 21s - loss: 0.0109 - accuracy: 0.9993 - val_loss: 0.1848 - val_accuracy: 0.9454 - 21s/epoch - 54ms/step\n",
      "Epoch 196/200\n",
      "390/390 - 21s - loss: 0.0129 - accuracy: 0.9992 - val_loss: 0.1805 - val_accuracy: 0.9463 - 21s/epoch - 54ms/step\n",
      "Epoch 197/200\n",
      "390/390 - 21s - loss: 0.0149 - accuracy: 0.9993 - val_loss: 0.1790 - val_accuracy: 0.9457 - 21s/epoch - 54ms/step\n",
      "Epoch 198/200\n",
      "390/390 - 20s - loss: 0.0181 - accuracy: 0.9993 - val_loss: 0.1786 - val_accuracy: 0.9461 - 20s/epoch - 51ms/step\n",
      "Epoch 199/200\n",
      "390/390 - 21s - loss: 0.0225 - accuracy: 0.9991 - val_loss: 0.1787 - val_accuracy: 0.9464 - 21s/epoch - 54ms/step\n",
      "Epoch 200/200\n",
      "390/390 - 20s - loss: 0.0282 - accuracy: 0.9992 - val_loss: 0.1813 - val_accuracy: 0.9466 - 20s/epoch - 51ms/step\n",
      " Using best val_acc=0.9466 from last 20 epochs\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.1813 - accuracy: 0.9466\n",
      " Finished: Lam=0.8, Repeat=2, Acc=0.9466\n",
      "\n",
      " lambda: Lam=0.8, Repeat=3/3\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "269001286 561714176\n",
      "Epoch 1/200\n",
      "390/390 - 22s - loss: 0.7554 - accuracy: 0.7511 - val_loss: 3.1967 - val_accuracy: 0.4035 - 22s/epoch - 56ms/step\n",
      "Epoch 2/200\n",
      "390/390 - 21s - loss: 0.6205 - accuracy: 0.7900 - val_loss: 1.4040 - val_accuracy: 0.6229 - 21s/epoch - 54ms/step\n",
      "Epoch 3/200\n",
      "390/390 - 21s - loss: 0.5274 - accuracy: 0.8216 - val_loss: 1.3740 - val_accuracy: 0.6278 - 21s/epoch - 53ms/step\n",
      "Epoch 4/200\n",
      "390/390 - 21s - loss: 0.4641 - accuracy: 0.8409 - val_loss: 1.0029 - val_accuracy: 0.7261 - 21s/epoch - 53ms/step\n",
      "Epoch 5/200\n",
      "390/390 - 21s - loss: 0.4224 - accuracy: 0.8564 - val_loss: 0.7312 - val_accuracy: 0.7733 - 21s/epoch - 53ms/step\n",
      "Epoch 6/200\n",
      "390/390 - 21s - loss: 0.3695 - accuracy: 0.8729 - val_loss: 0.4946 - val_accuracy: 0.8469 - 21s/epoch - 54ms/step\n",
      "Epoch 7/200\n",
      "390/390 - 21s - loss: 0.3275 - accuracy: 0.8872 - val_loss: 0.6534 - val_accuracy: 0.7967 - 21s/epoch - 54ms/step\n",
      "Epoch 8/200\n",
      "390/390 - 20s - loss: 0.2974 - accuracy: 0.8996 - val_loss: 0.5431 - val_accuracy: 0.8291 - 20s/epoch - 51ms/step\n",
      "Epoch 9/200\n",
      "390/390 - 21s - loss: 0.2732 - accuracy: 0.9056 - val_loss: 0.6191 - val_accuracy: 0.7968 - 21s/epoch - 53ms/step\n",
      "Epoch 10/200\n",
      "390/390 - 21s - loss: 0.2582 - accuracy: 0.9114 - val_loss: 0.4373 - val_accuracy: 0.8605 - 21s/epoch - 53ms/step\n",
      "Epoch 11/200\n",
      "390/390 - 21s - loss: 0.2421 - accuracy: 0.9165 - val_loss: 0.4463 - val_accuracy: 0.8538 - 21s/epoch - 53ms/step\n",
      "Epoch 12/200\n",
      "390/390 - 21s - loss: 0.2257 - accuracy: 0.9220 - val_loss: 0.4125 - val_accuracy: 0.8678 - 21s/epoch - 53ms/step\n",
      "Epoch 13/200\n",
      "390/390 - 21s - loss: 0.2106 - accuracy: 0.9275 - val_loss: 0.5211 - val_accuracy: 0.8460 - 21s/epoch - 54ms/step\n",
      "Epoch 14/200\n",
      "390/390 - 20s - loss: 0.2063 - accuracy: 0.9278 - val_loss: 0.4427 - val_accuracy: 0.8604 - 20s/epoch - 51ms/step\n",
      "Epoch 15/200\n",
      "390/390 - 20s - loss: 0.1995 - accuracy: 0.9304 - val_loss: 0.4266 - val_accuracy: 0.8643 - 20s/epoch - 51ms/step\n",
      "Epoch 16/200\n",
      "390/390 - 21s - loss: 0.1913 - accuracy: 0.9327 - val_loss: 0.3739 - val_accuracy: 0.8792 - 21s/epoch - 53ms/step\n",
      "Epoch 17/200\n",
      "390/390 - 20s - loss: 0.1863 - accuracy: 0.9350 - val_loss: 0.4407 - val_accuracy: 0.8601 - 20s/epoch - 51ms/step\n",
      "Epoch 18/200\n",
      "390/390 - 21s - loss: 0.1799 - accuracy: 0.9375 - val_loss: 0.3748 - val_accuracy: 0.8821 - 21s/epoch - 54ms/step\n",
      "Epoch 19/200\n",
      "390/390 - 21s - loss: 0.1717 - accuracy: 0.9398 - val_loss: 0.4127 - val_accuracy: 0.8787 - 21s/epoch - 55ms/step\n",
      "Epoch 20/200\n",
      "390/390 - 21s - loss: 0.1722 - accuracy: 0.9391 - val_loss: 0.4487 - val_accuracy: 0.8602 - 21s/epoch - 53ms/step\n",
      "Epoch 21/200\n",
      "390/390 - 21s - loss: 0.1648 - accuracy: 0.9430 - val_loss: 0.5057 - val_accuracy: 0.8516 - 21s/epoch - 53ms/step\n",
      "Epoch 22/200\n",
      "390/390 - 21s - loss: 0.1599 - accuracy: 0.9442 - val_loss: 0.3862 - val_accuracy: 0.8775 - 21s/epoch - 54ms/step\n",
      "Epoch 23/200\n",
      "390/390 - 20s - loss: 0.1552 - accuracy: 0.9468 - val_loss: 0.3723 - val_accuracy: 0.8837 - 20s/epoch - 52ms/step\n",
      "Epoch 24/200\n",
      "390/390 - 21s - loss: 0.1555 - accuracy: 0.9440 - val_loss: 0.4408 - val_accuracy: 0.8652 - 21s/epoch - 54ms/step\n",
      "Epoch 25/200\n",
      "390/390 - 21s - loss: 0.1505 - accuracy: 0.9463 - val_loss: 0.3368 - val_accuracy: 0.8934 - 21s/epoch - 54ms/step\n",
      "Epoch 26/200\n",
      "390/390 - 20s - loss: 0.1491 - accuracy: 0.9479 - val_loss: 0.4371 - val_accuracy: 0.8708 - 20s/epoch - 52ms/step\n",
      "Epoch 27/200\n",
      "390/390 - 20s - loss: 0.1427 - accuracy: 0.9507 - val_loss: 0.4758 - val_accuracy: 0.8683 - 20s/epoch - 51ms/step\n",
      "Epoch 28/200\n",
      "390/390 - 21s - loss: 0.1405 - accuracy: 0.9520 - val_loss: 0.3875 - val_accuracy: 0.8885 - 21s/epoch - 54ms/step\n",
      "Epoch 29/200\n",
      "390/390 - 21s - loss: 0.1400 - accuracy: 0.9508 - val_loss: 0.3458 - val_accuracy: 0.8959 - 21s/epoch - 53ms/step\n",
      "Epoch 30/200\n",
      "390/390 - 20s - loss: 0.1403 - accuracy: 0.9519 - val_loss: 0.4571 - val_accuracy: 0.8686 - 20s/epoch - 51ms/step\n",
      "Epoch 31/200\n",
      "390/390 - 20s - loss: 0.1355 - accuracy: 0.9533 - val_loss: 0.4268 - val_accuracy: 0.8757 - 20s/epoch - 51ms/step\n",
      "Epoch 32/200\n",
      "390/390 - 20s - loss: 0.1348 - accuracy: 0.9521 - val_loss: 0.3937 - val_accuracy: 0.8901 - 20s/epoch - 51ms/step\n",
      "Epoch 33/200\n",
      "390/390 - 21s - loss: 0.1326 - accuracy: 0.9540 - val_loss: 0.3705 - val_accuracy: 0.8909 - 21s/epoch - 53ms/step\n",
      "Epoch 34/200\n",
      "390/390 - 21s - loss: 0.1312 - accuracy: 0.9541 - val_loss: 0.5280 - val_accuracy: 0.8526 - 21s/epoch - 54ms/step\n",
      "Epoch 35/200\n",
      "390/390 - 21s - loss: 0.1287 - accuracy: 0.9542 - val_loss: 0.3990 - val_accuracy: 0.8852 - 21s/epoch - 53ms/step\n",
      "Epoch 36/200\n",
      "390/390 - 20s - loss: 0.1306 - accuracy: 0.9536 - val_loss: 0.4001 - val_accuracy: 0.8838 - 20s/epoch - 51ms/step\n",
      "Epoch 37/200\n",
      "390/390 - 21s - loss: 0.1287 - accuracy: 0.9542 - val_loss: 0.3445 - val_accuracy: 0.8987 - 21s/epoch - 54ms/step\n",
      "Epoch 38/200\n",
      "390/390 - 20s - loss: 0.1268 - accuracy: 0.9560 - val_loss: 0.4108 - val_accuracy: 0.8834 - 20s/epoch - 51ms/step\n",
      "Epoch 39/200\n",
      "390/390 - 21s - loss: 0.1219 - accuracy: 0.9580 - val_loss: 0.4572 - val_accuracy: 0.8746 - 21s/epoch - 53ms/step\n",
      "Epoch 40/200\n",
      "390/390 - 20s - loss: 0.1229 - accuracy: 0.9571 - val_loss: 0.3594 - val_accuracy: 0.8937 - 20s/epoch - 51ms/step\n",
      "Epoch 41/200\n",
      "390/390 - 20s - loss: 0.1197 - accuracy: 0.9578 - val_loss: 0.4208 - val_accuracy: 0.8780 - 20s/epoch - 51ms/step\n",
      "Epoch 42/200\n",
      "390/390 - 20s - loss: 0.1204 - accuracy: 0.9578 - val_loss: 0.5266 - val_accuracy: 0.8539 - 20s/epoch - 51ms/step\n",
      "Epoch 43/200\n",
      "390/390 - 21s - loss: 0.1176 - accuracy: 0.9592 - val_loss: 0.3364 - val_accuracy: 0.9026 - 21s/epoch - 54ms/step\n",
      "Epoch 44/200\n",
      "390/390 - 20s - loss: 0.1222 - accuracy: 0.9569 - val_loss: 0.4108 - val_accuracy: 0.8836 - 20s/epoch - 51ms/step\n",
      "Epoch 45/200\n",
      "390/390 - 21s - loss: 0.1124 - accuracy: 0.9608 - val_loss: 0.4837 - val_accuracy: 0.8652 - 21s/epoch - 53ms/step\n",
      "Epoch 46/200\n",
      "390/390 - 21s - loss: 0.1150 - accuracy: 0.9599 - val_loss: 0.4813 - val_accuracy: 0.8724 - 21s/epoch - 54ms/step\n",
      "Epoch 47/200\n",
      "390/390 - 21s - loss: 0.1143 - accuracy: 0.9593 - val_loss: 0.4228 - val_accuracy: 0.8778 - 21s/epoch - 55ms/step\n",
      "Epoch 48/200\n",
      "390/390 - 20s - loss: 0.1134 - accuracy: 0.9597 - val_loss: 0.5462 - val_accuracy: 0.8555 - 20s/epoch - 52ms/step\n",
      "Epoch 49/200\n",
      "390/390 - 21s - loss: 0.1115 - accuracy: 0.9608 - val_loss: 0.4904 - val_accuracy: 0.8694 - 21s/epoch - 54ms/step\n",
      "Epoch 50/200\n",
      "390/390 - 20s - loss: 0.1110 - accuracy: 0.9613 - val_loss: 0.4168 - val_accuracy: 0.8860 - 20s/epoch - 52ms/step\n",
      "Epoch 51/200\n",
      "390/390 - 21s - loss: 0.1183 - accuracy: 0.9582 - val_loss: 0.3454 - val_accuracy: 0.9011 - 21s/epoch - 53ms/step\n",
      "Epoch 52/200\n",
      "390/390 - 21s - loss: 0.1072 - accuracy: 0.9624 - val_loss: 0.4093 - val_accuracy: 0.8855 - 21s/epoch - 53ms/step\n",
      "Epoch 53/200\n",
      "390/390 - 20s - loss: 0.1095 - accuracy: 0.9617 - val_loss: 0.4801 - val_accuracy: 0.8736 - 20s/epoch - 51ms/step\n",
      "Epoch 54/200\n",
      "390/390 - 21s - loss: 0.1088 - accuracy: 0.9615 - val_loss: 0.3388 - val_accuracy: 0.9068 - 21s/epoch - 53ms/step\n",
      "Epoch 55/200\n",
      "390/390 - 21s - loss: 0.1053 - accuracy: 0.9634 - val_loss: 0.3343 - val_accuracy: 0.9020 - 21s/epoch - 53ms/step\n",
      "Epoch 56/200\n",
      "390/390 - 20s - loss: 0.1063 - accuracy: 0.9631 - val_loss: 0.3427 - val_accuracy: 0.9008 - 20s/epoch - 52ms/step\n",
      "Epoch 57/200\n",
      "390/390 - 21s - loss: 0.1051 - accuracy: 0.9643 - val_loss: 0.3897 - val_accuracy: 0.8910 - 21s/epoch - 53ms/step\n",
      "Epoch 58/200\n",
      "390/390 - 20s - loss: 0.1082 - accuracy: 0.9627 - val_loss: 0.3786 - val_accuracy: 0.8939 - 20s/epoch - 52ms/step\n",
      "Epoch 59/200\n",
      "390/390 - 21s - loss: 0.1018 - accuracy: 0.9640 - val_loss: 0.4385 - val_accuracy: 0.8814 - 21s/epoch - 55ms/step\n",
      "Epoch 60/200\n",
      "390/390 - 20s - loss: 0.1038 - accuracy: 0.9639 - val_loss: 0.3651 - val_accuracy: 0.8970 - 20s/epoch - 51ms/step\n",
      "Epoch 61/200\n",
      "390/390 - 20s - loss: 0.1072 - accuracy: 0.9632 - val_loss: 0.3260 - val_accuracy: 0.9063 - 20s/epoch - 51ms/step\n",
      "Epoch 62/200\n",
      "390/390 - 21s - loss: 0.1058 - accuracy: 0.9631 - val_loss: 0.3390 - val_accuracy: 0.8990 - 21s/epoch - 54ms/step\n",
      "Epoch 63/200\n",
      "390/390 - 21s - loss: 0.1007 - accuracy: 0.9639 - val_loss: 0.4194 - val_accuracy: 0.8852 - 21s/epoch - 53ms/step\n",
      "Epoch 64/200\n",
      "390/390 - 21s - loss: 0.1028 - accuracy: 0.9647 - val_loss: 0.4279 - val_accuracy: 0.8833 - 21s/epoch - 54ms/step\n",
      "Epoch 65/200\n",
      "390/390 - 20s - loss: 0.1024 - accuracy: 0.9644 - val_loss: 0.4034 - val_accuracy: 0.8897 - 20s/epoch - 51ms/step\n",
      "Epoch 66/200\n",
      "390/390 - 21s - loss: 0.1030 - accuracy: 0.9628 - val_loss: 0.3612 - val_accuracy: 0.8975 - 21s/epoch - 55ms/step\n",
      "Epoch 67/200\n",
      "390/390 - 21s - loss: 0.1017 - accuracy: 0.9644 - val_loss: 0.4280 - val_accuracy: 0.8852 - 21s/epoch - 55ms/step\n",
      "Epoch 68/200\n",
      "390/390 - 20s - loss: 0.0961 - accuracy: 0.9663 - val_loss: 0.3620 - val_accuracy: 0.8966 - 20s/epoch - 51ms/step\n",
      "Epoch 69/200\n",
      "390/390 - 21s - loss: 0.1007 - accuracy: 0.9651 - val_loss: 0.4159 - val_accuracy: 0.8823 - 21s/epoch - 53ms/step\n",
      "Epoch 70/200\n",
      "390/390 - 21s - loss: 0.0991 - accuracy: 0.9658 - val_loss: 0.3532 - val_accuracy: 0.8996 - 21s/epoch - 54ms/step\n",
      "Epoch 71/200\n",
      "390/390 - 20s - loss: 0.0990 - accuracy: 0.9657 - val_loss: 0.3657 - val_accuracy: 0.8964 - 20s/epoch - 51ms/step\n",
      "Epoch 72/200\n",
      "390/390 - 21s - loss: 0.1013 - accuracy: 0.9648 - val_loss: 0.5367 - val_accuracy: 0.8560 - 21s/epoch - 55ms/step\n",
      "Epoch 73/200\n",
      "390/390 - 21s - loss: 0.0946 - accuracy: 0.9675 - val_loss: 0.3408 - val_accuracy: 0.9052 - 21s/epoch - 54ms/step\n",
      "Epoch 74/200\n",
      "390/390 - 20s - loss: 0.0956 - accuracy: 0.9670 - val_loss: 0.4590 - val_accuracy: 0.8781 - 20s/epoch - 51ms/step\n",
      "Epoch 75/200\n",
      "390/390 - 21s - loss: 0.0971 - accuracy: 0.9659 - val_loss: 0.3358 - val_accuracy: 0.9031 - 21s/epoch - 54ms/step\n",
      "Epoch 76/200\n",
      "390/390 - 21s - loss: 0.0894 - accuracy: 0.9695 - val_loss: 0.4720 - val_accuracy: 0.8738 - 21s/epoch - 55ms/step\n",
      "Epoch 77/200\n",
      "390/390 - 21s - loss: 0.0972 - accuracy: 0.9655 - val_loss: 0.4221 - val_accuracy: 0.8819 - 21s/epoch - 53ms/step\n",
      "Epoch 78/200\n",
      "390/390 - 21s - loss: 0.0929 - accuracy: 0.9675 - val_loss: 0.3334 - val_accuracy: 0.9036 - 21s/epoch - 54ms/step\n",
      "Epoch 79/200\n",
      "390/390 - 20s - loss: 0.0958 - accuracy: 0.9660 - val_loss: 0.3595 - val_accuracy: 0.8981 - 20s/epoch - 52ms/step\n",
      "Epoch 80/200\n",
      "390/390 - 20s - loss: 0.0929 - accuracy: 0.9672 - val_loss: 0.3555 - val_accuracy: 0.9039 - 20s/epoch - 51ms/step\n",
      "Epoch 81/200\n",
      "390/390 - 21s - loss: 0.0922 - accuracy: 0.9670 - val_loss: 0.4736 - val_accuracy: 0.8796 - 21s/epoch - 53ms/step\n",
      "Epoch 82/200\n",
      "390/390 - 21s - loss: 0.0907 - accuracy: 0.9689 - val_loss: 0.4065 - val_accuracy: 0.8933 - 21s/epoch - 53ms/step\n",
      "Epoch 83/200\n",
      "390/390 - 21s - loss: 0.0945 - accuracy: 0.9673 - val_loss: 0.4290 - val_accuracy: 0.8832 - 21s/epoch - 53ms/step\n",
      "Epoch 84/200\n",
      "390/390 - 20s - loss: 0.0891 - accuracy: 0.9691 - val_loss: 0.3976 - val_accuracy: 0.8952 - 20s/epoch - 51ms/step\n",
      "Epoch 85/200\n",
      "390/390 - 21s - loss: 0.0942 - accuracy: 0.9662 - val_loss: 0.3846 - val_accuracy: 0.8966 - 21s/epoch - 54ms/step\n",
      "Epoch 86/200\n",
      "390/390 - 21s - loss: 0.0897 - accuracy: 0.9687 - val_loss: 0.4482 - val_accuracy: 0.8776 - 21s/epoch - 53ms/step\n",
      "Epoch 87/200\n",
      "390/390 - 20s - loss: 0.0922 - accuracy: 0.9685 - val_loss: 0.3116 - val_accuracy: 0.9100 - 20s/epoch - 52ms/step\n",
      "Epoch 88/200\n",
      "390/390 - 21s - loss: 0.0899 - accuracy: 0.9689 - val_loss: 0.3944 - val_accuracy: 0.8884 - 21s/epoch - 54ms/step\n",
      "Epoch 89/200\n",
      "390/390 - 20s - loss: 0.0890 - accuracy: 0.9690 - val_loss: 0.3520 - val_accuracy: 0.9018 - 20s/epoch - 52ms/step\n",
      "Epoch 90/200\n",
      "390/390 - 21s - loss: 0.0887 - accuracy: 0.9695 - val_loss: 0.3585 - val_accuracy: 0.9023 - 21s/epoch - 55ms/step\n",
      "Epoch 91/200\n",
      "390/390 - 21s - loss: 0.0906 - accuracy: 0.9685 - val_loss: 0.4047 - val_accuracy: 0.8921 - 21s/epoch - 54ms/step\n",
      "Epoch 92/200\n",
      "390/390 - 20s - loss: 0.0906 - accuracy: 0.9681 - val_loss: 0.4535 - val_accuracy: 0.8839 - 20s/epoch - 51ms/step\n",
      "Epoch 93/200\n",
      "390/390 - 20s - loss: 0.0856 - accuracy: 0.9698 - val_loss: 0.4035 - val_accuracy: 0.8915 - 20s/epoch - 51ms/step\n",
      "Epoch 94/200\n",
      "390/390 - 20s - loss: 0.0891 - accuracy: 0.9687 - val_loss: 0.4303 - val_accuracy: 0.8867 - 20s/epoch - 52ms/step\n",
      "Epoch 95/200\n",
      "390/390 - 21s - loss: 0.0827 - accuracy: 0.9704 - val_loss: 0.4673 - val_accuracy: 0.8843 - 21s/epoch - 54ms/step\n",
      "Epoch 96/200\n",
      "390/390 - 21s - loss: 0.0892 - accuracy: 0.9688 - val_loss: 0.4040 - val_accuracy: 0.8845 - 21s/epoch - 54ms/step\n",
      "Epoch 97/200\n",
      "390/390 - 21s - loss: 0.0848 - accuracy: 0.9704 - val_loss: 0.3693 - val_accuracy: 0.9006 - 21s/epoch - 54ms/step\n",
      "Epoch 98/200\n",
      "390/390 - 20s - loss: 0.0892 - accuracy: 0.9690 - val_loss: 0.3852 - val_accuracy: 0.8904 - 20s/epoch - 51ms/step\n",
      "Epoch 99/200\n",
      "390/390 - 21s - loss: 0.0845 - accuracy: 0.9701 - val_loss: 0.3278 - val_accuracy: 0.9096 - 21s/epoch - 54ms/step\n",
      "Epoch 100/200\n",
      "390/390 - 22s - loss: 0.0818 - accuracy: 0.9718 - val_loss: 0.3708 - val_accuracy: 0.9003 - 22s/epoch - 55ms/step\n",
      "Epoch 101/200\n",
      "390/390 - 20s - loss: 0.0849 - accuracy: 0.9704 - val_loss: 0.3639 - val_accuracy: 0.9021 - 20s/epoch - 51ms/step\n",
      "Epoch 102/200\n",
      "390/390 - 21s - loss: 0.0831 - accuracy: 0.9710 - val_loss: 0.4923 - val_accuracy: 0.8742 - 21s/epoch - 54ms/step\n",
      "Epoch 103/200\n",
      "390/390 - 21s - loss: 0.0778 - accuracy: 0.9734 - val_loss: 0.4154 - val_accuracy: 0.8930 - 21s/epoch - 53ms/step\n",
      "Epoch 104/200\n",
      "390/390 - 20s - loss: 0.0880 - accuracy: 0.9696 - val_loss: 0.3833 - val_accuracy: 0.8927 - 20s/epoch - 52ms/step\n",
      "Epoch 105/200\n",
      "390/390 - 21s - loss: 0.0825 - accuracy: 0.9707 - val_loss: 0.3951 - val_accuracy: 0.8970 - 21s/epoch - 54ms/step\n",
      "Epoch 106/200\n",
      "390/390 - 20s - loss: 0.0802 - accuracy: 0.9723 - val_loss: 0.3624 - val_accuracy: 0.9018 - 20s/epoch - 51ms/step\n",
      "Epoch 107/200\n",
      "390/390 - 21s - loss: 0.0829 - accuracy: 0.9717 - val_loss: 0.3611 - val_accuracy: 0.9007 - 21s/epoch - 54ms/step\n",
      "Epoch 108/200\n",
      "390/390 - 21s - loss: 0.0810 - accuracy: 0.9718 - val_loss: 0.4316 - val_accuracy: 0.8791 - 21s/epoch - 53ms/step\n",
      "Epoch 109/200\n",
      "390/390 - 21s - loss: 0.0762 - accuracy: 0.9738 - val_loss: 0.3422 - val_accuracy: 0.9085 - 21s/epoch - 55ms/step\n",
      "Epoch 110/200\n",
      "390/390 - 20s - loss: 0.0800 - accuracy: 0.9714 - val_loss: 0.4287 - val_accuracy: 0.8879 - 20s/epoch - 51ms/step\n",
      "Epoch 111/200\n",
      "390/390 - 20s - loss: 0.0821 - accuracy: 0.9722 - val_loss: 0.4381 - val_accuracy: 0.8848 - 20s/epoch - 51ms/step\n",
      "Epoch 112/200\n",
      "390/390 - 21s - loss: 0.0768 - accuracy: 0.9735 - val_loss: 0.3391 - val_accuracy: 0.9021 - 21s/epoch - 53ms/step\n",
      "Epoch 113/200\n",
      "390/390 - 21s - loss: 0.0782 - accuracy: 0.9727 - val_loss: 0.4157 - val_accuracy: 0.8902 - 21s/epoch - 54ms/step\n",
      "Epoch 114/200\n",
      "390/390 - 20s - loss: 0.0814 - accuracy: 0.9718 - val_loss: 0.3775 - val_accuracy: 0.8982 - 20s/epoch - 51ms/step\n",
      "Epoch 115/200\n",
      "390/390 - 21s - loss: 0.0758 - accuracy: 0.9742 - val_loss: 0.3343 - val_accuracy: 0.9073 - 21s/epoch - 55ms/step\n",
      "Epoch 116/200\n",
      "390/390 - 20s - loss: 0.0781 - accuracy: 0.9721 - val_loss: 0.4328 - val_accuracy: 0.8847 - 20s/epoch - 52ms/step\n",
      "Epoch 117/200\n",
      "390/390 - 21s - loss: 0.0768 - accuracy: 0.9736 - val_loss: 0.3960 - val_accuracy: 0.8943 - 21s/epoch - 54ms/step\n",
      "Epoch 118/200\n",
      "390/390 - 20s - loss: 0.0794 - accuracy: 0.9720 - val_loss: 0.4354 - val_accuracy: 0.8863 - 20s/epoch - 51ms/step\n",
      "Epoch 119/200\n",
      "390/390 - 20s - loss: 0.0731 - accuracy: 0.9750 - val_loss: 0.3350 - val_accuracy: 0.9078 - 20s/epoch - 51ms/step\n",
      "Epoch 120/200\n",
      "390/390 - 20s - loss: 0.0740 - accuracy: 0.9752 - val_loss: 0.4033 - val_accuracy: 0.8916 - 20s/epoch - 51ms/step\n",
      "Epoch 121/200\n",
      "390/390 - 21s - loss: 0.0761 - accuracy: 0.9731 - val_loss: 0.3868 - val_accuracy: 0.8973 - 21s/epoch - 55ms/step\n",
      "Epoch 122/200\n",
      "390/390 - 21s - loss: 0.0741 - accuracy: 0.9740 - val_loss: 0.4509 - val_accuracy: 0.8816 - 21s/epoch - 53ms/step\n",
      "Epoch 123/200\n",
      "390/390 - 21s - loss: 0.0708 - accuracy: 0.9751 - val_loss: 0.3643 - val_accuracy: 0.9056 - 21s/epoch - 54ms/step\n",
      "Epoch 124/200\n",
      "390/390 - 21s - loss: 0.0710 - accuracy: 0.9760 - val_loss: 0.2998 - val_accuracy: 0.9159 - 21s/epoch - 54ms/step\n",
      "Epoch 125/200\n",
      "390/390 - 20s - loss: 0.0710 - accuracy: 0.9754 - val_loss: 0.3431 - val_accuracy: 0.9033 - 20s/epoch - 52ms/step\n",
      "Epoch 126/200\n",
      "390/390 - 21s - loss: 0.0687 - accuracy: 0.9762 - val_loss: 0.4324 - val_accuracy: 0.8870 - 21s/epoch - 54ms/step\n",
      "Epoch 127/200\n",
      "390/390 - 21s - loss: 0.0701 - accuracy: 0.9754 - val_loss: 0.3594 - val_accuracy: 0.9040 - 21s/epoch - 54ms/step\n",
      "Epoch 128/200\n",
      "390/390 - 20s - loss: 0.0723 - accuracy: 0.9749 - val_loss: 0.3414 - val_accuracy: 0.9073 - 20s/epoch - 51ms/step\n",
      "Epoch 129/200\n",
      "390/390 - 21s - loss: 0.0759 - accuracy: 0.9738 - val_loss: 0.3219 - val_accuracy: 0.9115 - 21s/epoch - 54ms/step\n",
      "Epoch 130/200\n",
      "390/390 - 21s - loss: 0.0700 - accuracy: 0.9755 - val_loss: 0.3433 - val_accuracy: 0.9021 - 21s/epoch - 53ms/step\n",
      "Epoch 131/200\n",
      "390/390 - 20s - loss: 0.0667 - accuracy: 0.9769 - val_loss: 0.3803 - val_accuracy: 0.9003 - 20s/epoch - 52ms/step\n",
      "Epoch 132/200\n",
      "390/390 - 20s - loss: 0.0715 - accuracy: 0.9745 - val_loss: 0.3806 - val_accuracy: 0.8941 - 20s/epoch - 53ms/step\n",
      "Epoch 133/200\n",
      "390/390 - 21s - loss: 0.0664 - accuracy: 0.9768 - val_loss: 0.5576 - val_accuracy: 0.8611 - 21s/epoch - 54ms/step\n",
      "Epoch 134/200\n",
      "390/390 - 20s - loss: 0.0667 - accuracy: 0.9775 - val_loss: 0.3892 - val_accuracy: 0.8983 - 20s/epoch - 51ms/step\n",
      "Epoch 135/200\n",
      "390/390 - 21s - loss: 0.0649 - accuracy: 0.9778 - val_loss: 0.3558 - val_accuracy: 0.9035 - 21s/epoch - 54ms/step\n",
      "Epoch 136/200\n",
      "390/390 - 20s - loss: 0.0670 - accuracy: 0.9761 - val_loss: 0.5302 - val_accuracy: 0.8671 - 20s/epoch - 51ms/step\n",
      "Epoch 137/200\n",
      "390/390 - 21s - loss: 0.0668 - accuracy: 0.9762 - val_loss: 0.3292 - val_accuracy: 0.9120 - 21s/epoch - 53ms/step\n",
      "Epoch 138/200\n",
      "390/390 - 21s - loss: 0.0642 - accuracy: 0.9778 - val_loss: 0.3287 - val_accuracy: 0.9125 - 21s/epoch - 53ms/step\n",
      "Epoch 139/200\n",
      "390/390 - 21s - loss: 0.0614 - accuracy: 0.9795 - val_loss: 0.3271 - val_accuracy: 0.9120 - 21s/epoch - 53ms/step\n",
      "Epoch 140/200\n",
      "390/390 - 21s - loss: 0.0619 - accuracy: 0.9783 - val_loss: 0.4186 - val_accuracy: 0.8919 - 21s/epoch - 53ms/step\n",
      "Epoch 141/200\n",
      "390/390 - 20s - loss: 0.0634 - accuracy: 0.9784 - val_loss: 0.5086 - val_accuracy: 0.8762 - 20s/epoch - 51ms/step\n",
      "Epoch 142/200\n",
      "390/390 - 21s - loss: 0.0615 - accuracy: 0.9787 - val_loss: 0.3091 - val_accuracy: 0.9168 - 21s/epoch - 53ms/step\n",
      "Epoch 143/200\n",
      "390/390 - 20s - loss: 0.0621 - accuracy: 0.9789 - val_loss: 0.3231 - val_accuracy: 0.9123 - 20s/epoch - 51ms/step\n",
      "Epoch 144/200\n",
      "390/390 - 20s - loss: 0.0612 - accuracy: 0.9788 - val_loss: 0.4145 - val_accuracy: 0.8901 - 20s/epoch - 51ms/step\n",
      "Epoch 145/200\n",
      "390/390 - 22s - loss: 0.0637 - accuracy: 0.9782 - val_loss: 0.3296 - val_accuracy: 0.9091 - 22s/epoch - 55ms/step\n",
      "Epoch 146/200\n",
      "390/390 - 21s - loss: 0.0627 - accuracy: 0.9788 - val_loss: 0.3535 - val_accuracy: 0.9032 - 21s/epoch - 53ms/step\n",
      "Epoch 147/200\n",
      "390/390 - 22s - loss: 0.0583 - accuracy: 0.9799 - val_loss: 0.3723 - val_accuracy: 0.9001 - 22s/epoch - 55ms/step\n",
      "Epoch 148/200\n",
      "390/390 - 21s - loss: 0.0583 - accuracy: 0.9797 - val_loss: 0.3827 - val_accuracy: 0.8973 - 21s/epoch - 53ms/step\n",
      "Epoch 149/200\n",
      "390/390 - 20s - loss: 0.0541 - accuracy: 0.9814 - val_loss: 0.3278 - val_accuracy: 0.9102 - 20s/epoch - 52ms/step\n",
      "Epoch 150/200\n",
      "390/390 - 20s - loss: 0.0541 - accuracy: 0.9818 - val_loss: 0.3060 - val_accuracy: 0.9208 - 20s/epoch - 52ms/step\n",
      "Epoch 151/200\n",
      "390/390 - 20s - loss: 0.0503 - accuracy: 0.9828 - val_loss: 0.3231 - val_accuracy: 0.9119 - 20s/epoch - 51ms/step\n",
      "Epoch 152/200\n",
      "390/390 - 21s - loss: 0.0566 - accuracy: 0.9810 - val_loss: 0.3115 - val_accuracy: 0.9165 - 21s/epoch - 53ms/step\n",
      "Epoch 153/200\n",
      "390/390 - 21s - loss: 0.0565 - accuracy: 0.9810 - val_loss: 0.3131 - val_accuracy: 0.9145 - 21s/epoch - 53ms/step\n",
      "Epoch 154/200\n",
      "390/390 - 21s - loss: 0.0537 - accuracy: 0.9820 - val_loss: 0.5913 - val_accuracy: 0.8579 - 21s/epoch - 54ms/step\n",
      "Epoch 155/200\n",
      "390/390 - 20s - loss: 0.0506 - accuracy: 0.9829 - val_loss: 0.3069 - val_accuracy: 0.9180 - 20s/epoch - 52ms/step\n",
      "Epoch 156/200\n",
      "390/390 - 21s - loss: 0.0493 - accuracy: 0.9833 - val_loss: 0.3619 - val_accuracy: 0.9017 - 21s/epoch - 54ms/step\n",
      "Epoch 157/200\n",
      "390/390 - 20s - loss: 0.0514 - accuracy: 0.9826 - val_loss: 0.3345 - val_accuracy: 0.9116 - 20s/epoch - 52ms/step\n",
      "Epoch 158/200\n",
      "390/390 - 20s - loss: 0.0473 - accuracy: 0.9839 - val_loss: 0.3104 - val_accuracy: 0.9156 - 20s/epoch - 51ms/step\n",
      "Epoch 159/200\n",
      "390/390 - 20s - loss: 0.0450 - accuracy: 0.9849 - val_loss: 0.3423 - val_accuracy: 0.9106 - 20s/epoch - 51ms/step\n",
      "Epoch 160/200\n",
      "390/390 - 21s - loss: 0.0462 - accuracy: 0.9843 - val_loss: 0.3611 - val_accuracy: 0.9053 - 21s/epoch - 54ms/step\n",
      "Epoch 161/200\n",
      "390/390 - 20s - loss: 0.0466 - accuracy: 0.9844 - val_loss: 0.3442 - val_accuracy: 0.9100 - 20s/epoch - 52ms/step\n",
      "Epoch 162/200\n",
      "390/390 - 21s - loss: 0.0463 - accuracy: 0.9844 - val_loss: 0.2904 - val_accuracy: 0.9196 - 21s/epoch - 54ms/step\n",
      "Epoch 163/200\n",
      "390/390 - 21s - loss: 0.0426 - accuracy: 0.9860 - val_loss: 0.3402 - val_accuracy: 0.9111 - 21s/epoch - 53ms/step\n",
      "Epoch 164/200\n",
      "390/390 - 20s - loss: 0.0452 - accuracy: 0.9847 - val_loss: 0.3931 - val_accuracy: 0.8968 - 20s/epoch - 51ms/step\n",
      "Epoch 165/200\n",
      "390/390 - 21s - loss: 0.0410 - accuracy: 0.9865 - val_loss: 0.3353 - val_accuracy: 0.9118 - 21s/epoch - 53ms/step\n",
      "Epoch 166/200\n",
      "390/390 - 20s - loss: 0.0379 - accuracy: 0.9878 - val_loss: 0.3007 - val_accuracy: 0.9186 - 20s/epoch - 51ms/step\n",
      "Epoch 167/200\n",
      "390/390 - 20s - loss: 0.0361 - accuracy: 0.9881 - val_loss: 0.3094 - val_accuracy: 0.9170 - 20s/epoch - 51ms/step\n",
      "Epoch 168/200\n",
      "390/390 - 21s - loss: 0.0357 - accuracy: 0.9881 - val_loss: 0.3327 - val_accuracy: 0.9155 - 21s/epoch - 54ms/step\n",
      "Epoch 169/200\n",
      "390/390 - 21s - loss: 0.0376 - accuracy: 0.9879 - val_loss: 0.3726 - val_accuracy: 0.9041 - 21s/epoch - 53ms/step\n",
      "Epoch 170/200\n",
      "390/390 - 20s - loss: 0.0383 - accuracy: 0.9876 - val_loss: 0.3177 - val_accuracy: 0.9130 - 20s/epoch - 53ms/step\n",
      "Epoch 171/200\n",
      "390/390 - 21s - loss: 0.0372 - accuracy: 0.9878 - val_loss: 0.2892 - val_accuracy: 0.9211 - 21s/epoch - 54ms/step\n",
      "Epoch 172/200\n",
      "390/390 - 21s - loss: 0.0325 - accuracy: 0.9893 - val_loss: 0.3143 - val_accuracy: 0.9146 - 21s/epoch - 53ms/step\n",
      "Epoch 173/200\n",
      "390/390 - 20s - loss: 0.0294 - accuracy: 0.9905 - val_loss: 0.3109 - val_accuracy: 0.9182 - 20s/epoch - 51ms/step\n",
      "Epoch 174/200\n",
      "390/390 - 21s - loss: 0.0261 - accuracy: 0.9919 - val_loss: 0.3033 - val_accuracy: 0.9201 - 21s/epoch - 54ms/step\n",
      "Epoch 175/200\n",
      "390/390 - 21s - loss: 0.0259 - accuracy: 0.9920 - val_loss: 0.2701 - val_accuracy: 0.9272 - 21s/epoch - 53ms/step\n",
      "Epoch 176/200\n",
      "390/390 - 22s - loss: 0.0234 - accuracy: 0.9926 - val_loss: 0.2980 - val_accuracy: 0.9190 - 22s/epoch - 56ms/step\n",
      "Epoch 177/200\n",
      "390/390 - 21s - loss: 0.0227 - accuracy: 0.9930 - val_loss: 0.3072 - val_accuracy: 0.9215 - 21s/epoch - 54ms/step\n",
      "Epoch 178/200\n",
      "390/390 - 20s - loss: 0.0215 - accuracy: 0.9935 - val_loss: 0.2578 - val_accuracy: 0.9319 - 20s/epoch - 51ms/step\n",
      "Epoch 179/200\n",
      "390/390 - 20s - loss: 0.0202 - accuracy: 0.9938 - val_loss: 0.2539 - val_accuracy: 0.9297 - 20s/epoch - 51ms/step\n",
      "Epoch 180/200\n",
      "390/390 - 20s - loss: 0.0193 - accuracy: 0.9947 - val_loss: 0.2660 - val_accuracy: 0.9315 - 20s/epoch - 51ms/step\n",
      "Epoch 181/200\n",
      "390/390 - 21s - loss: 0.0177 - accuracy: 0.9950 - val_loss: 0.2871 - val_accuracy: 0.9221 - 21s/epoch - 55ms/step\n",
      "Epoch 182/200\n",
      "390/390 - 20s - loss: 0.0143 - accuracy: 0.9965 - val_loss: 0.2223 - val_accuracy: 0.9382 - 20s/epoch - 52ms/step\n",
      "Epoch 183/200\n",
      "390/390 - 20s - loss: 0.0143 - accuracy: 0.9960 - val_loss: 0.2498 - val_accuracy: 0.9308 - 20s/epoch - 51ms/step\n",
      "Epoch 184/200\n",
      "390/390 - 20s - loss: 0.0136 - accuracy: 0.9962 - val_loss: 0.2172 - val_accuracy: 0.9390 - 20s/epoch - 51ms/step\n",
      "Epoch 185/200\n",
      "390/390 - 21s - loss: 0.0124 - accuracy: 0.9969 - val_loss: 0.2237 - val_accuracy: 0.9368 - 21s/epoch - 54ms/step\n",
      "Epoch 186/200\n",
      "390/390 - 21s - loss: 0.0111 - accuracy: 0.9975 - val_loss: 0.2034 - val_accuracy: 0.9428 - 21s/epoch - 54ms/step\n",
      "Epoch 187/200\n",
      "390/390 - 22s - loss: 0.0112 - accuracy: 0.9976 - val_loss: 0.2233 - val_accuracy: 0.9385 - 22s/epoch - 55ms/step\n",
      "Epoch 188/200\n",
      "390/390 - 20s - loss: 0.0093 - accuracy: 0.9983 - val_loss: 0.2068 - val_accuracy: 0.9406 - 20s/epoch - 51ms/step\n",
      "Epoch 189/200\n",
      "390/390 - 20s - loss: 0.0085 - accuracy: 0.9989 - val_loss: 0.2041 - val_accuracy: 0.9418 - 20s/epoch - 51ms/step\n",
      "Epoch 190/200\n",
      "390/390 - 21s - loss: 0.0087 - accuracy: 0.9988 - val_loss: 0.1930 - val_accuracy: 0.9447 - 21s/epoch - 53ms/step\n",
      "Epoch 191/200\n",
      "390/390 - 21s - loss: 0.0085 - accuracy: 0.9988 - val_loss: 0.1978 - val_accuracy: 0.9441 - 21s/epoch - 54ms/step\n",
      "Epoch 192/200\n",
      "390/390 - 20s - loss: 0.0089 - accuracy: 0.9990 - val_loss: 0.1885 - val_accuracy: 0.9430 - 20s/epoch - 51ms/step\n",
      "Epoch 193/200\n",
      "390/390 - 21s - loss: 0.0094 - accuracy: 0.9991 - val_loss: 0.1870 - val_accuracy: 0.9453 - 21s/epoch - 54ms/step\n",
      "Epoch 194/200\n",
      "390/390 - 21s - loss: 0.0102 - accuracy: 0.9990 - val_loss: 0.1911 - val_accuracy: 0.9430 - 21s/epoch - 54ms/step\n",
      "Epoch 195/200\n",
      "390/390 - 21s - loss: 0.0113 - accuracy: 0.9991 - val_loss: 0.1844 - val_accuracy: 0.9438 - 21s/epoch - 53ms/step\n",
      "Epoch 196/200\n",
      "390/390 - 21s - loss: 0.0126 - accuracy: 0.9993 - val_loss: 0.1828 - val_accuracy: 0.9444 - 21s/epoch - 54ms/step\n",
      "Epoch 197/200\n",
      "390/390 - 20s - loss: 0.0144 - accuracy: 0.9993 - val_loss: 0.1799 - val_accuracy: 0.9446 - 20s/epoch - 51ms/step\n",
      "Epoch 198/200\n",
      "390/390 - 20s - loss: 0.0181 - accuracy: 0.9990 - val_loss: 0.1797 - val_accuracy: 0.9452 - 20s/epoch - 51ms/step\n",
      "Epoch 199/200\n",
      "390/390 - 21s - loss: 0.0222 - accuracy: 0.9992 - val_loss: 0.1791 - val_accuracy: 0.9461 - 21s/epoch - 53ms/step\n",
      "Epoch 200/200\n",
      "390/390 - 20s - loss: 0.0282 - accuracy: 0.9993 - val_loss: 0.1820 - val_accuracy: 0.9458 - 20s/epoch - 51ms/step\n",
      " Using best val_acc=0.9461 from last 20 epochs\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.1791 - accuracy: 0.9461\n",
      " Finished: Lam=0.8, Repeat=3, Acc=0.9461\n",
      "\n",
      " lambda: Lam=0.7, Repeat=1/3\n",
      "34 0\n",
      "53 1\n",
      "47 2\n",
      "94 0\n",
      "101 1\n",
      "107 2\n",
      "210 0\n",
      "189 1\n",
      "196 2\n",
      "382 0\n",
      "463 1\n",
      "339 2\n",
      "321009422 561714176\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 1/200\n",
      "390/390 - 25s - loss: 0.7080 - accuracy: 0.7672 - val_loss: 4.9204 - val_accuracy: 0.2543 - 25s/epoch - 63ms/step\n",
      "Epoch 2/200\n",
      "390/390 - 21s - loss: 0.5900 - accuracy: 0.7999 - val_loss: 1.3321 - val_accuracy: 0.6674 - 21s/epoch - 53ms/step\n",
      "Epoch 3/200\n",
      "390/390 - 21s - loss: 0.5033 - accuracy: 0.8304 - val_loss: 1.2523 - val_accuracy: 0.6542 - 21s/epoch - 53ms/step\n",
      "Epoch 4/200\n",
      "390/390 - 20s - loss: 0.4422 - accuracy: 0.8490 - val_loss: 0.7441 - val_accuracy: 0.7690 - 20s/epoch - 51ms/step\n",
      "Epoch 5/200\n",
      "390/390 - 21s - loss: 0.4032 - accuracy: 0.8630 - val_loss: 0.6355 - val_accuracy: 0.8046 - 21s/epoch - 53ms/step\n",
      "Epoch 6/200\n",
      "390/390 - 20s - loss: 0.3522 - accuracy: 0.8792 - val_loss: 0.5212 - val_accuracy: 0.8376 - 20s/epoch - 51ms/step\n",
      "Epoch 7/200\n",
      "390/390 - 21s - loss: 0.3151 - accuracy: 0.8915 - val_loss: 0.4433 - val_accuracy: 0.8578 - 21s/epoch - 55ms/step\n",
      "Epoch 8/200\n",
      "390/390 - 20s - loss: 0.2817 - accuracy: 0.9025 - val_loss: 0.5870 - val_accuracy: 0.8140 - 20s/epoch - 51ms/step\n",
      "Epoch 9/200\n",
      "390/390 - 20s - loss: 0.2544 - accuracy: 0.9122 - val_loss: 0.5804 - val_accuracy: 0.8311 - 20s/epoch - 51ms/step\n",
      "Epoch 10/200\n",
      "390/390 - 21s - loss: 0.2374 - accuracy: 0.9178 - val_loss: 0.4086 - val_accuracy: 0.8658 - 21s/epoch - 53ms/step\n",
      "Epoch 11/200\n",
      "390/390 - 20s - loss: 0.2240 - accuracy: 0.9221 - val_loss: 0.4268 - val_accuracy: 0.8650 - 20s/epoch - 51ms/step\n",
      "Epoch 12/200\n",
      "390/390 - 21s - loss: 0.2091 - accuracy: 0.9282 - val_loss: 0.3908 - val_accuracy: 0.8770 - 21s/epoch - 53ms/step\n",
      "Epoch 13/200\n",
      "390/390 - 20s - loss: 0.2051 - accuracy: 0.9291 - val_loss: 0.3524 - val_accuracy: 0.8844 - 20s/epoch - 51ms/step\n",
      "Epoch 14/200\n",
      "390/390 - 20s - loss: 0.1899 - accuracy: 0.9332 - val_loss: 0.3910 - val_accuracy: 0.8759 - 20s/epoch - 51ms/step\n",
      "Epoch 15/200\n",
      "390/390 - 21s - loss: 0.1861 - accuracy: 0.9354 - val_loss: 0.4507 - val_accuracy: 0.8639 - 21s/epoch - 54ms/step\n",
      "Epoch 16/200\n",
      "390/390 - 21s - loss: 0.1773 - accuracy: 0.9380 - val_loss: 0.3733 - val_accuracy: 0.8890 - 21s/epoch - 53ms/step\n",
      "Epoch 17/200\n",
      "390/390 - 21s - loss: 0.1695 - accuracy: 0.9411 - val_loss: 0.4107 - val_accuracy: 0.8672 - 21s/epoch - 54ms/step\n",
      "Epoch 18/200\n",
      "390/390 - 21s - loss: 0.1650 - accuracy: 0.9429 - val_loss: 0.4087 - val_accuracy: 0.8732 - 21s/epoch - 53ms/step\n",
      "Epoch 19/200\n",
      "390/390 - 21s - loss: 0.1614 - accuracy: 0.9440 - val_loss: 0.4154 - val_accuracy: 0.8818 - 21s/epoch - 54ms/step\n",
      "Epoch 20/200\n",
      "390/390 - 21s - loss: 0.1586 - accuracy: 0.9444 - val_loss: 0.3999 - val_accuracy: 0.8731 - 21s/epoch - 53ms/step\n",
      "Epoch 21/200\n",
      "390/390 - 21s - loss: 0.1511 - accuracy: 0.9470 - val_loss: 0.3603 - val_accuracy: 0.8923 - 21s/epoch - 54ms/step\n",
      "Epoch 22/200\n",
      "390/390 - 21s - loss: 0.1538 - accuracy: 0.9472 - val_loss: 0.3503 - val_accuracy: 0.8911 - 21s/epoch - 54ms/step\n",
      "Epoch 23/200\n",
      "390/390 - 20s - loss: 0.1473 - accuracy: 0.9492 - val_loss: 0.4409 - val_accuracy: 0.8690 - 20s/epoch - 51ms/step\n",
      "Epoch 24/200\n",
      "390/390 - 21s - loss: 0.1383 - accuracy: 0.9509 - val_loss: 0.4672 - val_accuracy: 0.8680 - 21s/epoch - 53ms/step\n",
      "Epoch 25/200\n",
      "390/390 - 21s - loss: 0.1348 - accuracy: 0.9534 - val_loss: 0.4129 - val_accuracy: 0.8785 - 21s/epoch - 53ms/step\n",
      "Epoch 26/200\n",
      "390/390 - 21s - loss: 0.1372 - accuracy: 0.9517 - val_loss: 0.4718 - val_accuracy: 0.8686 - 21s/epoch - 55ms/step\n",
      "Epoch 27/200\n",
      "390/390 - 20s - loss: 0.1338 - accuracy: 0.9540 - val_loss: 0.3444 - val_accuracy: 0.8983 - 20s/epoch - 51ms/step\n",
      "Epoch 28/200\n",
      "390/390 - 21s - loss: 0.1302 - accuracy: 0.9539 - val_loss: 0.4249 - val_accuracy: 0.8795 - 21s/epoch - 53ms/step\n",
      "Epoch 29/200\n",
      "390/390 - 20s - loss: 0.1290 - accuracy: 0.9558 - val_loss: 0.4711 - val_accuracy: 0.8712 - 20s/epoch - 51ms/step\n",
      "Epoch 30/200\n",
      "390/390 - 20s - loss: 0.1258 - accuracy: 0.9564 - val_loss: 0.4966 - val_accuracy: 0.8562 - 20s/epoch - 51ms/step\n",
      "Epoch 31/200\n",
      "390/390 - 21s - loss: 0.1270 - accuracy: 0.9557 - val_loss: 0.3660 - val_accuracy: 0.8869 - 21s/epoch - 55ms/step\n",
      "Epoch 32/200\n",
      "390/390 - 20s - loss: 0.1256 - accuracy: 0.9561 - val_loss: 0.3426 - val_accuracy: 0.8982 - 20s/epoch - 51ms/step\n",
      "Epoch 33/200\n",
      "390/390 - 21s - loss: 0.1231 - accuracy: 0.9569 - val_loss: 0.4088 - val_accuracy: 0.8801 - 21s/epoch - 54ms/step\n",
      "Epoch 34/200\n",
      "390/390 - 21s - loss: 0.1221 - accuracy: 0.9576 - val_loss: 0.4429 - val_accuracy: 0.8669 - 21s/epoch - 53ms/step\n",
      "Epoch 35/200\n",
      "390/390 - 20s - loss: 0.1201 - accuracy: 0.9583 - val_loss: 0.3787 - val_accuracy: 0.8882 - 20s/epoch - 51ms/step\n",
      "Epoch 36/200\n",
      "390/390 - 21s - loss: 0.1201 - accuracy: 0.9577 - val_loss: 0.3758 - val_accuracy: 0.8950 - 21s/epoch - 53ms/step\n",
      "Epoch 37/200\n",
      "390/390 - 21s - loss: 0.1221 - accuracy: 0.9579 - val_loss: 0.4644 - val_accuracy: 0.8732 - 21s/epoch - 55ms/step\n",
      "Epoch 38/200\n",
      "390/390 - 20s - loss: 0.1196 - accuracy: 0.9572 - val_loss: 0.3766 - val_accuracy: 0.8911 - 20s/epoch - 51ms/step\n",
      "Epoch 39/200\n",
      "390/390 - 21s - loss: 0.1156 - accuracy: 0.9593 - val_loss: 0.3603 - val_accuracy: 0.8979 - 21s/epoch - 54ms/step\n",
      "Epoch 40/200\n",
      "390/390 - 20s - loss: 0.1154 - accuracy: 0.9601 - val_loss: 0.4885 - val_accuracy: 0.8626 - 20s/epoch - 51ms/step\n",
      "Epoch 41/200\n",
      "390/390 - 20s - loss: 0.1094 - accuracy: 0.9621 - val_loss: 0.3220 - val_accuracy: 0.9006 - 20s/epoch - 51ms/step\n",
      "Epoch 42/200\n",
      "390/390 - 21s - loss: 0.1121 - accuracy: 0.9610 - val_loss: 0.4139 - val_accuracy: 0.8859 - 21s/epoch - 53ms/step\n",
      "Epoch 43/200\n",
      "390/390 - 21s - loss: 0.1107 - accuracy: 0.9617 - val_loss: 0.4578 - val_accuracy: 0.8762 - 21s/epoch - 53ms/step\n",
      "Epoch 44/200\n",
      "390/390 - 20s - loss: 0.1080 - accuracy: 0.9621 - val_loss: 0.3345 - val_accuracy: 0.9008 - 20s/epoch - 51ms/step\n",
      "Epoch 45/200\n",
      "390/390 - 21s - loss: 0.1084 - accuracy: 0.9613 - val_loss: 0.3900 - val_accuracy: 0.8869 - 21s/epoch - 53ms/step\n",
      "Epoch 46/200\n",
      "390/390 - 21s - loss: 0.1055 - accuracy: 0.9632 - val_loss: 0.3577 - val_accuracy: 0.8983 - 21s/epoch - 53ms/step\n",
      "Epoch 47/200\n",
      "390/390 - 21s - loss: 0.1091 - accuracy: 0.9618 - val_loss: 0.4280 - val_accuracy: 0.8804 - 21s/epoch - 53ms/step\n",
      "Epoch 48/200\n",
      "390/390 - 21s - loss: 0.1064 - accuracy: 0.9624 - val_loss: 0.4121 - val_accuracy: 0.8796 - 21s/epoch - 53ms/step\n",
      "Epoch 49/200\n",
      "390/390 - 21s - loss: 0.1046 - accuracy: 0.9641 - val_loss: 0.3360 - val_accuracy: 0.9060 - 21s/epoch - 55ms/step\n",
      "Epoch 50/200\n",
      "390/390 - 21s - loss: 0.1040 - accuracy: 0.9639 - val_loss: 0.4108 - val_accuracy: 0.8850 - 21s/epoch - 54ms/step\n",
      "Epoch 51/200\n",
      "390/390 - 21s - loss: 0.0996 - accuracy: 0.9660 - val_loss: 0.4579 - val_accuracy: 0.8809 - 21s/epoch - 54ms/step\n",
      "Epoch 52/200\n",
      "390/390 - 21s - loss: 0.1027 - accuracy: 0.9632 - val_loss: 0.5752 - val_accuracy: 0.8553 - 21s/epoch - 54ms/step\n",
      "Epoch 53/200\n",
      "390/390 - 21s - loss: 0.1012 - accuracy: 0.9645 - val_loss: 0.3237 - val_accuracy: 0.9049 - 21s/epoch - 53ms/step\n",
      "Epoch 54/200\n",
      "390/390 - 20s - loss: 0.1011 - accuracy: 0.9639 - val_loss: 0.3312 - val_accuracy: 0.9028 - 20s/epoch - 51ms/step\n",
      "Epoch 55/200\n",
      "390/390 - 20s - loss: 0.0991 - accuracy: 0.9654 - val_loss: 0.4966 - val_accuracy: 0.8722 - 20s/epoch - 51ms/step\n",
      "Epoch 56/200\n",
      "390/390 - 21s - loss: 0.1016 - accuracy: 0.9649 - val_loss: 0.3919 - val_accuracy: 0.8900 - 21s/epoch - 53ms/step\n",
      "Epoch 57/200\n",
      "390/390 - 21s - loss: 0.1001 - accuracy: 0.9651 - val_loss: 0.3871 - val_accuracy: 0.8939 - 21s/epoch - 53ms/step\n",
      "Epoch 58/200\n",
      "390/390 - 21s - loss: 0.1009 - accuracy: 0.9649 - val_loss: 0.3656 - val_accuracy: 0.8958 - 21s/epoch - 55ms/step\n",
      "Epoch 59/200\n",
      "390/390 - 21s - loss: 0.1018 - accuracy: 0.9646 - val_loss: 0.3503 - val_accuracy: 0.8998 - 21s/epoch - 55ms/step\n",
      "Epoch 60/200\n",
      "390/390 - 21s - loss: 0.0963 - accuracy: 0.9665 - val_loss: 0.4057 - val_accuracy: 0.8901 - 21s/epoch - 53ms/step\n",
      "Epoch 61/200\n",
      "390/390 - 21s - loss: 0.1004 - accuracy: 0.9648 - val_loss: 0.3695 - val_accuracy: 0.8880 - 21s/epoch - 54ms/step\n",
      "Epoch 62/200\n",
      "390/390 - 21s - loss: 0.0963 - accuracy: 0.9662 - val_loss: 0.6580 - val_accuracy: 0.8423 - 21s/epoch - 53ms/step\n",
      "Epoch 63/200\n",
      "390/390 - 21s - loss: 0.0974 - accuracy: 0.9661 - val_loss: 0.4364 - val_accuracy: 0.8819 - 21s/epoch - 54ms/step\n",
      "Epoch 64/200\n",
      "390/390 - 20s - loss: 0.0991 - accuracy: 0.9650 - val_loss: 0.3218 - val_accuracy: 0.9032 - 20s/epoch - 51ms/step\n",
      "Epoch 65/200\n",
      "390/390 - 21s - loss: 0.0965 - accuracy: 0.9662 - val_loss: 0.3248 - val_accuracy: 0.9083 - 21s/epoch - 54ms/step\n",
      "Epoch 66/200\n",
      "390/390 - 20s - loss: 0.0928 - accuracy: 0.9674 - val_loss: 0.4720 - val_accuracy: 0.8731 - 20s/epoch - 51ms/step\n",
      "Epoch 67/200\n",
      "390/390 - 20s - loss: 0.0951 - accuracy: 0.9663 - val_loss: 0.3566 - val_accuracy: 0.8960 - 20s/epoch - 51ms/step\n",
      "Epoch 68/200\n",
      "390/390 - 20s - loss: 0.0913 - accuracy: 0.9679 - val_loss: 0.4834 - val_accuracy: 0.8721 - 20s/epoch - 51ms/step\n",
      "Epoch 69/200\n",
      "390/390 - 20s - loss: 0.0938 - accuracy: 0.9676 - val_loss: 0.5819 - val_accuracy: 0.8549 - 20s/epoch - 51ms/step\n",
      "Epoch 70/200\n",
      "390/390 - 21s - loss: 0.0949 - accuracy: 0.9670 - val_loss: 0.4317 - val_accuracy: 0.8817 - 21s/epoch - 53ms/step\n",
      "Epoch 71/200\n",
      "390/390 - 20s - loss: 0.0927 - accuracy: 0.9678 - val_loss: 0.4622 - val_accuracy: 0.8802 - 20s/epoch - 51ms/step\n",
      "Epoch 72/200\n",
      "390/390 - 21s - loss: 0.0937 - accuracy: 0.9674 - val_loss: 0.3317 - val_accuracy: 0.9087 - 21s/epoch - 53ms/step\n",
      "Epoch 73/200\n",
      "390/390 - 20s - loss: 0.0892 - accuracy: 0.9692 - val_loss: 0.4656 - val_accuracy: 0.8769 - 20s/epoch - 51ms/step\n",
      "Epoch 74/200\n",
      "390/390 - 20s - loss: 0.0907 - accuracy: 0.9684 - val_loss: 0.4015 - val_accuracy: 0.8903 - 20s/epoch - 51ms/step\n",
      "Epoch 75/200\n",
      "390/390 - 21s - loss: 0.0905 - accuracy: 0.9689 - val_loss: 0.3441 - val_accuracy: 0.9033 - 21s/epoch - 54ms/step\n",
      "Epoch 76/200\n",
      "390/390 - 21s - loss: 0.0917 - accuracy: 0.9680 - val_loss: 0.4811 - val_accuracy: 0.8684 - 21s/epoch - 54ms/step\n",
      "Epoch 77/200\n",
      "390/390 - 21s - loss: 0.0849 - accuracy: 0.9708 - val_loss: 0.4375 - val_accuracy: 0.8797 - 21s/epoch - 53ms/step\n",
      "Epoch 78/200\n",
      "390/390 - 21s - loss: 0.0907 - accuracy: 0.9684 - val_loss: 0.3487 - val_accuracy: 0.9057 - 21s/epoch - 53ms/step\n",
      "Epoch 79/200\n",
      "390/390 - 21s - loss: 0.0882 - accuracy: 0.9693 - val_loss: 0.3816 - val_accuracy: 0.8916 - 21s/epoch - 53ms/step\n",
      "Epoch 80/200\n",
      "390/390 - 20s - loss: 0.0913 - accuracy: 0.9682 - val_loss: 0.4174 - val_accuracy: 0.8845 - 20s/epoch - 51ms/step\n",
      "Epoch 81/200\n",
      "390/390 - 21s - loss: 0.0876 - accuracy: 0.9697 - val_loss: 0.4377 - val_accuracy: 0.8793 - 21s/epoch - 54ms/step\n",
      "Epoch 82/200\n",
      "390/390 - 21s - loss: 0.0837 - accuracy: 0.9712 - val_loss: 0.3686 - val_accuracy: 0.9041 - 21s/epoch - 53ms/step\n",
      "Epoch 83/200\n",
      "390/390 - 20s - loss: 0.0910 - accuracy: 0.9682 - val_loss: 0.4955 - val_accuracy: 0.8673 - 20s/epoch - 51ms/step\n",
      "Epoch 84/200\n",
      "390/390 - 21s - loss: 0.0834 - accuracy: 0.9712 - val_loss: 0.4425 - val_accuracy: 0.8881 - 21s/epoch - 53ms/step\n",
      "Epoch 85/200\n",
      "390/390 - 21s - loss: 0.0827 - accuracy: 0.9707 - val_loss: 0.5044 - val_accuracy: 0.8689 - 21s/epoch - 53ms/step\n",
      "Epoch 86/200\n",
      "390/390 - 21s - loss: 0.0832 - accuracy: 0.9715 - val_loss: 0.4561 - val_accuracy: 0.8776 - 21s/epoch - 55ms/step\n",
      "Epoch 87/200\n",
      "390/390 - 21s - loss: 0.0860 - accuracy: 0.9703 - val_loss: 0.3578 - val_accuracy: 0.8999 - 21s/epoch - 54ms/step\n",
      "Epoch 88/200\n",
      "390/390 - 21s - loss: 0.0840 - accuracy: 0.9709 - val_loss: 0.3487 - val_accuracy: 0.9077 - 21s/epoch - 53ms/step\n",
      "Epoch 89/200\n",
      "390/390 - 22s - loss: 0.0836 - accuracy: 0.9711 - val_loss: 0.4435 - val_accuracy: 0.8892 - 22s/epoch - 56ms/step\n",
      "Epoch 90/200\n",
      "390/390 - 21s - loss: 0.0865 - accuracy: 0.9695 - val_loss: 0.3264 - val_accuracy: 0.9074 - 21s/epoch - 53ms/step\n",
      "Epoch 91/200\n",
      "390/390 - 20s - loss: 0.0805 - accuracy: 0.9725 - val_loss: 0.3241 - val_accuracy: 0.9069 - 20s/epoch - 51ms/step\n",
      "Epoch 92/200\n",
      "390/390 - 20s - loss: 0.0780 - accuracy: 0.9730 - val_loss: 0.3980 - val_accuracy: 0.8946 - 20s/epoch - 51ms/step\n",
      "Epoch 93/200\n",
      "390/390 - 21s - loss: 0.0804 - accuracy: 0.9720 - val_loss: 0.3588 - val_accuracy: 0.9046 - 21s/epoch - 53ms/step\n",
      "Epoch 94/200\n",
      "390/390 - 21s - loss: 0.0851 - accuracy: 0.9704 - val_loss: 0.4268 - val_accuracy: 0.8805 - 21s/epoch - 54ms/step\n",
      "Epoch 95/200\n",
      "390/390 - 20s - loss: 0.0837 - accuracy: 0.9699 - val_loss: 0.3293 - val_accuracy: 0.9105 - 20s/epoch - 51ms/step\n",
      "Epoch 96/200\n",
      "390/390 - 21s - loss: 0.0791 - accuracy: 0.9732 - val_loss: 0.3365 - val_accuracy: 0.9017 - 21s/epoch - 53ms/step\n",
      "Epoch 97/200\n",
      "390/390 - 21s - loss: 0.0793 - accuracy: 0.9730 - val_loss: 0.3365 - val_accuracy: 0.9101 - 21s/epoch - 53ms/step\n",
      "Epoch 98/200\n",
      "390/390 - 20s - loss: 0.0813 - accuracy: 0.9724 - val_loss: 0.4849 - val_accuracy: 0.8723 - 20s/epoch - 52ms/step\n",
      "Epoch 99/200\n",
      "390/390 - 20s - loss: 0.0793 - accuracy: 0.9727 - val_loss: 0.3869 - val_accuracy: 0.8974 - 20s/epoch - 51ms/step\n",
      "Epoch 100/200\n",
      "390/390 - 21s - loss: 0.0813 - accuracy: 0.9713 - val_loss: 0.3844 - val_accuracy: 0.8914 - 21s/epoch - 53ms/step\n",
      "Epoch 101/200\n",
      "390/390 - 21s - loss: 0.0792 - accuracy: 0.9723 - val_loss: 0.3633 - val_accuracy: 0.9006 - 21s/epoch - 53ms/step\n",
      "Epoch 102/200\n",
      "390/390 - 21s - loss: 0.0785 - accuracy: 0.9722 - val_loss: 0.4310 - val_accuracy: 0.8868 - 21s/epoch - 54ms/step\n",
      "Epoch 103/200\n",
      "390/390 - 21s - loss: 0.0769 - accuracy: 0.9735 - val_loss: 0.4416 - val_accuracy: 0.8879 - 21s/epoch - 53ms/step\n",
      "Epoch 104/200\n",
      "390/390 - 20s - loss: 0.0788 - accuracy: 0.9720 - val_loss: 0.3383 - val_accuracy: 0.9023 - 20s/epoch - 51ms/step\n",
      "Epoch 105/200\n",
      "390/390 - 21s - loss: 0.0770 - accuracy: 0.9731 - val_loss: 0.4769 - val_accuracy: 0.8742 - 21s/epoch - 53ms/step\n",
      "Epoch 106/200\n",
      "390/390 - 21s - loss: 0.0796 - accuracy: 0.9728 - val_loss: 0.4268 - val_accuracy: 0.8870 - 21s/epoch - 54ms/step\n",
      "Epoch 107/200\n",
      "390/390 - 20s - loss: 0.0774 - accuracy: 0.9730 - val_loss: 0.3858 - val_accuracy: 0.8963 - 20s/epoch - 51ms/step\n",
      "Epoch 108/200\n",
      "390/390 - 20s - loss: 0.0785 - accuracy: 0.9733 - val_loss: 0.3784 - val_accuracy: 0.8976 - 20s/epoch - 51ms/step\n",
      "Epoch 109/200\n",
      "390/390 - 21s - loss: 0.0720 - accuracy: 0.9749 - val_loss: 0.3729 - val_accuracy: 0.9007 - 21s/epoch - 53ms/step\n",
      "Epoch 110/200\n",
      "390/390 - 20s - loss: 0.0769 - accuracy: 0.9740 - val_loss: 0.3604 - val_accuracy: 0.8996 - 20s/epoch - 51ms/step\n",
      "Epoch 111/200\n",
      "390/390 - 21s - loss: 0.0732 - accuracy: 0.9750 - val_loss: 0.3757 - val_accuracy: 0.8969 - 21s/epoch - 54ms/step\n",
      "Epoch 112/200\n",
      "390/390 - 21s - loss: 0.0732 - accuracy: 0.9744 - val_loss: 0.5437 - val_accuracy: 0.8692 - 21s/epoch - 54ms/step\n",
      "Epoch 113/200\n",
      "390/390 - 20s - loss: 0.0726 - accuracy: 0.9743 - val_loss: 0.4788 - val_accuracy: 0.8775 - 20s/epoch - 51ms/step\n",
      "Epoch 114/200\n",
      "390/390 - 20s - loss: 0.0757 - accuracy: 0.9745 - val_loss: 0.4073 - val_accuracy: 0.8934 - 20s/epoch - 50ms/step\n",
      "Epoch 115/200\n",
      "390/390 - 20s - loss: 0.0735 - accuracy: 0.9747 - val_loss: 0.4357 - val_accuracy: 0.8831 - 20s/epoch - 51ms/step\n",
      "Epoch 116/200\n",
      "390/390 - 20s - loss: 0.0734 - accuracy: 0.9743 - val_loss: 0.3952 - val_accuracy: 0.8991 - 20s/epoch - 51ms/step\n",
      "Epoch 117/200\n",
      "390/390 - 21s - loss: 0.0770 - accuracy: 0.9731 - val_loss: 0.4939 - val_accuracy: 0.8747 - 21s/epoch - 54ms/step\n",
      "Epoch 118/200\n",
      "390/390 - 21s - loss: 0.0703 - accuracy: 0.9763 - val_loss: 0.2979 - val_accuracy: 0.9171 - 21s/epoch - 54ms/step\n",
      "Epoch 119/200\n",
      "390/390 - 21s - loss: 0.0667 - accuracy: 0.9770 - val_loss: 0.4386 - val_accuracy: 0.8895 - 21s/epoch - 54ms/step\n",
      "Epoch 120/200\n",
      "390/390 - 21s - loss: 0.0732 - accuracy: 0.9749 - val_loss: 0.3191 - val_accuracy: 0.9143 - 21s/epoch - 53ms/step\n",
      "Epoch 121/200\n",
      "390/390 - 20s - loss: 0.0676 - accuracy: 0.9765 - val_loss: 0.3839 - val_accuracy: 0.9005 - 20s/epoch - 51ms/step\n",
      "Epoch 122/200\n",
      "390/390 - 20s - loss: 0.0689 - accuracy: 0.9762 - val_loss: 0.4098 - val_accuracy: 0.8932 - 20s/epoch - 50ms/step\n",
      "Epoch 123/200\n",
      "390/390 - 21s - loss: 0.0702 - accuracy: 0.9760 - val_loss: 0.3910 - val_accuracy: 0.8924 - 21s/epoch - 55ms/step\n",
      "Epoch 124/200\n",
      "390/390 - 21s - loss: 0.0684 - accuracy: 0.9764 - val_loss: 0.3209 - val_accuracy: 0.9135 - 21s/epoch - 54ms/step\n",
      "Epoch 125/200\n",
      "390/390 - 21s - loss: 0.0654 - accuracy: 0.9774 - val_loss: 0.3715 - val_accuracy: 0.9008 - 21s/epoch - 55ms/step\n",
      "Epoch 126/200\n",
      "390/390 - 21s - loss: 0.0702 - accuracy: 0.9757 - val_loss: 0.3630 - val_accuracy: 0.9028 - 21s/epoch - 53ms/step\n",
      "Epoch 127/200\n",
      "390/390 - 21s - loss: 0.0630 - accuracy: 0.9780 - val_loss: 0.4245 - val_accuracy: 0.8851 - 21s/epoch - 54ms/step\n",
      "Epoch 128/200\n",
      "390/390 - 22s - loss: 0.0677 - accuracy: 0.9764 - val_loss: 0.3744 - val_accuracy: 0.8958 - 22s/epoch - 56ms/step\n",
      "Epoch 129/200\n",
      "390/390 - 22s - loss: 0.0674 - accuracy: 0.9765 - val_loss: 0.3440 - val_accuracy: 0.9044 - 22s/epoch - 55ms/step\n",
      "Epoch 130/200\n",
      "390/390 - 21s - loss: 0.0638 - accuracy: 0.9782 - val_loss: 0.4017 - val_accuracy: 0.8939 - 21s/epoch - 53ms/step\n",
      "Epoch 131/200\n",
      "390/390 - 20s - loss: 0.0664 - accuracy: 0.9774 - val_loss: 0.3204 - val_accuracy: 0.9141 - 20s/epoch - 51ms/step\n",
      "Epoch 132/200\n",
      "390/390 - 20s - loss: 0.0659 - accuracy: 0.9773 - val_loss: 0.4170 - val_accuracy: 0.8944 - 20s/epoch - 51ms/step\n",
      "Epoch 133/200\n",
      "390/390 - 20s - loss: 0.0621 - accuracy: 0.9786 - val_loss: 0.3434 - val_accuracy: 0.9084 - 20s/epoch - 52ms/step\n",
      "Epoch 134/200\n",
      "390/390 - 20s - loss: 0.0649 - accuracy: 0.9787 - val_loss: 0.2852 - val_accuracy: 0.9227 - 20s/epoch - 52ms/step\n",
      "Epoch 135/200\n",
      "390/390 - 21s - loss: 0.0611 - accuracy: 0.9792 - val_loss: 0.4936 - val_accuracy: 0.8823 - 21s/epoch - 54ms/step\n",
      "Epoch 136/200\n",
      "390/390 - 21s - loss: 0.0611 - accuracy: 0.9793 - val_loss: 0.4910 - val_accuracy: 0.8740 - 21s/epoch - 53ms/step\n",
      "Epoch 137/200\n",
      "390/390 - 20s - loss: 0.0646 - accuracy: 0.9776 - val_loss: 0.3226 - val_accuracy: 0.9103 - 20s/epoch - 51ms/step\n",
      "Epoch 138/200\n",
      "390/390 - 21s - loss: 0.0566 - accuracy: 0.9806 - val_loss: 0.3850 - val_accuracy: 0.8942 - 21s/epoch - 53ms/step\n",
      "Epoch 139/200\n",
      "390/390 - 21s - loss: 0.0576 - accuracy: 0.9800 - val_loss: 0.2975 - val_accuracy: 0.9208 - 21s/epoch - 54ms/step\n",
      "Epoch 140/200\n",
      "390/390 - 21s - loss: 0.0588 - accuracy: 0.9797 - val_loss: 0.3244 - val_accuracy: 0.9122 - 21s/epoch - 53ms/step\n",
      "Epoch 141/200\n",
      "390/390 - 21s - loss: 0.0613 - accuracy: 0.9791 - val_loss: 0.3734 - val_accuracy: 0.8984 - 21s/epoch - 54ms/step\n",
      "Epoch 142/200\n",
      "390/390 - 21s - loss: 0.0597 - accuracy: 0.9792 - val_loss: 0.3786 - val_accuracy: 0.9018 - 21s/epoch - 53ms/step\n",
      "Epoch 143/200\n",
      "390/390 - 20s - loss: 0.0605 - accuracy: 0.9791 - val_loss: 0.4096 - val_accuracy: 0.8920 - 20s/epoch - 51ms/step\n",
      "Epoch 144/200\n",
      "390/390 - 20s - loss: 0.0582 - accuracy: 0.9798 - val_loss: 0.3294 - val_accuracy: 0.9120 - 20s/epoch - 51ms/step\n",
      "Epoch 145/200\n",
      "390/390 - 21s - loss: 0.0606 - accuracy: 0.9793 - val_loss: 0.3412 - val_accuracy: 0.9067 - 21s/epoch - 53ms/step\n",
      "Epoch 146/200\n",
      "390/390 - 20s - loss: 0.0553 - accuracy: 0.9813 - val_loss: 0.3140 - val_accuracy: 0.9137 - 20s/epoch - 51ms/step\n",
      "Epoch 147/200\n",
      "390/390 - 21s - loss: 0.0531 - accuracy: 0.9823 - val_loss: 0.3652 - val_accuracy: 0.9054 - 21s/epoch - 54ms/step\n",
      "Epoch 148/200\n",
      "390/390 - 21s - loss: 0.0559 - accuracy: 0.9803 - val_loss: 0.3181 - val_accuracy: 0.9138 - 21s/epoch - 53ms/step\n",
      "Epoch 149/200\n",
      "390/390 - 20s - loss: 0.0558 - accuracy: 0.9808 - val_loss: 0.2981 - val_accuracy: 0.9173 - 20s/epoch - 51ms/step\n",
      "Epoch 150/200\n",
      "390/390 - 21s - loss: 0.0509 - accuracy: 0.9830 - val_loss: 0.3610 - val_accuracy: 0.9071 - 21s/epoch - 53ms/step\n",
      "Epoch 151/200\n",
      "390/390 - 21s - loss: 0.0508 - accuracy: 0.9822 - val_loss: 0.3434 - val_accuracy: 0.9072 - 21s/epoch - 53ms/step\n",
      "Epoch 152/200\n",
      "390/390 - 21s - loss: 0.0541 - accuracy: 0.9813 - val_loss: 0.3544 - val_accuracy: 0.9029 - 21s/epoch - 53ms/step\n",
      "Epoch 153/200\n",
      "390/390 - 21s - loss: 0.0522 - accuracy: 0.9823 - val_loss: 0.3378 - val_accuracy: 0.9087 - 21s/epoch - 54ms/step\n",
      "Epoch 154/200\n",
      "390/390 - 21s - loss: 0.0500 - accuracy: 0.9830 - val_loss: 0.3912 - val_accuracy: 0.8978 - 21s/epoch - 54ms/step\n",
      "Epoch 155/200\n",
      "390/390 - 21s - loss: 0.0498 - accuracy: 0.9832 - val_loss: 0.3064 - val_accuracy: 0.9189 - 21s/epoch - 53ms/step\n",
      "Epoch 156/200\n",
      "390/390 - 21s - loss: 0.0477 - accuracy: 0.9834 - val_loss: 0.3016 - val_accuracy: 0.9208 - 21s/epoch - 53ms/step\n",
      "Epoch 157/200\n",
      "390/390 - 21s - loss: 0.0444 - accuracy: 0.9845 - val_loss: 0.3594 - val_accuracy: 0.9045 - 21s/epoch - 53ms/step\n",
      "Epoch 158/200\n",
      "390/390 - 20s - loss: 0.0493 - accuracy: 0.9836 - val_loss: 0.3666 - val_accuracy: 0.9041 - 20s/epoch - 51ms/step\n",
      "Epoch 159/200\n",
      "390/390 - 20s - loss: 0.0473 - accuracy: 0.9839 - val_loss: 0.3155 - val_accuracy: 0.9152 - 20s/epoch - 51ms/step\n",
      "Epoch 160/200\n",
      "390/390 - 20s - loss: 0.0437 - accuracy: 0.9853 - val_loss: 0.2910 - val_accuracy: 0.9190 - 20s/epoch - 51ms/step\n",
      "Epoch 161/200\n",
      "390/390 - 20s - loss: 0.0463 - accuracy: 0.9845 - val_loss: 0.3795 - val_accuracy: 0.8981 - 20s/epoch - 51ms/step\n",
      "Epoch 162/200\n",
      "390/390 - 21s - loss: 0.0383 - accuracy: 0.9874 - val_loss: 0.3197 - val_accuracy: 0.9113 - 21s/epoch - 53ms/step\n",
      "Epoch 163/200\n",
      "390/390 - 21s - loss: 0.0411 - accuracy: 0.9859 - val_loss: 0.3143 - val_accuracy: 0.9140 - 21s/epoch - 54ms/step\n",
      "Epoch 164/200\n",
      "390/390 - 21s - loss: 0.0410 - accuracy: 0.9863 - val_loss: 0.3319 - val_accuracy: 0.9119 - 21s/epoch - 54ms/step\n",
      "Epoch 165/200\n",
      "390/390 - 21s - loss: 0.0406 - accuracy: 0.9865 - val_loss: 0.3024 - val_accuracy: 0.9156 - 21s/epoch - 54ms/step\n",
      "Epoch 166/200\n",
      "390/390 - 21s - loss: 0.0349 - accuracy: 0.9884 - val_loss: 0.3000 - val_accuracy: 0.9234 - 21s/epoch - 53ms/step\n",
      "Epoch 167/200\n",
      "390/390 - 21s - loss: 0.0390 - accuracy: 0.9871 - val_loss: 0.3106 - val_accuracy: 0.9152 - 21s/epoch - 54ms/step\n",
      "Epoch 168/200\n",
      "390/390 - 21s - loss: 0.0354 - accuracy: 0.9885 - val_loss: 0.3548 - val_accuracy: 0.9052 - 21s/epoch - 53ms/step\n",
      "Epoch 169/200\n",
      "390/390 - 22s - loss: 0.0332 - accuracy: 0.9896 - val_loss: 0.3260 - val_accuracy: 0.9123 - 22s/epoch - 55ms/step\n",
      "Epoch 170/200\n",
      "390/390 - 21s - loss: 0.0306 - accuracy: 0.9901 - val_loss: 0.3206 - val_accuracy: 0.9137 - 21s/epoch - 54ms/step\n",
      "Epoch 171/200\n",
      "390/390 - 21s - loss: 0.0355 - accuracy: 0.9886 - val_loss: 0.2788 - val_accuracy: 0.9262 - 21s/epoch - 55ms/step\n",
      "Epoch 172/200\n",
      "390/390 - 21s - loss: 0.0297 - accuracy: 0.9903 - val_loss: 0.3649 - val_accuracy: 0.9043 - 21s/epoch - 53ms/step\n",
      "Epoch 173/200\n",
      "390/390 - 21s - loss: 0.0300 - accuracy: 0.9900 - val_loss: 0.2931 - val_accuracy: 0.9235 - 21s/epoch - 53ms/step\n",
      "Epoch 174/200\n",
      "390/390 - 20s - loss: 0.0268 - accuracy: 0.9915 - val_loss: 0.2717 - val_accuracy: 0.9267 - 20s/epoch - 51ms/step\n",
      "Epoch 175/200\n",
      "390/390 - 21s - loss: 0.0236 - accuracy: 0.9924 - val_loss: 0.2628 - val_accuracy: 0.9276 - 21s/epoch - 53ms/step\n",
      "Epoch 176/200\n",
      "390/390 - 20s - loss: 0.0232 - accuracy: 0.9930 - val_loss: 0.2614 - val_accuracy: 0.9298 - 20s/epoch - 51ms/step\n",
      "Epoch 177/200\n",
      "390/390 - 20s - loss: 0.0208 - accuracy: 0.9936 - val_loss: 0.3019 - val_accuracy: 0.9210 - 20s/epoch - 51ms/step\n",
      "Epoch 178/200\n",
      "390/390 - 21s - loss: 0.0235 - accuracy: 0.9928 - val_loss: 0.2743 - val_accuracy: 0.9240 - 21s/epoch - 54ms/step\n",
      "Epoch 179/200\n",
      "390/390 - 20s - loss: 0.0222 - accuracy: 0.9931 - val_loss: 0.2485 - val_accuracy: 0.9306 - 20s/epoch - 51ms/step\n",
      "Epoch 180/200\n",
      "390/390 - 21s - loss: 0.0188 - accuracy: 0.9945 - val_loss: 0.2414 - val_accuracy: 0.9319 - 21s/epoch - 53ms/step\n",
      "Epoch 181/200\n",
      "390/390 - 20s - loss: 0.0179 - accuracy: 0.9947 - val_loss: 0.2415 - val_accuracy: 0.9328 - 20s/epoch - 51ms/step\n",
      "Epoch 182/200\n",
      "390/390 - 20s - loss: 0.0145 - accuracy: 0.9959 - val_loss: 0.2401 - val_accuracy: 0.9347 - 20s/epoch - 51ms/step\n",
      "Epoch 183/200\n",
      "390/390 - 21s - loss: 0.0129 - accuracy: 0.9969 - val_loss: 0.2308 - val_accuracy: 0.9378 - 21s/epoch - 54ms/step\n",
      "Epoch 184/200\n",
      "390/390 - 21s - loss: 0.0121 - accuracy: 0.9970 - val_loss: 0.2192 - val_accuracy: 0.9393 - 21s/epoch - 53ms/step\n",
      "Epoch 185/200\n",
      "390/390 - 20s - loss: 0.0111 - accuracy: 0.9974 - val_loss: 0.2293 - val_accuracy: 0.9369 - 20s/epoch - 51ms/step\n",
      "Epoch 186/200\n",
      "390/390 - 21s - loss: 0.0105 - accuracy: 0.9975 - val_loss: 0.2074 - val_accuracy: 0.9416 - 21s/epoch - 54ms/step\n",
      "Epoch 187/200\n",
      "390/390 - 21s - loss: 0.0095 - accuracy: 0.9982 - val_loss: 0.2112 - val_accuracy: 0.9408 - 21s/epoch - 53ms/step\n",
      "Epoch 188/200\n",
      "390/390 - 20s - loss: 0.0086 - accuracy: 0.9983 - val_loss: 0.2025 - val_accuracy: 0.9413 - 20s/epoch - 51ms/step\n",
      "Epoch 189/200\n",
      "390/390 - 20s - loss: 0.0081 - accuracy: 0.9989 - val_loss: 0.2038 - val_accuracy: 0.9419 - 20s/epoch - 51ms/step\n",
      "Epoch 190/200\n",
      "390/390 - 21s - loss: 0.0082 - accuracy: 0.9986 - val_loss: 0.1951 - val_accuracy: 0.9441 - 21s/epoch - 54ms/step\n",
      "Epoch 191/200\n",
      "390/390 - 20s - loss: 0.0090 - accuracy: 0.9987 - val_loss: 0.1923 - val_accuracy: 0.9447 - 20s/epoch - 51ms/step\n",
      "Epoch 192/200\n",
      "390/390 - 21s - loss: 0.0081 - accuracy: 0.9992 - val_loss: 0.1906 - val_accuracy: 0.9443 - 21s/epoch - 53ms/step\n",
      "Epoch 193/200\n",
      "390/390 - 21s - loss: 0.0088 - accuracy: 0.9990 - val_loss: 0.1866 - val_accuracy: 0.9440 - 21s/epoch - 54ms/step\n",
      "Epoch 194/200\n",
      "390/390 - 20s - loss: 0.0093 - accuracy: 0.9993 - val_loss: 0.1878 - val_accuracy: 0.9442 - 20s/epoch - 51ms/step\n",
      "Epoch 195/200\n",
      "390/390 - 21s - loss: 0.0100 - accuracy: 0.9994 - val_loss: 0.1818 - val_accuracy: 0.9444 - 21s/epoch - 54ms/step\n",
      "Epoch 196/200\n",
      "390/390 - 21s - loss: 0.0117 - accuracy: 0.9995 - val_loss: 0.1791 - val_accuracy: 0.9446 - 21s/epoch - 55ms/step\n",
      "Epoch 197/200\n",
      "390/390 - 20s - loss: 0.0138 - accuracy: 0.9993 - val_loss: 0.1760 - val_accuracy: 0.9451 - 20s/epoch - 51ms/step\n",
      "Epoch 198/200\n",
      "390/390 - 21s - loss: 0.0164 - accuracy: 0.9994 - val_loss: 0.1780 - val_accuracy: 0.9453 - 21s/epoch - 53ms/step\n",
      "Epoch 199/200\n",
      "390/390 - 21s - loss: 0.0206 - accuracy: 0.9993 - val_loss: 0.1768 - val_accuracy: 0.9457 - 21s/epoch - 53ms/step\n",
      "Epoch 200/200\n",
      "390/390 - 21s - loss: 0.0269 - accuracy: 0.9992 - val_loss: 0.1811 - val_accuracy: 0.9449 - 21s/epoch - 53ms/step\n",
      " Using best val_acc=0.9457 from last 20 epochs\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.1768 - accuracy: 0.9457\n",
      " Finished: Lam=0.7, Repeat=1, Acc=0.9457\n",
      "\n",
      " lambda: Lam=0.7, Repeat=2/3\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "321009422 561714176\n",
      "Epoch 1/200\n",
      "390/390 - 22s - loss: 0.7080 - accuracy: 0.7683 - val_loss: 4.7097 - val_accuracy: 0.2033 - 22s/epoch - 56ms/step\n",
      "Epoch 2/200\n",
      "390/390 - 20s - loss: 0.6025 - accuracy: 0.7951 - val_loss: 0.9315 - val_accuracy: 0.7077 - 20s/epoch - 51ms/step\n",
      "Epoch 3/200\n",
      "390/390 - 20s - loss: 0.5058 - accuracy: 0.8293 - val_loss: 1.6737 - val_accuracy: 0.5590 - 20s/epoch - 51ms/step\n",
      "Epoch 4/200\n",
      "390/390 - 21s - loss: 0.4472 - accuracy: 0.8469 - val_loss: 0.7538 - val_accuracy: 0.7733 - 21s/epoch - 53ms/step\n",
      "Epoch 5/200\n",
      "390/390 - 20s - loss: 0.4053 - accuracy: 0.8632 - val_loss: 0.6877 - val_accuracy: 0.8000 - 20s/epoch - 51ms/step\n",
      "Epoch 6/200\n",
      "390/390 - 20s - loss: 0.3597 - accuracy: 0.8768 - val_loss: 0.5497 - val_accuracy: 0.8199 - 20s/epoch - 53ms/step\n",
      "Epoch 7/200\n",
      "390/390 - 20s - loss: 0.3122 - accuracy: 0.8917 - val_loss: 0.6500 - val_accuracy: 0.8020 - 20s/epoch - 51ms/step\n",
      "Epoch 8/200\n",
      "390/390 - 20s - loss: 0.2841 - accuracy: 0.9025 - val_loss: 0.4032 - val_accuracy: 0.8649 - 20s/epoch - 51ms/step\n",
      "Epoch 9/200\n",
      "390/390 - 20s - loss: 0.2586 - accuracy: 0.9098 - val_loss: 0.4705 - val_accuracy: 0.8572 - 20s/epoch - 51ms/step\n",
      "Epoch 10/200\n",
      "390/390 - 20s - loss: 0.2417 - accuracy: 0.9161 - val_loss: 0.3806 - val_accuracy: 0.8786 - 20s/epoch - 51ms/step\n",
      "Epoch 11/200\n",
      "390/390 - 20s - loss: 0.2275 - accuracy: 0.9211 - val_loss: 0.4586 - val_accuracy: 0.8545 - 20s/epoch - 51ms/step\n",
      "Epoch 12/200\n",
      "390/390 - 21s - loss: 0.2167 - accuracy: 0.9250 - val_loss: 0.3107 - val_accuracy: 0.9007 - 21s/epoch - 54ms/step\n",
      "Epoch 13/200\n",
      "390/390 - 20s - loss: 0.1995 - accuracy: 0.9312 - val_loss: 0.5378 - val_accuracy: 0.8400 - 20s/epoch - 51ms/step\n",
      "Epoch 14/200\n",
      "390/390 - 20s - loss: 0.1926 - accuracy: 0.9323 - val_loss: 0.4241 - val_accuracy: 0.8737 - 20s/epoch - 51ms/step\n",
      "Epoch 15/200\n",
      "390/390 - 21s - loss: 0.1856 - accuracy: 0.9352 - val_loss: 0.3927 - val_accuracy: 0.8788 - 21s/epoch - 53ms/step\n",
      "Epoch 16/200\n",
      "390/390 - 21s - loss: 0.1792 - accuracy: 0.9372 - val_loss: 0.3559 - val_accuracy: 0.8922 - 21s/epoch - 54ms/step\n",
      "Epoch 17/200\n",
      "390/390 - 21s - loss: 0.1743 - accuracy: 0.9401 - val_loss: 0.3339 - val_accuracy: 0.8909 - 21s/epoch - 53ms/step\n",
      "Epoch 18/200\n",
      "390/390 - 20s - loss: 0.1679 - accuracy: 0.9413 - val_loss: 0.3415 - val_accuracy: 0.8941 - 20s/epoch - 51ms/step\n",
      "Epoch 19/200\n",
      "390/390 - 21s - loss: 0.1647 - accuracy: 0.9427 - val_loss: 0.3706 - val_accuracy: 0.8814 - 21s/epoch - 54ms/step\n",
      "Epoch 20/200\n",
      "390/390 - 20s - loss: 0.1601 - accuracy: 0.9447 - val_loss: 0.4549 - val_accuracy: 0.8585 - 20s/epoch - 51ms/step\n",
      "Epoch 21/200\n",
      "390/390 - 21s - loss: 0.1517 - accuracy: 0.9463 - val_loss: 0.3972 - val_accuracy: 0.8788 - 21s/epoch - 53ms/step\n",
      "Epoch 22/200\n",
      "390/390 - 21s - loss: 0.1509 - accuracy: 0.9469 - val_loss: 0.3947 - val_accuracy: 0.8834 - 21s/epoch - 53ms/step\n",
      "Epoch 23/200\n",
      "390/390 - 20s - loss: 0.1467 - accuracy: 0.9480 - val_loss: 0.4004 - val_accuracy: 0.8817 - 20s/epoch - 51ms/step\n",
      "Epoch 24/200\n",
      "390/390 - 21s - loss: 0.1416 - accuracy: 0.9510 - val_loss: 0.3722 - val_accuracy: 0.8878 - 21s/epoch - 53ms/step\n",
      "Epoch 25/200\n",
      "390/390 - 22s - loss: 0.1400 - accuracy: 0.9511 - val_loss: 0.3825 - val_accuracy: 0.8867 - 22s/epoch - 55ms/step\n",
      "Epoch 26/200\n",
      "390/390 - 21s - loss: 0.1366 - accuracy: 0.9511 - val_loss: 0.4448 - val_accuracy: 0.8680 - 21s/epoch - 53ms/step\n",
      "Epoch 27/200\n",
      "390/390 - 20s - loss: 0.1355 - accuracy: 0.9533 - val_loss: 0.3397 - val_accuracy: 0.9036 - 20s/epoch - 51ms/step\n",
      "Epoch 28/200\n",
      "390/390 - 21s - loss: 0.1360 - accuracy: 0.9524 - val_loss: 0.3697 - val_accuracy: 0.8862 - 21s/epoch - 53ms/step\n",
      "Epoch 29/200\n",
      "390/390 - 20s - loss: 0.1290 - accuracy: 0.9543 - val_loss: 0.3832 - val_accuracy: 0.8880 - 20s/epoch - 51ms/step\n",
      "Epoch 30/200\n",
      "390/390 - 20s - loss: 0.1266 - accuracy: 0.9564 - val_loss: 0.3646 - val_accuracy: 0.8918 - 20s/epoch - 51ms/step\n",
      "Epoch 31/200\n",
      "390/390 - 21s - loss: 0.1299 - accuracy: 0.9543 - val_loss: 0.3617 - val_accuracy: 0.8927 - 21s/epoch - 54ms/step\n",
      "Epoch 32/200\n",
      "390/390 - 20s - loss: 0.1263 - accuracy: 0.9568 - val_loss: 0.3806 - val_accuracy: 0.8878 - 20s/epoch - 52ms/step\n",
      "Epoch 33/200\n",
      "390/390 - 21s - loss: 0.1222 - accuracy: 0.9564 - val_loss: 0.3494 - val_accuracy: 0.8987 - 21s/epoch - 53ms/step\n",
      "Epoch 34/200\n",
      "390/390 - 21s - loss: 0.1220 - accuracy: 0.9574 - val_loss: 0.3647 - val_accuracy: 0.8914 - 21s/epoch - 55ms/step\n",
      "Epoch 35/200\n",
      "390/390 - 20s - loss: 0.1219 - accuracy: 0.9571 - val_loss: 0.3873 - val_accuracy: 0.8851 - 20s/epoch - 51ms/step\n",
      "Epoch 36/200\n",
      "390/390 - 22s - loss: 0.1148 - accuracy: 0.9598 - val_loss: 0.4471 - val_accuracy: 0.8720 - 22s/epoch - 55ms/step\n",
      "Epoch 37/200\n",
      "390/390 - 20s - loss: 0.1149 - accuracy: 0.9590 - val_loss: 0.3702 - val_accuracy: 0.8921 - 20s/epoch - 52ms/step\n",
      "Epoch 38/200\n",
      "390/390 - 20s - loss: 0.1182 - accuracy: 0.9586 - val_loss: 0.3576 - val_accuracy: 0.8978 - 20s/epoch - 51ms/step\n",
      "Epoch 39/200\n",
      "390/390 - 20s - loss: 0.1149 - accuracy: 0.9593 - val_loss: 0.3733 - val_accuracy: 0.8922 - 20s/epoch - 51ms/step\n",
      "Epoch 40/200\n",
      "390/390 - 21s - loss: 0.1114 - accuracy: 0.9605 - val_loss: 0.3902 - val_accuracy: 0.8887 - 21s/epoch - 54ms/step\n",
      "Epoch 41/200\n",
      "390/390 - 21s - loss: 0.1157 - accuracy: 0.9593 - val_loss: 0.4169 - val_accuracy: 0.8837 - 21s/epoch - 53ms/step\n",
      "Epoch 42/200\n",
      "390/390 - 21s - loss: 0.1133 - accuracy: 0.9607 - val_loss: 0.3847 - val_accuracy: 0.8906 - 21s/epoch - 54ms/step\n",
      "Epoch 43/200\n",
      "390/390 - 21s - loss: 0.1110 - accuracy: 0.9619 - val_loss: 0.3589 - val_accuracy: 0.8960 - 21s/epoch - 53ms/step\n",
      "Epoch 44/200\n",
      "390/390 - 20s - loss: 0.1067 - accuracy: 0.9620 - val_loss: 0.4018 - val_accuracy: 0.8833 - 20s/epoch - 51ms/step\n",
      "Epoch 45/200\n",
      "390/390 - 21s - loss: 0.1116 - accuracy: 0.9610 - val_loss: 0.3933 - val_accuracy: 0.8893 - 21s/epoch - 53ms/step\n",
      "Epoch 46/200\n",
      "390/390 - 21s - loss: 0.1085 - accuracy: 0.9618 - val_loss: 0.3596 - val_accuracy: 0.9009 - 21s/epoch - 53ms/step\n",
      "Epoch 47/200\n",
      "390/390 - 21s - loss: 0.1067 - accuracy: 0.9629 - val_loss: 0.4217 - val_accuracy: 0.8821 - 21s/epoch - 54ms/step\n",
      "Epoch 48/200\n",
      "390/390 - 20s - loss: 0.1071 - accuracy: 0.9629 - val_loss: 0.3916 - val_accuracy: 0.8922 - 20s/epoch - 51ms/step\n",
      "Epoch 49/200\n",
      "390/390 - 21s - loss: 0.1070 - accuracy: 0.9635 - val_loss: 0.3920 - val_accuracy: 0.8858 - 21s/epoch - 53ms/step\n",
      "Epoch 50/200\n",
      "390/390 - 21s - loss: 0.1065 - accuracy: 0.9629 - val_loss: 0.4717 - val_accuracy: 0.8775 - 21s/epoch - 55ms/step\n",
      "Epoch 51/200\n",
      "390/390 - 20s - loss: 0.1047 - accuracy: 0.9645 - val_loss: 0.3967 - val_accuracy: 0.8858 - 20s/epoch - 52ms/step\n",
      "Epoch 52/200\n",
      "390/390 - 20s - loss: 0.1024 - accuracy: 0.9637 - val_loss: 0.3564 - val_accuracy: 0.8974 - 20s/epoch - 51ms/step\n",
      "Epoch 53/200\n",
      "390/390 - 20s - loss: 0.1034 - accuracy: 0.9639 - val_loss: 0.4220 - val_accuracy: 0.8866 - 20s/epoch - 51ms/step\n",
      "Epoch 54/200\n",
      "390/390 - 21s - loss: 0.1019 - accuracy: 0.9647 - val_loss: 0.4131 - val_accuracy: 0.8854 - 21s/epoch - 53ms/step\n",
      "Epoch 55/200\n",
      "390/390 - 21s - loss: 0.1010 - accuracy: 0.9647 - val_loss: 0.3543 - val_accuracy: 0.9014 - 21s/epoch - 55ms/step\n",
      "Epoch 56/200\n",
      "390/390 - 20s - loss: 0.0986 - accuracy: 0.9658 - val_loss: 0.3678 - val_accuracy: 0.8950 - 20s/epoch - 52ms/step\n",
      "Epoch 57/200\n",
      "390/390 - 21s - loss: 0.1002 - accuracy: 0.9650 - val_loss: 0.4890 - val_accuracy: 0.8702 - 21s/epoch - 53ms/step\n",
      "Epoch 58/200\n",
      "390/390 - 20s - loss: 0.0989 - accuracy: 0.9656 - val_loss: 0.3516 - val_accuracy: 0.9000 - 20s/epoch - 51ms/step\n",
      "Epoch 59/200\n",
      "390/390 - 20s - loss: 0.1026 - accuracy: 0.9642 - val_loss: 0.3597 - val_accuracy: 0.9006 - 20s/epoch - 51ms/step\n",
      "Epoch 60/200\n",
      "390/390 - 21s - loss: 0.0945 - accuracy: 0.9663 - val_loss: 0.4034 - val_accuracy: 0.8875 - 21s/epoch - 54ms/step\n",
      "Epoch 61/200\n",
      "390/390 - 22s - loss: 0.0975 - accuracy: 0.9660 - val_loss: 0.3918 - val_accuracy: 0.8959 - 22s/epoch - 55ms/step\n",
      "Epoch 62/200\n",
      "390/390 - 21s - loss: 0.0958 - accuracy: 0.9660 - val_loss: 0.3949 - val_accuracy: 0.8848 - 21s/epoch - 55ms/step\n",
      "Epoch 63/200\n",
      "390/390 - 21s - loss: 0.0958 - accuracy: 0.9664 - val_loss: 0.3497 - val_accuracy: 0.9040 - 21s/epoch - 54ms/step\n",
      "Epoch 64/200\n",
      "390/390 - 20s - loss: 0.0963 - accuracy: 0.9670 - val_loss: 0.3516 - val_accuracy: 0.8964 - 20s/epoch - 51ms/step\n",
      "Epoch 65/200\n",
      "390/390 - 20s - loss: 0.0957 - accuracy: 0.9671 - val_loss: 0.3726 - val_accuracy: 0.8979 - 20s/epoch - 51ms/step\n",
      "Epoch 66/200\n",
      "390/390 - 20s - loss: 0.0983 - accuracy: 0.9659 - val_loss: 0.3871 - val_accuracy: 0.8892 - 20s/epoch - 50ms/step\n",
      "Epoch 67/200\n",
      "390/390 - 20s - loss: 0.0933 - accuracy: 0.9671 - val_loss: 0.3663 - val_accuracy: 0.8929 - 20s/epoch - 51ms/step\n",
      "Epoch 68/200\n",
      "390/390 - 20s - loss: 0.0964 - accuracy: 0.9661 - val_loss: 0.4010 - val_accuracy: 0.8885 - 20s/epoch - 51ms/step\n",
      "Epoch 69/200\n",
      "390/390 - 20s - loss: 0.0918 - accuracy: 0.9684 - val_loss: 0.3680 - val_accuracy: 0.8988 - 20s/epoch - 51ms/step\n",
      "Epoch 70/200\n",
      "390/390 - 21s - loss: 0.0945 - accuracy: 0.9669 - val_loss: 0.4975 - val_accuracy: 0.8708 - 21s/epoch - 53ms/step\n",
      "Epoch 71/200\n",
      "390/390 - 20s - loss: 0.0983 - accuracy: 0.9656 - val_loss: 0.4094 - val_accuracy: 0.8837 - 20s/epoch - 51ms/step\n",
      "Epoch 72/200\n",
      "390/390 - 20s - loss: 0.0904 - accuracy: 0.9689 - val_loss: 0.5085 - val_accuracy: 0.8641 - 20s/epoch - 51ms/step\n",
      "Epoch 73/200\n",
      "390/390 - 21s - loss: 0.0876 - accuracy: 0.9693 - val_loss: 0.3381 - val_accuracy: 0.9051 - 21s/epoch - 54ms/step\n",
      "Epoch 74/200\n",
      "390/390 - 20s - loss: 0.0899 - accuracy: 0.9692 - val_loss: 0.3739 - val_accuracy: 0.8977 - 20s/epoch - 50ms/step\n",
      "Epoch 75/200\n",
      "390/390 - 20s - loss: 0.0886 - accuracy: 0.9697 - val_loss: 0.4330 - val_accuracy: 0.8860 - 20s/epoch - 51ms/step\n",
      "Epoch 76/200\n",
      "390/390 - 21s - loss: 0.0945 - accuracy: 0.9670 - val_loss: 0.4910 - val_accuracy: 0.8720 - 21s/epoch - 53ms/step\n",
      "Epoch 77/200\n",
      "390/390 - 21s - loss: 0.0895 - accuracy: 0.9684 - val_loss: 0.4043 - val_accuracy: 0.8889 - 21s/epoch - 54ms/step\n",
      "Epoch 78/200\n",
      "390/390 - 21s - loss: 0.0876 - accuracy: 0.9691 - val_loss: 0.3961 - val_accuracy: 0.8916 - 21s/epoch - 54ms/step\n",
      "Epoch 79/200\n",
      "390/390 - 22s - loss: 0.0906 - accuracy: 0.9677 - val_loss: 0.3336 - val_accuracy: 0.9037 - 22s/epoch - 55ms/step\n",
      "Epoch 80/200\n",
      "390/390 - 20s - loss: 0.0837 - accuracy: 0.9704 - val_loss: 0.3983 - val_accuracy: 0.8897 - 20s/epoch - 51ms/step\n",
      "Epoch 81/200\n",
      "390/390 - 21s - loss: 0.0855 - accuracy: 0.9702 - val_loss: 0.3517 - val_accuracy: 0.9067 - 21s/epoch - 53ms/step\n",
      "Epoch 82/200\n",
      "390/390 - 21s - loss: 0.0867 - accuracy: 0.9690 - val_loss: 0.3446 - val_accuracy: 0.9033 - 21s/epoch - 55ms/step\n",
      "Epoch 83/200\n",
      "390/390 - 21s - loss: 0.0905 - accuracy: 0.9686 - val_loss: 0.3914 - val_accuracy: 0.8909 - 21s/epoch - 53ms/step\n",
      "Epoch 84/200\n",
      "390/390 - 21s - loss: 0.0829 - accuracy: 0.9716 - val_loss: 0.3776 - val_accuracy: 0.8995 - 21s/epoch - 53ms/step\n",
      "Epoch 85/200\n",
      "390/390 - 21s - loss: 0.0868 - accuracy: 0.9700 - val_loss: 0.3366 - val_accuracy: 0.9069 - 21s/epoch - 54ms/step\n",
      "Epoch 86/200\n",
      "390/390 - 20s - loss: 0.0847 - accuracy: 0.9708 - val_loss: 0.3472 - val_accuracy: 0.9008 - 20s/epoch - 51ms/step\n",
      "Epoch 87/200\n",
      "390/390 - 21s - loss: 0.0848 - accuracy: 0.9709 - val_loss: 0.3637 - val_accuracy: 0.8995 - 21s/epoch - 53ms/step\n",
      "Epoch 88/200\n",
      "390/390 - 21s - loss: 0.0847 - accuracy: 0.9703 - val_loss: 0.4472 - val_accuracy: 0.8794 - 21s/epoch - 53ms/step\n",
      "Epoch 89/200\n",
      "390/390 - 22s - loss: 0.0888 - accuracy: 0.9698 - val_loss: 0.3472 - val_accuracy: 0.8994 - 22s/epoch - 56ms/step\n",
      "Epoch 90/200\n",
      "390/390 - 21s - loss: 0.0800 - accuracy: 0.9724 - val_loss: 0.3539 - val_accuracy: 0.9021 - 21s/epoch - 54ms/step\n",
      "Epoch 91/200\n",
      "390/390 - 21s - loss: 0.0791 - accuracy: 0.9729 - val_loss: 0.3540 - val_accuracy: 0.9042 - 21s/epoch - 53ms/step\n",
      "Epoch 92/200\n",
      "390/390 - 20s - loss: 0.0799 - accuracy: 0.9720 - val_loss: 0.5131 - val_accuracy: 0.8712 - 20s/epoch - 51ms/step\n",
      "Epoch 93/200\n",
      "390/390 - 21s - loss: 0.0838 - accuracy: 0.9710 - val_loss: 0.4256 - val_accuracy: 0.8818 - 21s/epoch - 53ms/step\n",
      "Epoch 94/200\n",
      "390/390 - 20s - loss: 0.0831 - accuracy: 0.9714 - val_loss: 0.4570 - val_accuracy: 0.8804 - 20s/epoch - 51ms/step\n",
      "Epoch 95/200\n",
      "390/390 - 21s - loss: 0.0821 - accuracy: 0.9721 - val_loss: 0.3663 - val_accuracy: 0.8989 - 21s/epoch - 53ms/step\n",
      "Epoch 96/200\n",
      "390/390 - 20s - loss: 0.0811 - accuracy: 0.9719 - val_loss: 0.3367 - val_accuracy: 0.9066 - 20s/epoch - 51ms/step\n",
      "Epoch 97/200\n",
      "390/390 - 21s - loss: 0.0808 - accuracy: 0.9732 - val_loss: 0.3782 - val_accuracy: 0.8945 - 21s/epoch - 53ms/step\n",
      "Epoch 98/200\n",
      "390/390 - 20s - loss: 0.0810 - accuracy: 0.9714 - val_loss: 0.4086 - val_accuracy: 0.8946 - 20s/epoch - 51ms/step\n",
      "Epoch 99/200\n",
      "390/390 - 21s - loss: 0.0801 - accuracy: 0.9727 - val_loss: 0.3529 - val_accuracy: 0.9036 - 21s/epoch - 54ms/step\n",
      "Epoch 100/200\n",
      "390/390 - 20s - loss: 0.0790 - accuracy: 0.9732 - val_loss: 0.3839 - val_accuracy: 0.8961 - 20s/epoch - 51ms/step\n",
      "Epoch 101/200\n",
      "390/390 - 20s - loss: 0.0783 - accuracy: 0.9728 - val_loss: 0.3479 - val_accuracy: 0.9007 - 20s/epoch - 51ms/step\n",
      "Epoch 102/200\n",
      "390/390 - 21s - loss: 0.0785 - accuracy: 0.9724 - val_loss: 0.3528 - val_accuracy: 0.9041 - 21s/epoch - 53ms/step\n",
      "Epoch 103/200\n",
      "390/390 - 20s - loss: 0.0796 - accuracy: 0.9719 - val_loss: 0.4061 - val_accuracy: 0.8881 - 20s/epoch - 53ms/step\n",
      "Epoch 104/200\n",
      "390/390 - 21s - loss: 0.0805 - accuracy: 0.9721 - val_loss: 0.3110 - val_accuracy: 0.9141 - 21s/epoch - 54ms/step\n",
      "Epoch 105/200\n",
      "390/390 - 21s - loss: 0.0777 - accuracy: 0.9732 - val_loss: 0.3416 - val_accuracy: 0.9049 - 21s/epoch - 53ms/step\n",
      "Epoch 106/200\n",
      "390/390 - 20s - loss: 0.0783 - accuracy: 0.9724 - val_loss: 0.3761 - val_accuracy: 0.8939 - 20s/epoch - 52ms/step\n",
      "Epoch 107/200\n",
      "390/390 - 20s - loss: 0.0768 - accuracy: 0.9736 - val_loss: 0.3227 - val_accuracy: 0.9100 - 20s/epoch - 52ms/step\n",
      "Epoch 108/200\n",
      "390/390 - 20s - loss: 0.0738 - accuracy: 0.9750 - val_loss: 0.3322 - val_accuracy: 0.9069 - 20s/epoch - 51ms/step\n",
      "Epoch 109/200\n",
      "390/390 - 21s - loss: 0.0766 - accuracy: 0.9733 - val_loss: 0.3523 - val_accuracy: 0.9062 - 21s/epoch - 53ms/step\n",
      "Epoch 110/200\n",
      "390/390 - 20s - loss: 0.0778 - accuracy: 0.9729 - val_loss: 0.3873 - val_accuracy: 0.8947 - 20s/epoch - 51ms/step\n",
      "Epoch 111/200\n",
      "390/390 - 21s - loss: 0.0713 - accuracy: 0.9749 - val_loss: 0.6995 - val_accuracy: 0.8431 - 21s/epoch - 54ms/step\n",
      "Epoch 112/200\n",
      "390/390 - 21s - loss: 0.0735 - accuracy: 0.9753 - val_loss: 0.3797 - val_accuracy: 0.9000 - 21s/epoch - 55ms/step\n",
      "Epoch 113/200\n",
      "390/390 - 20s - loss: 0.0741 - accuracy: 0.9749 - val_loss: 0.3597 - val_accuracy: 0.9004 - 20s/epoch - 51ms/step\n",
      "Epoch 114/200\n",
      "390/390 - 21s - loss: 0.0709 - accuracy: 0.9761 - val_loss: 0.3971 - val_accuracy: 0.8971 - 21s/epoch - 55ms/step\n",
      "Epoch 115/200\n",
      "390/390 - 21s - loss: 0.0720 - accuracy: 0.9755 - val_loss: 0.4234 - val_accuracy: 0.8881 - 21s/epoch - 53ms/step\n",
      "Epoch 116/200\n",
      "390/390 - 20s - loss: 0.0708 - accuracy: 0.9758 - val_loss: 0.5647 - val_accuracy: 0.8624 - 20s/epoch - 51ms/step\n",
      "Epoch 117/200\n",
      "390/390 - 21s - loss: 0.0703 - accuracy: 0.9757 - val_loss: 0.3363 - val_accuracy: 0.9063 - 21s/epoch - 53ms/step\n",
      "Epoch 118/200\n",
      "390/390 - 22s - loss: 0.0749 - accuracy: 0.9745 - val_loss: 0.3925 - val_accuracy: 0.8979 - 22s/epoch - 57ms/step\n",
      "Epoch 119/200\n",
      "390/390 - 21s - loss: 0.0711 - accuracy: 0.9752 - val_loss: 0.4062 - val_accuracy: 0.8897 - 21s/epoch - 54ms/step\n",
      "Epoch 120/200\n",
      "390/390 - 21s - loss: 0.0725 - accuracy: 0.9751 - val_loss: 0.4138 - val_accuracy: 0.8922 - 21s/epoch - 55ms/step\n",
      "Epoch 121/200\n",
      "390/390 - 20s - loss: 0.0692 - accuracy: 0.9768 - val_loss: 0.3322 - val_accuracy: 0.9082 - 20s/epoch - 51ms/step\n",
      "Epoch 122/200\n",
      "390/390 - 20s - loss: 0.0681 - accuracy: 0.9764 - val_loss: 0.3995 - val_accuracy: 0.8924 - 20s/epoch - 51ms/step\n",
      "Epoch 123/200\n",
      "390/390 - 21s - loss: 0.0667 - accuracy: 0.9768 - val_loss: 0.3572 - val_accuracy: 0.9038 - 21s/epoch - 55ms/step\n",
      "Epoch 124/200\n",
      "390/390 - 21s - loss: 0.0698 - accuracy: 0.9761 - val_loss: 0.3772 - val_accuracy: 0.8993 - 21s/epoch - 54ms/step\n",
      "Epoch 125/200\n",
      "390/390 - 20s - loss: 0.0679 - accuracy: 0.9773 - val_loss: 0.3698 - val_accuracy: 0.9007 - 20s/epoch - 52ms/step\n",
      "Epoch 126/200\n",
      "390/390 - 21s - loss: 0.0684 - accuracy: 0.9765 - val_loss: 0.3238 - val_accuracy: 0.9081 - 21s/epoch - 54ms/step\n",
      "Epoch 127/200\n",
      "390/390 - 21s - loss: 0.0639 - accuracy: 0.9787 - val_loss: 0.3852 - val_accuracy: 0.8952 - 21s/epoch - 55ms/step\n",
      "Epoch 128/200\n",
      "390/390 - 21s - loss: 0.0684 - accuracy: 0.9767 - val_loss: 0.3916 - val_accuracy: 0.8948 - 21s/epoch - 53ms/step\n",
      "Epoch 129/200\n",
      "390/390 - 21s - loss: 0.0657 - accuracy: 0.9773 - val_loss: 0.4026 - val_accuracy: 0.8924 - 21s/epoch - 53ms/step\n",
      "Epoch 130/200\n",
      "390/390 - 20s - loss: 0.0682 - accuracy: 0.9771 - val_loss: 0.3267 - val_accuracy: 0.9116 - 20s/epoch - 51ms/step\n",
      "Epoch 131/200\n",
      "390/390 - 21s - loss: 0.0656 - accuracy: 0.9770 - val_loss: 0.4886 - val_accuracy: 0.8765 - 21s/epoch - 55ms/step\n",
      "Epoch 132/200\n",
      "390/390 - 20s - loss: 0.0681 - accuracy: 0.9770 - val_loss: 0.3477 - val_accuracy: 0.9056 - 20s/epoch - 51ms/step\n",
      "Epoch 133/200\n",
      "390/390 - 21s - loss: 0.0626 - accuracy: 0.9782 - val_loss: 0.4010 - val_accuracy: 0.8977 - 21s/epoch - 54ms/step\n",
      "Epoch 134/200\n",
      "390/390 - 21s - loss: 0.0607 - accuracy: 0.9788 - val_loss: 0.4351 - val_accuracy: 0.8905 - 21s/epoch - 54ms/step\n",
      "Epoch 135/200\n",
      "390/390 - 21s - loss: 0.0652 - accuracy: 0.9777 - val_loss: 0.3754 - val_accuracy: 0.8992 - 21s/epoch - 54ms/step\n",
      "Epoch 136/200\n",
      "390/390 - 22s - loss: 0.0643 - accuracy: 0.9779 - val_loss: 0.3266 - val_accuracy: 0.9094 - 22s/epoch - 56ms/step\n",
      "Epoch 137/200\n",
      "390/390 - 20s - loss: 0.0625 - accuracy: 0.9781 - val_loss: 0.4744 - val_accuracy: 0.8802 - 20s/epoch - 51ms/step\n",
      "Epoch 138/200\n",
      "390/390 - 21s - loss: 0.0584 - accuracy: 0.9809 - val_loss: 0.3629 - val_accuracy: 0.9024 - 21s/epoch - 55ms/step\n",
      "Epoch 139/200\n",
      "390/390 - 21s - loss: 0.0593 - accuracy: 0.9791 - val_loss: 0.3170 - val_accuracy: 0.9112 - 21s/epoch - 54ms/step\n",
      "Epoch 140/200\n",
      "390/390 - 20s - loss: 0.0571 - accuracy: 0.9804 - val_loss: 0.3094 - val_accuracy: 0.9152 - 20s/epoch - 51ms/step\n",
      "Epoch 141/200\n",
      "390/390 - 21s - loss: 0.0623 - accuracy: 0.9780 - val_loss: 0.3289 - val_accuracy: 0.9055 - 21s/epoch - 54ms/step\n",
      "Epoch 142/200\n",
      "390/390 - 21s - loss: 0.0598 - accuracy: 0.9795 - val_loss: 0.4331 - val_accuracy: 0.8796 - 21s/epoch - 54ms/step\n",
      "Epoch 143/200\n",
      "390/390 - 21s - loss: 0.0566 - accuracy: 0.9797 - val_loss: 0.3016 - val_accuracy: 0.9169 - 21s/epoch - 53ms/step\n",
      "Epoch 144/200\n",
      "390/390 - 21s - loss: 0.0577 - accuracy: 0.9802 - val_loss: 0.4110 - val_accuracy: 0.8967 - 21s/epoch - 53ms/step\n",
      "Epoch 145/200\n",
      "390/390 - 21s - loss: 0.0548 - accuracy: 0.9813 - val_loss: 0.3224 - val_accuracy: 0.9149 - 21s/epoch - 53ms/step\n",
      "Epoch 146/200\n",
      "390/390 - 21s - loss: 0.0543 - accuracy: 0.9812 - val_loss: 0.3715 - val_accuracy: 0.9036 - 21s/epoch - 53ms/step\n",
      "Epoch 147/200\n",
      "390/390 - 21s - loss: 0.0546 - accuracy: 0.9808 - val_loss: 0.3447 - val_accuracy: 0.9107 - 21s/epoch - 53ms/step\n",
      "Epoch 148/200\n",
      "390/390 - 20s - loss: 0.0551 - accuracy: 0.9808 - val_loss: 0.3572 - val_accuracy: 0.9045 - 20s/epoch - 51ms/step\n",
      "Epoch 149/200\n",
      "390/390 - 20s - loss: 0.0526 - accuracy: 0.9823 - val_loss: 0.3580 - val_accuracy: 0.9070 - 20s/epoch - 51ms/step\n",
      "Epoch 150/200\n",
      "390/390 - 21s - loss: 0.0528 - accuracy: 0.9822 - val_loss: 0.3163 - val_accuracy: 0.9122 - 21s/epoch - 55ms/step\n",
      "Epoch 151/200\n",
      "390/390 - 21s - loss: 0.0552 - accuracy: 0.9811 - val_loss: 0.3987 - val_accuracy: 0.8951 - 21s/epoch - 53ms/step\n",
      "Epoch 152/200\n",
      "390/390 - 20s - loss: 0.0538 - accuracy: 0.9814 - val_loss: 0.3867 - val_accuracy: 0.8991 - 20s/epoch - 50ms/step\n",
      "Epoch 153/200\n",
      "390/390 - 21s - loss: 0.0514 - accuracy: 0.9825 - val_loss: 0.3200 - val_accuracy: 0.9160 - 21s/epoch - 54ms/step\n",
      "Epoch 154/200\n",
      "390/390 - 21s - loss: 0.0497 - accuracy: 0.9828 - val_loss: 0.3108 - val_accuracy: 0.9148 - 21s/epoch - 53ms/step\n",
      "Epoch 155/200\n",
      "390/390 - 20s - loss: 0.0482 - accuracy: 0.9837 - val_loss: 0.3599 - val_accuracy: 0.9046 - 20s/epoch - 51ms/step\n",
      "Epoch 156/200\n",
      "390/390 - 21s - loss: 0.0493 - accuracy: 0.9836 - val_loss: 0.3234 - val_accuracy: 0.9138 - 21s/epoch - 54ms/step\n",
      "Epoch 157/200\n",
      "390/390 - 20s - loss: 0.0427 - accuracy: 0.9855 - val_loss: 0.3688 - val_accuracy: 0.9036 - 20s/epoch - 51ms/step\n",
      "Epoch 158/200\n",
      "390/390 - 20s - loss: 0.0459 - accuracy: 0.9847 - val_loss: 0.3270 - val_accuracy: 0.9154 - 20s/epoch - 52ms/step\n",
      "Epoch 159/200\n",
      "390/390 - 20s - loss: 0.0448 - accuracy: 0.9850 - val_loss: 0.3421 - val_accuracy: 0.9079 - 20s/epoch - 51ms/step\n",
      "Epoch 160/200\n",
      "390/390 - 20s - loss: 0.0459 - accuracy: 0.9847 - val_loss: 0.3058 - val_accuracy: 0.9165 - 20s/epoch - 52ms/step\n",
      "Epoch 161/200\n",
      "390/390 - 20s - loss: 0.0407 - accuracy: 0.9867 - val_loss: 0.3131 - val_accuracy: 0.9143 - 20s/epoch - 51ms/step\n",
      "Epoch 162/200\n",
      "390/390 - 21s - loss: 0.0413 - accuracy: 0.9859 - val_loss: 0.3061 - val_accuracy: 0.9198 - 21s/epoch - 54ms/step\n",
      "Epoch 163/200\n",
      "390/390 - 21s - loss: 0.0400 - accuracy: 0.9865 - val_loss: 0.3160 - val_accuracy: 0.9107 - 21s/epoch - 54ms/step\n",
      "Epoch 164/200\n",
      "390/390 - 20s - loss: 0.0407 - accuracy: 0.9867 - val_loss: 0.2907 - val_accuracy: 0.9217 - 20s/epoch - 50ms/step\n",
      "Epoch 165/200\n",
      "390/390 - 20s - loss: 0.0380 - accuracy: 0.9875 - val_loss: 0.2777 - val_accuracy: 0.9278 - 20s/epoch - 51ms/step\n",
      "Epoch 166/200\n",
      "390/390 - 21s - loss: 0.0362 - accuracy: 0.9881 - val_loss: 0.3163 - val_accuracy: 0.9182 - 21s/epoch - 54ms/step\n",
      "Epoch 167/200\n",
      "390/390 - 20s - loss: 0.0372 - accuracy: 0.9879 - val_loss: 0.3054 - val_accuracy: 0.9208 - 20s/epoch - 51ms/step\n",
      "Epoch 168/200\n",
      "390/390 - 21s - loss: 0.0365 - accuracy: 0.9874 - val_loss: 0.2943 - val_accuracy: 0.9231 - 21s/epoch - 54ms/step\n",
      "Epoch 169/200\n",
      "390/390 - 21s - loss: 0.0330 - accuracy: 0.9896 - val_loss: 0.3467 - val_accuracy: 0.9098 - 21s/epoch - 54ms/step\n",
      "Epoch 170/200\n",
      "390/390 - 21s - loss: 0.0320 - accuracy: 0.9891 - val_loss: 0.3016 - val_accuracy: 0.9204 - 21s/epoch - 53ms/step\n",
      "Epoch 171/200\n",
      "390/390 - 21s - loss: 0.0320 - accuracy: 0.9895 - val_loss: 0.3172 - val_accuracy: 0.9189 - 21s/epoch - 53ms/step\n",
      "Epoch 172/200\n",
      "390/390 - 21s - loss: 0.0313 - accuracy: 0.9903 - val_loss: 0.2806 - val_accuracy: 0.9252 - 21s/epoch - 54ms/step\n",
      "Epoch 173/200\n",
      "390/390 - 21s - loss: 0.0288 - accuracy: 0.9907 - val_loss: 0.3041 - val_accuracy: 0.9175 - 21s/epoch - 53ms/step\n",
      "Epoch 174/200\n",
      "390/390 - 21s - loss: 0.0271 - accuracy: 0.9920 - val_loss: 0.3770 - val_accuracy: 0.9041 - 21s/epoch - 54ms/step\n",
      "Epoch 175/200\n",
      "390/390 - 21s - loss: 0.0239 - accuracy: 0.9928 - val_loss: 0.2643 - val_accuracy: 0.9321 - 21s/epoch - 54ms/step\n",
      "Epoch 176/200\n",
      "390/390 - 20s - loss: 0.0248 - accuracy: 0.9917 - val_loss: 0.2918 - val_accuracy: 0.9193 - 20s/epoch - 51ms/step\n",
      "Epoch 177/200\n",
      "390/390 - 20s - loss: 0.0230 - accuracy: 0.9929 - val_loss: 0.2737 - val_accuracy: 0.9246 - 20s/epoch - 51ms/step\n",
      "Epoch 178/200\n",
      "390/390 - 21s - loss: 0.0214 - accuracy: 0.9935 - val_loss: 0.2607 - val_accuracy: 0.9291 - 21s/epoch - 53ms/step\n",
      "Epoch 179/200\n",
      "390/390 - 20s - loss: 0.0199 - accuracy: 0.9939 - val_loss: 0.2675 - val_accuracy: 0.9276 - 20s/epoch - 52ms/step\n",
      "Epoch 180/200\n",
      "390/390 - 21s - loss: 0.0193 - accuracy: 0.9941 - val_loss: 0.2490 - val_accuracy: 0.9302 - 21s/epoch - 54ms/step\n",
      "Epoch 181/200\n",
      "390/390 - 21s - loss: 0.0160 - accuracy: 0.9958 - val_loss: 0.2461 - val_accuracy: 0.9341 - 21s/epoch - 53ms/step\n",
      "Epoch 182/200\n",
      "390/390 - 21s - loss: 0.0131 - accuracy: 0.9966 - val_loss: 0.2297 - val_accuracy: 0.9359 - 21s/epoch - 53ms/step\n",
      "Epoch 183/200\n",
      "390/390 - 21s - loss: 0.0121 - accuracy: 0.9968 - val_loss: 0.2205 - val_accuracy: 0.9405 - 21s/epoch - 53ms/step\n",
      "Epoch 184/200\n",
      "390/390 - 21s - loss: 0.0127 - accuracy: 0.9969 - val_loss: 0.2379 - val_accuracy: 0.9358 - 21s/epoch - 53ms/step\n",
      "Epoch 185/200\n",
      "390/390 - 20s - loss: 0.0107 - accuracy: 0.9974 - val_loss: 0.2223 - val_accuracy: 0.9396 - 20s/epoch - 51ms/step\n",
      "Epoch 186/200\n",
      "390/390 - 21s - loss: 0.0097 - accuracy: 0.9978 - val_loss: 0.2264 - val_accuracy: 0.9380 - 21s/epoch - 54ms/step\n",
      "Epoch 187/200\n",
      "390/390 - 21s - loss: 0.0090 - accuracy: 0.9984 - val_loss: 0.2065 - val_accuracy: 0.9420 - 21s/epoch - 54ms/step\n",
      "Epoch 188/200\n",
      "390/390 - 20s - loss: 0.0080 - accuracy: 0.9987 - val_loss: 0.2149 - val_accuracy: 0.9404 - 20s/epoch - 51ms/step\n",
      "Epoch 189/200\n",
      "390/390 - 21s - loss: 0.0093 - accuracy: 0.9982 - val_loss: 0.2103 - val_accuracy: 0.9434 - 21s/epoch - 54ms/step\n",
      "Epoch 190/200\n",
      "390/390 - 20s - loss: 0.0090 - accuracy: 0.9986 - val_loss: 0.1989 - val_accuracy: 0.9440 - 20s/epoch - 51ms/step\n",
      "Epoch 191/200\n",
      "390/390 - 20s - loss: 0.0088 - accuracy: 0.9989 - val_loss: 0.1920 - val_accuracy: 0.9444 - 20s/epoch - 51ms/step\n",
      "Epoch 192/200\n",
      "390/390 - 20s - loss: 0.0079 - accuracy: 0.9992 - val_loss: 0.1935 - val_accuracy: 0.9438 - 20s/epoch - 52ms/step\n",
      "Epoch 193/200\n",
      "390/390 - 21s - loss: 0.0091 - accuracy: 0.9989 - val_loss: 0.1927 - val_accuracy: 0.9442 - 21s/epoch - 53ms/step\n",
      "Epoch 194/200\n",
      "390/390 - 21s - loss: 0.0094 - accuracy: 0.9992 - val_loss: 0.1885 - val_accuracy: 0.9444 - 21s/epoch - 55ms/step\n",
      "Epoch 195/200\n",
      "390/390 - 21s - loss: 0.0102 - accuracy: 0.9993 - val_loss: 0.1822 - val_accuracy: 0.9445 - 21s/epoch - 54ms/step\n",
      "Epoch 196/200\n",
      "390/390 - 21s - loss: 0.0116 - accuracy: 0.9994 - val_loss: 0.1787 - val_accuracy: 0.9443 - 21s/epoch - 54ms/step\n",
      "Epoch 197/200\n",
      "390/390 - 21s - loss: 0.0139 - accuracy: 0.9992 - val_loss: 0.1784 - val_accuracy: 0.9446 - 21s/epoch - 54ms/step\n",
      "Epoch 198/200\n",
      "390/390 - 21s - loss: 0.0172 - accuracy: 0.9992 - val_loss: 0.1764 - val_accuracy: 0.9452 - 21s/epoch - 54ms/step\n",
      "Epoch 199/200\n",
      "390/390 - 21s - loss: 0.0209 - accuracy: 0.9991 - val_loss: 0.1783 - val_accuracy: 0.9444 - 21s/epoch - 54ms/step\n",
      "Epoch 200/200\n",
      "390/390 - 20s - loss: 0.0265 - accuracy: 0.9993 - val_loss: 0.1804 - val_accuracy: 0.9447 - 20s/epoch - 51ms/step\n",
      " Using best val_acc=0.9452 from last 20 epochs\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.1764 - accuracy: 0.9452\n",
      " Finished: Lam=0.7, Repeat=2, Acc=0.9452\n",
      "\n",
      " lambda: Lam=0.7, Repeat=3/3\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "321009422 561714176\n",
      "Epoch 1/200\n",
      "390/390 - 22s - loss: 0.7101 - accuracy: 0.7694 - val_loss: 6.3743 - val_accuracy: 0.2534 - 22s/epoch - 56ms/step\n",
      "Epoch 2/200\n",
      "390/390 - 20s - loss: 0.6000 - accuracy: 0.7955 - val_loss: 1.0847 - val_accuracy: 0.6718 - 20s/epoch - 51ms/step\n",
      "Epoch 3/200\n",
      "390/390 - 20s - loss: 0.5051 - accuracy: 0.8276 - val_loss: 2.1178 - val_accuracy: 0.5411 - 20s/epoch - 51ms/step\n",
      "Epoch 4/200\n",
      "390/390 - 21s - loss: 0.4463 - accuracy: 0.8487 - val_loss: 0.9220 - val_accuracy: 0.7160 - 21s/epoch - 54ms/step\n",
      "Epoch 5/200\n",
      "390/390 - 21s - loss: 0.4031 - accuracy: 0.8627 - val_loss: 1.0643 - val_accuracy: 0.7156 - 21s/epoch - 53ms/step\n",
      "Epoch 6/200\n",
      "390/390 - 21s - loss: 0.3553 - accuracy: 0.8775 - val_loss: 0.5452 - val_accuracy: 0.8290 - 21s/epoch - 53ms/step\n",
      "Epoch 7/200\n",
      "390/390 - 21s - loss: 0.3098 - accuracy: 0.8926 - val_loss: 0.4582 - val_accuracy: 0.8513 - 21s/epoch - 53ms/step\n",
      "Epoch 8/200\n",
      "390/390 - 20s - loss: 0.2816 - accuracy: 0.9018 - val_loss: 0.4412 - val_accuracy: 0.8564 - 20s/epoch - 51ms/step\n",
      "Epoch 9/200\n",
      "390/390 - 20s - loss: 0.2659 - accuracy: 0.9096 - val_loss: 0.5091 - val_accuracy: 0.8438 - 20s/epoch - 51ms/step\n",
      "Epoch 10/200\n",
      "390/390 - 20s - loss: 0.2409 - accuracy: 0.9167 - val_loss: 0.4223 - val_accuracy: 0.8691 - 20s/epoch - 51ms/step\n",
      "Epoch 11/200\n",
      "390/390 - 20s - loss: 0.2254 - accuracy: 0.9216 - val_loss: 0.4311 - val_accuracy: 0.8677 - 20s/epoch - 51ms/step\n",
      "Epoch 12/200\n",
      "390/390 - 20s - loss: 0.2147 - accuracy: 0.9260 - val_loss: 0.3660 - val_accuracy: 0.8828 - 20s/epoch - 51ms/step\n",
      "Epoch 13/200\n",
      "390/390 - 20s - loss: 0.2044 - accuracy: 0.9296 - val_loss: 0.3891 - val_accuracy: 0.8713 - 20s/epoch - 51ms/step\n",
      "Epoch 14/200\n",
      "390/390 - 20s - loss: 0.1926 - accuracy: 0.9333 - val_loss: 0.4748 - val_accuracy: 0.8616 - 20s/epoch - 51ms/step\n",
      "Epoch 15/200\n",
      "390/390 - 20s - loss: 0.1856 - accuracy: 0.9361 - val_loss: 0.4644 - val_accuracy: 0.8541 - 20s/epoch - 51ms/step\n",
      "Epoch 16/200\n",
      "390/390 - 21s - loss: 0.1813 - accuracy: 0.9368 - val_loss: 0.3793 - val_accuracy: 0.8782 - 21s/epoch - 53ms/step\n",
      "Epoch 17/200\n",
      "390/390 - 20s - loss: 0.1733 - accuracy: 0.9398 - val_loss: 0.6086 - val_accuracy: 0.8257 - 20s/epoch - 52ms/step\n",
      "Epoch 18/200\n",
      "390/390 - 21s - loss: 0.1681 - accuracy: 0.9416 - val_loss: 0.4153 - val_accuracy: 0.8707 - 21s/epoch - 53ms/step\n",
      "Epoch 19/200\n",
      "390/390 - 21s - loss: 0.1624 - accuracy: 0.9442 - val_loss: 0.4632 - val_accuracy: 0.8623 - 21s/epoch - 53ms/step\n",
      "Epoch 20/200\n",
      "390/390 - 21s - loss: 0.1557 - accuracy: 0.9461 - val_loss: 0.3837 - val_accuracy: 0.8873 - 21s/epoch - 53ms/step\n",
      "Epoch 21/200\n",
      "390/390 - 20s - loss: 0.1531 - accuracy: 0.9459 - val_loss: 0.4045 - val_accuracy: 0.8801 - 20s/epoch - 51ms/step\n",
      "Epoch 22/200\n",
      "390/390 - 21s - loss: 0.1496 - accuracy: 0.9475 - val_loss: 0.3968 - val_accuracy: 0.8840 - 21s/epoch - 55ms/step\n",
      "Epoch 23/200\n",
      "390/390 - 20s - loss: 0.1467 - accuracy: 0.9481 - val_loss: 0.3793 - val_accuracy: 0.8862 - 20s/epoch - 51ms/step\n",
      "Epoch 24/200\n",
      "390/390 - 21s - loss: 0.1467 - accuracy: 0.9487 - val_loss: 0.4064 - val_accuracy: 0.8786 - 21s/epoch - 53ms/step\n",
      "Epoch 25/200\n",
      "390/390 - 20s - loss: 0.1401 - accuracy: 0.9509 - val_loss: 0.3381 - val_accuracy: 0.8976 - 20s/epoch - 52ms/step\n",
      "Epoch 26/200\n",
      "390/390 - 20s - loss: 0.1363 - accuracy: 0.9522 - val_loss: 0.4177 - val_accuracy: 0.8740 - 20s/epoch - 51ms/step\n",
      "Epoch 27/200\n",
      "390/390 - 21s - loss: 0.1386 - accuracy: 0.9511 - val_loss: 0.3545 - val_accuracy: 0.8908 - 21s/epoch - 53ms/step\n",
      "Epoch 28/200\n",
      "390/390 - 21s - loss: 0.1319 - accuracy: 0.9545 - val_loss: 0.3383 - val_accuracy: 0.8947 - 21s/epoch - 53ms/step\n",
      "Epoch 29/200\n",
      "390/390 - 20s - loss: 0.1310 - accuracy: 0.9545 - val_loss: 0.3708 - val_accuracy: 0.8907 - 20s/epoch - 51ms/step\n",
      "Epoch 30/200\n",
      "390/390 - 21s - loss: 0.1289 - accuracy: 0.9555 - val_loss: 0.3461 - val_accuracy: 0.8985 - 21s/epoch - 53ms/step\n",
      "Epoch 31/200\n",
      "390/390 - 20s - loss: 0.1260 - accuracy: 0.9559 - val_loss: 0.3277 - val_accuracy: 0.8983 - 20s/epoch - 51ms/step\n",
      "Epoch 32/200\n",
      "390/390 - 21s - loss: 0.1205 - accuracy: 0.9587 - val_loss: 0.3610 - val_accuracy: 0.8953 - 21s/epoch - 54ms/step\n",
      "Epoch 33/200\n",
      "390/390 - 21s - loss: 0.1274 - accuracy: 0.9560 - val_loss: 0.3615 - val_accuracy: 0.8969 - 21s/epoch - 54ms/step\n",
      "Epoch 34/200\n",
      "390/390 - 21s - loss: 0.1211 - accuracy: 0.9576 - val_loss: 0.5065 - val_accuracy: 0.8605 - 21s/epoch - 54ms/step\n",
      "Epoch 35/200\n",
      "390/390 - 20s - loss: 0.1218 - accuracy: 0.9572 - val_loss: 0.3316 - val_accuracy: 0.8998 - 20s/epoch - 51ms/step\n",
      "Epoch 36/200\n",
      "390/390 - 21s - loss: 0.1184 - accuracy: 0.9594 - val_loss: 0.3455 - val_accuracy: 0.8932 - 21s/epoch - 53ms/step\n",
      "Epoch 37/200\n",
      "390/390 - 21s - loss: 0.1177 - accuracy: 0.9590 - val_loss: 0.4645 - val_accuracy: 0.8759 - 21s/epoch - 53ms/step\n",
      "Epoch 38/200\n",
      "390/390 - 20s - loss: 0.1116 - accuracy: 0.9614 - val_loss: 0.3709 - val_accuracy: 0.8910 - 20s/epoch - 51ms/step\n",
      "Epoch 39/200\n",
      "390/390 - 20s - loss: 0.1165 - accuracy: 0.9588 - val_loss: 0.4276 - val_accuracy: 0.8755 - 20s/epoch - 51ms/step\n",
      "Epoch 40/200\n",
      "390/390 - 21s - loss: 0.1194 - accuracy: 0.9579 - val_loss: 0.7143 - val_accuracy: 0.8113 - 21s/epoch - 53ms/step\n",
      "Epoch 41/200\n",
      "390/390 - 20s - loss: 0.1123 - accuracy: 0.9608 - val_loss: 0.4419 - val_accuracy: 0.8768 - 20s/epoch - 51ms/step\n",
      "Epoch 42/200\n",
      "390/390 - 20s - loss: 0.1093 - accuracy: 0.9623 - val_loss: 0.4053 - val_accuracy: 0.8898 - 20s/epoch - 51ms/step\n",
      "Epoch 43/200\n",
      "390/390 - 21s - loss: 0.1097 - accuracy: 0.9623 - val_loss: 0.4059 - val_accuracy: 0.8831 - 21s/epoch - 53ms/step\n",
      "Epoch 44/200\n",
      "390/390 - 20s - loss: 0.1114 - accuracy: 0.9615 - val_loss: 0.3808 - val_accuracy: 0.8919 - 20s/epoch - 51ms/step\n",
      "Epoch 45/200\n",
      "390/390 - 20s - loss: 0.1117 - accuracy: 0.9614 - val_loss: 0.4134 - val_accuracy: 0.8848 - 20s/epoch - 51ms/step\n",
      "Epoch 46/200\n",
      "390/390 - 21s - loss: 0.1068 - accuracy: 0.9621 - val_loss: 0.3691 - val_accuracy: 0.8987 - 21s/epoch - 53ms/step\n",
      "Epoch 47/200\n",
      "390/390 - 20s - loss: 0.1076 - accuracy: 0.9618 - val_loss: 0.4178 - val_accuracy: 0.8855 - 20s/epoch - 51ms/step\n",
      "Epoch 48/200\n",
      "390/390 - 21s - loss: 0.1084 - accuracy: 0.9616 - val_loss: 0.4061 - val_accuracy: 0.8867 - 21s/epoch - 53ms/step\n",
      "Epoch 49/200\n",
      "390/390 - 21s - loss: 0.1054 - accuracy: 0.9642 - val_loss: 0.3449 - val_accuracy: 0.8983 - 21s/epoch - 53ms/step\n",
      "Epoch 50/200\n",
      "390/390 - 20s - loss: 0.1038 - accuracy: 0.9635 - val_loss: 0.3152 - val_accuracy: 0.9053 - 20s/epoch - 51ms/step\n",
      "Epoch 51/200\n",
      "390/390 - 20s - loss: 0.1062 - accuracy: 0.9635 - val_loss: 0.4192 - val_accuracy: 0.8864 - 20s/epoch - 51ms/step\n",
      "Epoch 52/200\n",
      "390/390 - 21s - loss: 0.1048 - accuracy: 0.9629 - val_loss: 0.3730 - val_accuracy: 0.8970 - 21s/epoch - 53ms/step\n",
      "Epoch 53/200\n",
      "390/390 - 20s - loss: 0.1025 - accuracy: 0.9638 - val_loss: 0.3880 - val_accuracy: 0.8893 - 20s/epoch - 51ms/step\n",
      "Epoch 54/200\n",
      "390/390 - 21s - loss: 0.1021 - accuracy: 0.9643 - val_loss: 0.2975 - val_accuracy: 0.9110 - 21s/epoch - 53ms/step\n",
      "Epoch 55/200\n",
      "390/390 - 20s - loss: 0.0986 - accuracy: 0.9662 - val_loss: 0.3989 - val_accuracy: 0.8873 - 20s/epoch - 51ms/step\n",
      "Epoch 56/200\n",
      "390/390 - 21s - loss: 0.0992 - accuracy: 0.9653 - val_loss: 0.3995 - val_accuracy: 0.8893 - 21s/epoch - 55ms/step\n",
      "Epoch 57/200\n",
      "390/390 - 21s - loss: 0.1006 - accuracy: 0.9651 - val_loss: 0.3790 - val_accuracy: 0.8916 - 21s/epoch - 53ms/step\n",
      "Epoch 58/200\n",
      "390/390 - 21s - loss: 0.0984 - accuracy: 0.9652 - val_loss: 0.6400 - val_accuracy: 0.8446 - 21s/epoch - 53ms/step\n",
      "Epoch 59/200\n",
      "390/390 - 20s - loss: 0.0998 - accuracy: 0.9644 - val_loss: 0.3423 - val_accuracy: 0.8984 - 20s/epoch - 51ms/step\n",
      "Epoch 60/200\n",
      "390/390 - 21s - loss: 0.0985 - accuracy: 0.9664 - val_loss: 0.4061 - val_accuracy: 0.8839 - 21s/epoch - 54ms/step\n",
      "Epoch 61/200\n",
      "390/390 - 20s - loss: 0.0951 - accuracy: 0.9670 - val_loss: 0.3830 - val_accuracy: 0.8900 - 20s/epoch - 51ms/step\n",
      "Epoch 62/200\n",
      "390/390 - 20s - loss: 0.0948 - accuracy: 0.9665 - val_loss: 0.3200 - val_accuracy: 0.9061 - 20s/epoch - 51ms/step\n",
      "Epoch 63/200\n",
      "390/390 - 21s - loss: 0.0965 - accuracy: 0.9668 - val_loss: 0.3718 - val_accuracy: 0.8956 - 21s/epoch - 53ms/step\n",
      "Epoch 64/200\n",
      "390/390 - 21s - loss: 0.0926 - accuracy: 0.9675 - val_loss: 0.4152 - val_accuracy: 0.8870 - 21s/epoch - 54ms/step\n",
      "Epoch 65/200\n",
      "390/390 - 21s - loss: 0.0955 - accuracy: 0.9678 - val_loss: 0.3237 - val_accuracy: 0.9046 - 21s/epoch - 54ms/step\n",
      "Epoch 66/200\n",
      "390/390 - 21s - loss: 0.0950 - accuracy: 0.9665 - val_loss: 0.3914 - val_accuracy: 0.8894 - 21s/epoch - 54ms/step\n",
      "Epoch 67/200\n",
      "390/390 - 21s - loss: 0.0939 - accuracy: 0.9679 - val_loss: 0.4265 - val_accuracy: 0.8883 - 21s/epoch - 53ms/step\n",
      "Epoch 68/200\n",
      "390/390 - 20s - loss: 0.0995 - accuracy: 0.9657 - val_loss: 0.4959 - val_accuracy: 0.8704 - 20s/epoch - 50ms/step\n",
      "Epoch 69/200\n",
      "390/390 - 21s - loss: 0.0928 - accuracy: 0.9683 - val_loss: 0.4515 - val_accuracy: 0.8837 - 21s/epoch - 54ms/step\n",
      "Epoch 70/200\n",
      "390/390 - 21s - loss: 0.0922 - accuracy: 0.9680 - val_loss: 0.4406 - val_accuracy: 0.8802 - 21s/epoch - 53ms/step\n",
      "Epoch 71/200\n",
      "390/390 - 20s - loss: 0.0922 - accuracy: 0.9680 - val_loss: 0.3170 - val_accuracy: 0.9075 - 20s/epoch - 52ms/step\n",
      "Epoch 72/200\n",
      "390/390 - 21s - loss: 0.0904 - accuracy: 0.9678 - val_loss: 0.5920 - val_accuracy: 0.8549 - 21s/epoch - 54ms/step\n",
      "Epoch 73/200\n",
      "390/390 - 20s - loss: 0.0944 - accuracy: 0.9678 - val_loss: 0.3850 - val_accuracy: 0.8932 - 20s/epoch - 51ms/step\n",
      "Epoch 74/200\n",
      "390/390 - 20s - loss: 0.0883 - accuracy: 0.9690 - val_loss: 0.3377 - val_accuracy: 0.9052 - 20s/epoch - 51ms/step\n",
      "Epoch 75/200\n",
      "390/390 - 20s - loss: 0.0887 - accuracy: 0.9695 - val_loss: 0.4079 - val_accuracy: 0.8871 - 20s/epoch - 51ms/step\n",
      "Epoch 76/200\n",
      "390/390 - 21s - loss: 0.0904 - accuracy: 0.9692 - val_loss: 0.3158 - val_accuracy: 0.9088 - 21s/epoch - 53ms/step\n",
      "Epoch 77/200\n",
      "390/390 - 20s - loss: 0.0909 - accuracy: 0.9685 - val_loss: 0.3266 - val_accuracy: 0.9072 - 20s/epoch - 51ms/step\n",
      "Epoch 78/200\n",
      "390/390 - 20s - loss: 0.0875 - accuracy: 0.9696 - val_loss: 0.4878 - val_accuracy: 0.8688 - 20s/epoch - 51ms/step\n",
      "Epoch 79/200\n",
      "390/390 - 21s - loss: 0.0897 - accuracy: 0.9691 - val_loss: 0.3420 - val_accuracy: 0.9058 - 21s/epoch - 55ms/step\n",
      "Epoch 80/200\n",
      "390/390 - 20s - loss: 0.0890 - accuracy: 0.9692 - val_loss: 0.4342 - val_accuracy: 0.8819 - 20s/epoch - 51ms/step\n",
      "Epoch 81/200\n",
      "390/390 - 21s - loss: 0.0864 - accuracy: 0.9704 - val_loss: 0.3289 - val_accuracy: 0.9068 - 21s/epoch - 54ms/step\n",
      "Epoch 82/200\n",
      "390/390 - 21s - loss: 0.0842 - accuracy: 0.9708 - val_loss: 0.3396 - val_accuracy: 0.8990 - 21s/epoch - 53ms/step\n",
      "Epoch 83/200\n",
      "390/390 - 20s - loss: 0.0868 - accuracy: 0.9693 - val_loss: 0.3937 - val_accuracy: 0.8945 - 20s/epoch - 51ms/step\n",
      "Epoch 84/200\n",
      "390/390 - 20s - loss: 0.0833 - accuracy: 0.9724 - val_loss: 0.3532 - val_accuracy: 0.9035 - 20s/epoch - 51ms/step\n",
      "Epoch 85/200\n",
      "390/390 - 20s - loss: 0.0878 - accuracy: 0.9695 - val_loss: 0.3412 - val_accuracy: 0.9051 - 20s/epoch - 51ms/step\n",
      "Epoch 86/200\n",
      "390/390 - 20s - loss: 0.0835 - accuracy: 0.9707 - val_loss: 0.3364 - val_accuracy: 0.9059 - 20s/epoch - 51ms/step\n",
      "Epoch 87/200\n",
      "390/390 - 21s - loss: 0.0869 - accuracy: 0.9699 - val_loss: 0.3103 - val_accuracy: 0.9103 - 21s/epoch - 54ms/step\n",
      "Epoch 88/200\n",
      "390/390 - 21s - loss: 0.0806 - accuracy: 0.9718 - val_loss: 0.3640 - val_accuracy: 0.9009 - 21s/epoch - 54ms/step\n",
      "Epoch 89/200\n",
      "390/390 - 20s - loss: 0.0836 - accuracy: 0.9711 - val_loss: 0.4190 - val_accuracy: 0.8873 - 20s/epoch - 51ms/step\n",
      "Epoch 90/200\n",
      "390/390 - 21s - loss: 0.0863 - accuracy: 0.9699 - val_loss: 0.3243 - val_accuracy: 0.9074 - 21s/epoch - 54ms/step\n",
      "Epoch 91/200\n",
      "390/390 - 21s - loss: 0.0847 - accuracy: 0.9706 - val_loss: 0.3534 - val_accuracy: 0.9009 - 21s/epoch - 53ms/step\n",
      "Epoch 92/200\n",
      "390/390 - 21s - loss: 0.0830 - accuracy: 0.9711 - val_loss: 0.3632 - val_accuracy: 0.9006 - 21s/epoch - 53ms/step\n",
      "Epoch 93/200\n",
      "390/390 - 21s - loss: 0.0817 - accuracy: 0.9717 - val_loss: 0.3993 - val_accuracy: 0.8971 - 21s/epoch - 53ms/step\n",
      "Epoch 94/200\n",
      "390/390 - 21s - loss: 0.0825 - accuracy: 0.9708 - val_loss: 0.3537 - val_accuracy: 0.9025 - 21s/epoch - 53ms/step\n",
      "Epoch 95/200\n",
      "390/390 - 20s - loss: 0.0809 - accuracy: 0.9721 - val_loss: 0.3506 - val_accuracy: 0.8973 - 20s/epoch - 51ms/step\n",
      "Epoch 96/200\n",
      "390/390 - 20s - loss: 0.0811 - accuracy: 0.9718 - val_loss: 0.4021 - val_accuracy: 0.8890 - 20s/epoch - 51ms/step\n",
      "Epoch 97/200\n",
      "390/390 - 21s - loss: 0.0763 - accuracy: 0.9730 - val_loss: 0.3582 - val_accuracy: 0.9049 - 21s/epoch - 54ms/step\n",
      "Epoch 98/200\n",
      "390/390 - 20s - loss: 0.0814 - accuracy: 0.9714 - val_loss: 0.4148 - val_accuracy: 0.8887 - 20s/epoch - 51ms/step\n",
      "Epoch 99/200\n",
      "390/390 - 21s - loss: 0.0814 - accuracy: 0.9722 - val_loss: 0.3500 - val_accuracy: 0.9039 - 21s/epoch - 53ms/step\n",
      "Epoch 100/200\n",
      "390/390 - 21s - loss: 0.0810 - accuracy: 0.9721 - val_loss: 0.3483 - val_accuracy: 0.9054 - 21s/epoch - 53ms/step\n",
      "Epoch 101/200\n",
      "390/390 - 20s - loss: 0.0815 - accuracy: 0.9719 - val_loss: 0.3792 - val_accuracy: 0.8976 - 20s/epoch - 52ms/step\n",
      "Epoch 102/200\n",
      "390/390 - 21s - loss: 0.0747 - accuracy: 0.9745 - val_loss: 0.4028 - val_accuracy: 0.8921 - 21s/epoch - 53ms/step\n",
      "Epoch 103/200\n",
      "390/390 - 21s - loss: 0.0809 - accuracy: 0.9720 - val_loss: 0.3987 - val_accuracy: 0.8847 - 21s/epoch - 53ms/step\n",
      "Epoch 104/200\n",
      "390/390 - 20s - loss: 0.0777 - accuracy: 0.9728 - val_loss: 0.4201 - val_accuracy: 0.8894 - 20s/epoch - 51ms/step\n",
      "Epoch 105/200\n",
      "390/390 - 20s - loss: 0.0758 - accuracy: 0.9738 - val_loss: 0.3506 - val_accuracy: 0.9029 - 20s/epoch - 52ms/step\n",
      "Epoch 106/200\n",
      "390/390 - 21s - loss: 0.0812 - accuracy: 0.9713 - val_loss: 0.3263 - val_accuracy: 0.9093 - 21s/epoch - 55ms/step\n",
      "Epoch 107/200\n",
      "390/390 - 21s - loss: 0.0762 - accuracy: 0.9733 - val_loss: 0.4825 - val_accuracy: 0.8725 - 21s/epoch - 53ms/step\n",
      "Epoch 108/200\n",
      "390/390 - 21s - loss: 0.0757 - accuracy: 0.9745 - val_loss: 0.3722 - val_accuracy: 0.9006 - 21s/epoch - 54ms/step\n",
      "Epoch 109/200\n",
      "390/390 - 21s - loss: 0.0742 - accuracy: 0.9747 - val_loss: 0.3551 - val_accuracy: 0.9040 - 21s/epoch - 54ms/step\n",
      "Epoch 110/200\n",
      "390/390 - 20s - loss: 0.0727 - accuracy: 0.9749 - val_loss: 0.3793 - val_accuracy: 0.9011 - 20s/epoch - 51ms/step\n",
      "Epoch 111/200\n",
      "390/390 - 20s - loss: 0.0737 - accuracy: 0.9746 - val_loss: 0.3948 - val_accuracy: 0.8984 - 20s/epoch - 51ms/step\n",
      "Epoch 112/200\n",
      "390/390 - 20s - loss: 0.0726 - accuracy: 0.9747 - val_loss: 0.3691 - val_accuracy: 0.9009 - 20s/epoch - 51ms/step\n",
      "Epoch 113/200\n",
      "390/390 - 21s - loss: 0.0769 - accuracy: 0.9738 - val_loss: 0.3480 - val_accuracy: 0.9024 - 21s/epoch - 53ms/step\n",
      "Epoch 114/200\n",
      "390/390 - 20s - loss: 0.0759 - accuracy: 0.9730 - val_loss: 0.3978 - val_accuracy: 0.8922 - 20s/epoch - 52ms/step\n",
      "Epoch 115/200\n",
      "390/390 - 20s - loss: 0.0708 - accuracy: 0.9759 - val_loss: 0.3687 - val_accuracy: 0.9003 - 20s/epoch - 51ms/step\n",
      "Epoch 116/200\n",
      "390/390 - 20s - loss: 0.0756 - accuracy: 0.9728 - val_loss: 0.3601 - val_accuracy: 0.8990 - 20s/epoch - 51ms/step\n",
      "Epoch 117/200\n",
      "390/390 - 20s - loss: 0.0691 - accuracy: 0.9756 - val_loss: 0.4605 - val_accuracy: 0.8791 - 20s/epoch - 51ms/step\n",
      "Epoch 118/200\n",
      "390/390 - 21s - loss: 0.0740 - accuracy: 0.9744 - val_loss: 0.3235 - val_accuracy: 0.9089 - 21s/epoch - 53ms/step\n",
      "Epoch 119/200\n",
      "390/390 - 20s - loss: 0.0755 - accuracy: 0.9735 - val_loss: 0.3777 - val_accuracy: 0.8957 - 20s/epoch - 51ms/step\n",
      "Epoch 120/200\n",
      "390/390 - 20s - loss: 0.0721 - accuracy: 0.9750 - val_loss: 0.3877 - val_accuracy: 0.8919 - 20s/epoch - 53ms/step\n",
      "Epoch 121/200\n",
      "390/390 - 21s - loss: 0.0676 - accuracy: 0.9766 - val_loss: 0.4420 - val_accuracy: 0.8840 - 21s/epoch - 54ms/step\n",
      "Epoch 122/200\n",
      "390/390 - 21s - loss: 0.0684 - accuracy: 0.9766 - val_loss: 0.3341 - val_accuracy: 0.9055 - 21s/epoch - 53ms/step\n",
      "Epoch 123/200\n",
      "390/390 - 20s - loss: 0.0747 - accuracy: 0.9736 - val_loss: 0.3754 - val_accuracy: 0.8999 - 20s/epoch - 51ms/step\n",
      "Epoch 124/200\n",
      "390/390 - 21s - loss: 0.0679 - accuracy: 0.9774 - val_loss: 0.4289 - val_accuracy: 0.8905 - 21s/epoch - 54ms/step\n",
      "Epoch 125/200\n",
      "390/390 - 20s - loss: 0.0701 - accuracy: 0.9762 - val_loss: 0.3277 - val_accuracy: 0.9100 - 20s/epoch - 52ms/step\n",
      "Epoch 126/200\n",
      "390/390 - 21s - loss: 0.0676 - accuracy: 0.9765 - val_loss: 0.3772 - val_accuracy: 0.8978 - 21s/epoch - 53ms/step\n",
      "Epoch 127/200\n",
      "390/390 - 21s - loss: 0.0642 - accuracy: 0.9781 - val_loss: 0.3106 - val_accuracy: 0.9153 - 21s/epoch - 53ms/step\n",
      "Epoch 128/200\n",
      "390/390 - 21s - loss: 0.0715 - accuracy: 0.9754 - val_loss: 0.4377 - val_accuracy: 0.8881 - 21s/epoch - 53ms/step\n",
      "Epoch 129/200\n",
      "390/390 - 20s - loss: 0.0657 - accuracy: 0.9774 - val_loss: 0.3653 - val_accuracy: 0.9057 - 20s/epoch - 51ms/step\n",
      "Epoch 130/200\n",
      "390/390 - 21s - loss: 0.0670 - accuracy: 0.9766 - val_loss: 0.3146 - val_accuracy: 0.9113 - 21s/epoch - 54ms/step\n",
      "Epoch 131/200\n",
      "390/390 - 20s - loss: 0.0583 - accuracy: 0.9793 - val_loss: 0.5270 - val_accuracy: 0.8700 - 20s/epoch - 51ms/step\n",
      "Epoch 132/200\n",
      "390/390 - 21s - loss: 0.0631 - accuracy: 0.9776 - val_loss: 0.3239 - val_accuracy: 0.9128 - 21s/epoch - 53ms/step\n",
      "Epoch 133/200\n",
      "390/390 - 21s - loss: 0.0636 - accuracy: 0.9779 - val_loss: 0.3107 - val_accuracy: 0.9132 - 21s/epoch - 55ms/step\n",
      "Epoch 134/200\n",
      "390/390 - 21s - loss: 0.0643 - accuracy: 0.9772 - val_loss: 0.3933 - val_accuracy: 0.8922 - 21s/epoch - 54ms/step\n",
      "Epoch 135/200\n",
      "390/390 - 20s - loss: 0.0624 - accuracy: 0.9787 - val_loss: 0.3874 - val_accuracy: 0.8970 - 20s/epoch - 51ms/step\n",
      "Epoch 136/200\n",
      "390/390 - 21s - loss: 0.0623 - accuracy: 0.9785 - val_loss: 0.3332 - val_accuracy: 0.9085 - 21s/epoch - 55ms/step\n",
      "Epoch 137/200\n",
      "390/390 - 21s - loss: 0.0628 - accuracy: 0.9790 - val_loss: 0.3610 - val_accuracy: 0.9030 - 21s/epoch - 54ms/step\n",
      "Epoch 138/200\n",
      "390/390 - 21s - loss: 0.0621 - accuracy: 0.9793 - val_loss: 0.3114 - val_accuracy: 0.9116 - 21s/epoch - 53ms/step\n",
      "Epoch 139/200\n",
      "390/390 - 21s - loss: 0.0574 - accuracy: 0.9808 - val_loss: 0.3478 - val_accuracy: 0.9040 - 21s/epoch - 53ms/step\n",
      "Epoch 140/200\n",
      "390/390 - 20s - loss: 0.0587 - accuracy: 0.9798 - val_loss: 0.4311 - val_accuracy: 0.8889 - 20s/epoch - 51ms/step\n",
      "Epoch 141/200\n",
      "390/390 - 21s - loss: 0.0598 - accuracy: 0.9795 - val_loss: 0.5323 - val_accuracy: 0.8617 - 21s/epoch - 53ms/step\n",
      "Epoch 142/200\n",
      "390/390 - 20s - loss: 0.0576 - accuracy: 0.9804 - val_loss: 0.3377 - val_accuracy: 0.9100 - 20s/epoch - 51ms/step\n",
      "Epoch 143/200\n",
      "390/390 - 20s - loss: 0.0560 - accuracy: 0.9804 - val_loss: 0.3984 - val_accuracy: 0.8948 - 20s/epoch - 51ms/step\n",
      "Epoch 144/200\n",
      "390/390 - 20s - loss: 0.0594 - accuracy: 0.9800 - val_loss: 0.3867 - val_accuracy: 0.8944 - 20s/epoch - 50ms/step\n",
      "Epoch 145/200\n",
      "390/390 - 21s - loss: 0.0561 - accuracy: 0.9814 - val_loss: 0.3365 - val_accuracy: 0.9103 - 21s/epoch - 53ms/step\n",
      "Epoch 146/200\n",
      "390/390 - 20s - loss: 0.0569 - accuracy: 0.9804 - val_loss: 0.3355 - val_accuracy: 0.9051 - 20s/epoch - 51ms/step\n",
      "Epoch 147/200\n",
      "390/390 - 20s - loss: 0.0580 - accuracy: 0.9802 - val_loss: 0.3329 - val_accuracy: 0.9076 - 20s/epoch - 51ms/step\n",
      "Epoch 148/200\n",
      "390/390 - 21s - loss: 0.0564 - accuracy: 0.9810 - val_loss: 0.3433 - val_accuracy: 0.9094 - 21s/epoch - 54ms/step\n",
      "Epoch 149/200\n",
      "390/390 - 21s - loss: 0.0531 - accuracy: 0.9820 - val_loss: 0.3305 - val_accuracy: 0.9101 - 21s/epoch - 54ms/step\n",
      "Epoch 150/200\n",
      "390/390 - 20s - loss: 0.0531 - accuracy: 0.9814 - val_loss: 0.3470 - val_accuracy: 0.9103 - 20s/epoch - 52ms/step\n",
      "Epoch 151/200\n",
      "390/390 - 21s - loss: 0.0553 - accuracy: 0.9804 - val_loss: 0.3561 - val_accuracy: 0.9021 - 21s/epoch - 53ms/step\n",
      "Epoch 152/200\n",
      "390/390 - 20s - loss: 0.0516 - accuracy: 0.9825 - val_loss: 0.3272 - val_accuracy: 0.9094 - 20s/epoch - 52ms/step\n",
      "Epoch 153/200\n",
      "390/390 - 21s - loss: 0.0518 - accuracy: 0.9829 - val_loss: 0.3141 - val_accuracy: 0.9169 - 21s/epoch - 54ms/step\n",
      "Epoch 154/200\n",
      "390/390 - 20s - loss: 0.0496 - accuracy: 0.9830 - val_loss: 0.3031 - val_accuracy: 0.9173 - 20s/epoch - 51ms/step\n",
      "Epoch 155/200\n",
      "390/390 - 21s - loss: 0.0488 - accuracy: 0.9831 - val_loss: 0.2851 - val_accuracy: 0.9200 - 21s/epoch - 53ms/step\n",
      "Epoch 156/200\n",
      "390/390 - 20s - loss: 0.0488 - accuracy: 0.9832 - val_loss: 0.3885 - val_accuracy: 0.8991 - 20s/epoch - 51ms/step\n",
      "Epoch 157/200\n",
      "390/390 - 21s - loss: 0.0453 - accuracy: 0.9847 - val_loss: 0.3237 - val_accuracy: 0.9134 - 21s/epoch - 53ms/step\n",
      "Epoch 158/200\n",
      "390/390 - 20s - loss: 0.0459 - accuracy: 0.9839 - val_loss: 0.3099 - val_accuracy: 0.9145 - 20s/epoch - 51ms/step\n",
      "Epoch 159/200\n",
      "390/390 - 21s - loss: 0.0448 - accuracy: 0.9849 - val_loss: 0.3592 - val_accuracy: 0.9053 - 21s/epoch - 54ms/step\n",
      "Epoch 160/200\n",
      "390/390 - 20s - loss: 0.0419 - accuracy: 0.9857 - val_loss: 0.2964 - val_accuracy: 0.9209 - 20s/epoch - 51ms/step\n",
      "Epoch 161/200\n",
      "390/390 - 20s - loss: 0.0374 - accuracy: 0.9879 - val_loss: 0.3116 - val_accuracy: 0.9157 - 20s/epoch - 51ms/step\n",
      "Epoch 162/200\n",
      "390/390 - 20s - loss: 0.0443 - accuracy: 0.9848 - val_loss: 0.3229 - val_accuracy: 0.9126 - 20s/epoch - 52ms/step\n",
      "Epoch 163/200\n",
      "390/390 - 20s - loss: 0.0427 - accuracy: 0.9857 - val_loss: 0.3602 - val_accuracy: 0.9025 - 20s/epoch - 50ms/step\n",
      "Epoch 164/200\n",
      "390/390 - 21s - loss: 0.0403 - accuracy: 0.9867 - val_loss: 0.3248 - val_accuracy: 0.9096 - 21s/epoch - 53ms/step\n",
      "Epoch 165/200\n",
      "390/390 - 20s - loss: 0.0390 - accuracy: 0.9875 - val_loss: 0.2956 - val_accuracy: 0.9181 - 20s/epoch - 51ms/step\n",
      "Epoch 166/200\n",
      "390/390 - 21s - loss: 0.0367 - accuracy: 0.9882 - val_loss: 0.2764 - val_accuracy: 0.9245 - 21s/epoch - 54ms/step\n",
      "Epoch 167/200\n",
      "390/390 - 21s - loss: 0.0377 - accuracy: 0.9878 - val_loss: 0.2959 - val_accuracy: 0.9216 - 21s/epoch - 53ms/step\n",
      "Epoch 168/200\n",
      "390/390 - 21s - loss: 0.0338 - accuracy: 0.9890 - val_loss: 0.3082 - val_accuracy: 0.9183 - 21s/epoch - 53ms/step\n",
      "Epoch 169/200\n",
      "390/390 - 21s - loss: 0.0343 - accuracy: 0.9887 - val_loss: 0.2802 - val_accuracy: 0.9234 - 21s/epoch - 53ms/step\n",
      "Epoch 170/200\n",
      "390/390 - 20s - loss: 0.0339 - accuracy: 0.9885 - val_loss: 0.3144 - val_accuracy: 0.9153 - 20s/epoch - 51ms/step\n",
      "Epoch 171/200\n",
      "390/390 - 20s - loss: 0.0338 - accuracy: 0.9886 - val_loss: 0.3216 - val_accuracy: 0.9157 - 20s/epoch - 51ms/step\n",
      "Epoch 172/200\n",
      "390/390 - 21s - loss: 0.0301 - accuracy: 0.9905 - val_loss: 0.3118 - val_accuracy: 0.9164 - 21s/epoch - 55ms/step\n",
      "Epoch 173/200\n",
      "390/390 - 20s - loss: 0.0271 - accuracy: 0.9914 - val_loss: 0.2911 - val_accuracy: 0.9249 - 20s/epoch - 51ms/step\n",
      "Epoch 174/200\n",
      "390/390 - 20s - loss: 0.0266 - accuracy: 0.9921 - val_loss: 0.2897 - val_accuracy: 0.9263 - 20s/epoch - 51ms/step\n",
      "Epoch 175/200\n",
      "390/390 - 20s - loss: 0.0274 - accuracy: 0.9915 - val_loss: 0.2849 - val_accuracy: 0.9254 - 20s/epoch - 51ms/step\n",
      "Epoch 176/200\n",
      "390/390 - 21s - loss: 0.0260 - accuracy: 0.9918 - val_loss: 0.2795 - val_accuracy: 0.9285 - 21s/epoch - 53ms/step\n",
      "Epoch 177/200\n",
      "390/390 - 20s - loss: 0.0230 - accuracy: 0.9931 - val_loss: 0.2668 - val_accuracy: 0.9312 - 20s/epoch - 51ms/step\n",
      "Epoch 178/200\n",
      "390/390 - 21s - loss: 0.0203 - accuracy: 0.9939 - val_loss: 0.2503 - val_accuracy: 0.9333 - 21s/epoch - 55ms/step\n",
      "Epoch 179/200\n",
      "390/390 - 20s - loss: 0.0181 - accuracy: 0.9949 - val_loss: 0.2487 - val_accuracy: 0.9322 - 20s/epoch - 51ms/step\n",
      "Epoch 180/200\n",
      "390/390 - 21s - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.2363 - val_accuracy: 0.9379 - 21s/epoch - 53ms/step\n",
      "Epoch 181/200\n",
      "390/390 - 21s - loss: 0.0141 - accuracy: 0.9962 - val_loss: 0.2647 - val_accuracy: 0.9322 - 21s/epoch - 53ms/step\n",
      "Epoch 182/200\n",
      "390/390 - 21s - loss: 0.0140 - accuracy: 0.9959 - val_loss: 0.2704 - val_accuracy: 0.9289 - 21s/epoch - 53ms/step\n",
      "Epoch 183/200\n",
      "390/390 - 21s - loss: 0.0138 - accuracy: 0.9964 - val_loss: 0.2784 - val_accuracy: 0.9288 - 21s/epoch - 55ms/step\n",
      "Epoch 184/200\n",
      "390/390 - 20s - loss: 0.0124 - accuracy: 0.9970 - val_loss: 0.2273 - val_accuracy: 0.9372 - 20s/epoch - 51ms/step\n",
      "Epoch 185/200\n",
      "390/390 - 20s - loss: 0.0105 - accuracy: 0.9975 - val_loss: 0.2449 - val_accuracy: 0.9349 - 20s/epoch - 51ms/step\n",
      "Epoch 186/200\n",
      "390/390 - 21s - loss: 0.0085 - accuracy: 0.9984 - val_loss: 0.2355 - val_accuracy: 0.9360 - 21s/epoch - 53ms/step\n",
      "Epoch 187/200\n",
      "390/390 - 21s - loss: 0.0092 - accuracy: 0.9983 - val_loss: 0.2111 - val_accuracy: 0.9419 - 21s/epoch - 53ms/step\n",
      "Epoch 188/200\n",
      "390/390 - 20s - loss: 0.0090 - accuracy: 0.9983 - val_loss: 0.2151 - val_accuracy: 0.9394 - 20s/epoch - 50ms/step\n",
      "Epoch 189/200\n",
      "390/390 - 21s - loss: 0.0085 - accuracy: 0.9986 - val_loss: 0.2165 - val_accuracy: 0.9415 - 21s/epoch - 54ms/step\n",
      "Epoch 190/200\n",
      "390/390 - 21s - loss: 0.0078 - accuracy: 0.9988 - val_loss: 0.2045 - val_accuracy: 0.9413 - 21s/epoch - 53ms/step\n",
      "Epoch 191/200\n",
      "390/390 - 20s - loss: 0.0083 - accuracy: 0.9989 - val_loss: 0.1998 - val_accuracy: 0.9440 - 20s/epoch - 53ms/step\n",
      "Epoch 192/200\n",
      "390/390 - 21s - loss: 0.0079 - accuracy: 0.9992 - val_loss: 0.1965 - val_accuracy: 0.9437 - 21s/epoch - 53ms/step\n",
      "Epoch 193/200\n",
      "390/390 - 21s - loss: 0.0088 - accuracy: 0.9990 - val_loss: 0.1871 - val_accuracy: 0.9454 - 21s/epoch - 53ms/step\n",
      "Epoch 194/200\n",
      "390/390 - 20s - loss: 0.0088 - accuracy: 0.9993 - val_loss: 0.1828 - val_accuracy: 0.9458 - 20s/epoch - 51ms/step\n",
      "Epoch 195/200\n",
      "390/390 - 20s - loss: 0.0101 - accuracy: 0.9994 - val_loss: 0.1806 - val_accuracy: 0.9475 - 20s/epoch - 52ms/step\n",
      "Epoch 196/200\n",
      "390/390 - 20s - loss: 0.0120 - accuracy: 0.9992 - val_loss: 0.1808 - val_accuracy: 0.9472 - 20s/epoch - 52ms/step\n",
      "Epoch 197/200\n",
      "390/390 - 21s - loss: 0.0140 - accuracy: 0.9993 - val_loss: 0.1786 - val_accuracy: 0.9460 - 21s/epoch - 53ms/step\n",
      "Epoch 198/200\n",
      "390/390 - 21s - loss: 0.0166 - accuracy: 0.9995 - val_loss: 0.1769 - val_accuracy: 0.9461 - 21s/epoch - 53ms/step\n",
      "Epoch 199/200\n",
      "390/390 - 21s - loss: 0.0207 - accuracy: 0.9993 - val_loss: 0.1764 - val_accuracy: 0.9469 - 21s/epoch - 53ms/step\n",
      "Epoch 200/200\n",
      "390/390 - 20s - loss: 0.0266 - accuracy: 0.9992 - val_loss: 0.1794 - val_accuracy: 0.9468 - 20s/epoch - 51ms/step\n",
      " Using best val_acc=0.9475 from last 20 epochs\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.1806 - accuracy: 0.9475\n",
      " Finished: Lam=0.7, Repeat=3, Acc=0.9475\n"
     ]
    }
   ],
   "source": [
    "for lam_idx in range(start_lr_idx, len(Lam)):\n",
    "    lam = Lam[lam_idx]\n",
    "    for rep in range(start_repeat, repeats):\n",
    "        print(f\"\\n lambda: Lam={lam}, Repeat={rep+1}/{repeats}\")\n",
    "        if progress[\"Cri_exist\"] == 0:\n",
    "            model=load_Res()\n",
    "            weight_list,channel_label=prune_model(model,G,P,x_dist,y_dist,lam)\n",
    "            model_p=model_pr(model,weight_list,channel_label)\n",
    "            flops_p,par_p=compute_flops_params(model_p)\n",
    "            P_=par_p/par\n",
    "            F=flops_p/flops\n",
    "            C_P=channel_G(model_p)\n",
    "            print(flops_p,flops)\n",
    "            progress[\"P_list\"].append(P_)\n",
    "            progress[\"F_list\"].append([flops_p,F])\n",
    "            progress[\"C_list\"].append([C_P])\n",
    "            progress[\"Cri_exist\"] = 1\n",
    "            save_progress(progress)\n",
    "            model_p.save(\"Res_18_pruned.h5\")\n",
    "        else:\n",
    "            model_p=tf.keras.models.load_model('Res_18_pruned.h5',custom_objects={\n",
    "                'CustomWeightDecaySGD': CustomWeightDecaySGD,\n",
    "                'WarmUpCosine': WarmUpCosine})\n",
    "            flops_p,par_p=compute_flops_params(model_p)\n",
    "            F=flops_p/flops\n",
    "            print(flops_p,flops)\n",
    "        retrain(model_p,x_train,y_train_onehot,x_test,y_test_onehot)\n",
    "        loss_p, acc_p = model_p.evaluate(x_test, y_test_onehot)\n",
    "        print(f\" Finished: Lam={lam}, Repeat={rep+1}, Acc={acc_p:.4f}\")\n",
    "        progress[\"results\"].append(acc_p)\n",
    "        progress[\"last_lam_idx\"] = lam_idx\n",
    "        progress[\"last_repeat\"] = rep+1\n",
    "        save_progress(progress)\n",
    "    progress[\"E_list\"].append(sum(progress[\"results\"])/(repeats*acc))\n",
    "    progress[\"results\"]=[]\n",
    "    progress[\"Cri_exist\"] = 0\n",
    "    progress[\"last_repeat\"] = 0\n",
    "    progress[\"last_lam_idx\"] = lam_idx + 1\n",
    "    start_repeat=0\n",
    "    save_progress(progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c12f34-8140-4f10-9042-9930a4affa81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f88fb92-3b19-446f-a27c-d562646c77da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9424]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.9424,0.9406,0.9391]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ca786d3-3ce5-459d-ba20-b3c56e231a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9421, 0.9449, 0.9433]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.9421,0.9449,0.9433]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0497c224-dee8-4e14-beba-c65580ab96f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9451, 0.9466, 0.9461]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.9451,0.9466,0.9461]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa898ed-bfa6-4f97-8812-c3b919254417",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.9457,0.9452,0.9475]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7637e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3254720f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d05cea08-167f-4079-973e-c2499f1e4adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.21097767067728945,\n",
       " 0.3448167909478093,\n",
       " 0.5083822068993015,\n",
       " 0.570894501373188]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress[\"P_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb2b2a78-8962-4f9c-9dc6-02cecef3442d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[132506468, 0.23589660660442366],\n",
       " [199343756, 0.35488468070992746],\n",
       " [269001286, 0.4788935325000592],\n",
       " [321009422, 0.5714817886312344]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress[\"F_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abab6d05-d731-40a3-966b-18fc8ab29075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9966098160812437,\n",
       " 0.9995055994940815,\n",
       " 1.0021542037030278,\n",
       " 1.0023660836201163]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress[\"E_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4bf44490-4833-4b01-a8fb-567c61a41fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[22, 39, 102, 69, 66, 180, 126, 118, 378, 224, 281, 606]],\n",
       " [[25, 44, 120, 78, 83, 231, 156, 152, 462, 269, 367, 810]],\n",
       " [[26, 51, 126, 89, 91, 288, 191, 175, 528, 336, 420, 1053]],\n",
       " [[34, 53, 141, 94, 101, 321, 210, 189, 588, 382, 463, 1017]]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress[\"C_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20508db3-3cc6-4ef0-880e-829b7973a951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

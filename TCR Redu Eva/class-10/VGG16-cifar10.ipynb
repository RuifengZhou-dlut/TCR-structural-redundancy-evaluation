{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe0bfee-94f9-4bfe-b5ba-3dbd7a1db50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers,regularizers,metrics,optimizers\n",
    "import random\n",
    "import pandas as pd\n",
    "from scipy.linalg import sqrtm\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f6e232e-8349-497f-8b0f-b9fe83802773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "import os\n",
    "config=tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "config.gpu_options.allow_growth=True\n",
    "sess=tf.compat.v1.Session(config=config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2c2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code is used to train VGGNet-16 using the complete CIFAR-10 dataset. \n",
    "It employs an explicit Weight Decay SGD optimizer along with WarmUp + \n",
    "Cosine Annealing Learning Rate Scheduler, and enhances the generalization \n",
    "ability of the trained network through data augmentation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00756899-de82-4707-8d3e-dfc8e029a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "initial_lr = 0.1\n",
    "weight_decay = 1e-4\n",
    "epochs = 200\n",
    "warmup_epochs = 5\n",
    "batch_size = 128\n",
    "image_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac50bd07-9c9f-4597-aa51-8185045694b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "y_train_onehot=tf.keras.utils.to_categorical(y_train,num_classes=10)\n",
    "y_test_onehot=tf.keras.utils.to_categorical(y_test,num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "037d6fdb-10b8-4085-89e7-3740e5bf4836",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_16=[64,64,128,128,256,256,256,512,512,512,512,512,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b906167-e420-495e-9178-1fb8b42903e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vggnet16(NN,input_shape=(32,32,3),num_class=10):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=NN[0], kernel_size=(3, 3), padding='same'\n",
    "                            ,input_shape=input_shape,use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(layers.Conv2D(filters=NN[1], kernel_size=(3, 3), padding='same',use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    \n",
    "    #2\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Conv2D(NN[2], (3, 3), padding='same',use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(layers.Conv2D(NN[3], (3, 3), padding='same',use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    #5\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Conv2D(NN[4], (3, 3), padding='same',use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(layers.Conv2D(NN[5], (3, 3), padding='same',use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization()) \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(layers.Conv2D(NN[6], (3, 3), padding='same',use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization()) \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    #10\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Conv2D(NN[7], (3, 3), padding='same',use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(layers.Conv2D(NN[8], (3, 3), padding='same',use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(layers.Conv2D(NN[9], (3, 3), padding='same',use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    #15\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Conv2D(NN[10], (3, 3), padding='same',use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(layers.Conv2D(NN[11], (3, 3), padding='same',use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(layers.Conv2D(NN[12], (3, 3), padding='same',use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    #\n",
    "    #25\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(NN[12]))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(layers.Dense(NN[12]))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(layers.Dense(num_class,activation='softmax'))\n",
    "    return model\n",
    "model = build_vggnet16(NN_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6de5d87e-8b19-4706-9888-f2c67d3f1855",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_vars = []\n",
    "no_decay_vars = []\n",
    "for var in model.trainable_variables:\n",
    "    if 'kernel' in var.name and 'bn' not in var.name.lower():\n",
    "        decay_vars.append(var)\n",
    "    else:\n",
    "        no_decay_vars.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef4de869-d593-45ec-9f2d-178901b461cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "   WarmUp + Cosine Annealing Learning Rate Scheduler \n",
    "   This scheduler divides the training process into two stages:\n",
    "   1) Warm-up stage (step < warmup_steps):\n",
    "   The learning rate linearly increases from warmup_lr to base_lr, \n",
    "   aiming to alleviate the instability of gradients or excessive \n",
    "   parameter updates at the beginning of training, thereby enhancing \n",
    "   the stability of the training process.\n",
    "   2) Cosine Annealing stage (step >= warmup_steps):\n",
    "   The learning rate smoothly decays from base_lr to a value close \n",
    "   to 0 according to the cosine function, which is helpful for \n",
    "   conducting more precise parameter search in the later stage of \n",
    "   training and improving the final convergence performance. \n",
    "   \"\"\"\n",
    "    def __init__(self, base_lr, total_steps, warmup_steps, warmup_lr=0.0):\n",
    "        \"\"\"\n",
    "        Parameter description:\n",
    "        - base_lr: The maximum learning rate to be used after the warm-up period\n",
    "        - total_steps: The total number of steps for training (epochs Ã— steps_per_epoch)\n",
    "        - warmup_steps: The number of steps in the warm-up phase\n",
    "        - warmup_lr: The initial learning rate for warm-up, default is 0\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.warmup_lr = warmup_lr\n",
    "    def __call__(self, step):\n",
    "        if step is None:\n",
    "            step = tf.constant(0)\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        total_steps = tf.cast(self.total_steps, tf.float32)\n",
    "        warmup_percent_done = step / warmup_steps\n",
    "        learning_rate = tf.where(\n",
    "            step < warmup_steps,\n",
    "            self.warmup_lr + (self.base_lr - self.warmup_lr) * warmup_percent_done,\n",
    "            self.base_lr * 0.5 * (1.0 + tf.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps)))\n",
    "        )\n",
    "        return learning_rate\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"base_lr\": self.base_lr,\n",
    "            \"total_steps\": self.total_steps,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"warmup_lr\": self.warmup_lr,\n",
    "        }\n",
    "total_steps = epochs * (x_train.shape[0] // batch_size)\n",
    "warmup_steps = warmup_epochs * (x_train.shape[0] // batch_size)\n",
    "lr_schedule = WarmUpCosine(initial_lr, total_steps, warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de2afd73-796d-4384-8fd6-1b240f9f7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWeightDecaySGD(tf.keras.optimizers.SGD):\n",
    "    \"\"\"\n",
    "    SGD optimizer with explicit Weight Decay (Decoupled Weight Decay) \n",
    "    This optimizer, based on the standard SGD, manually applies weight \n",
    "    decay to the parameters, instead of achieving it through the L2 \n",
    "    regularization term in the loss. This \"decoupled weight decay\"\n",
    "    is in line with the idea of AdamW/SGDW and can avoid the problem \n",
    "    that weight decay is indirectly scaled by factors such as learning \n",
    "    rate and momentum. \n",
    "    \"\"\"\n",
    "    def __init__(self, weight_decay, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameter description:\n",
    "        - weight_decay: Weight decay coefficient (usually ranging from 1e-4 to 1e-2)\n",
    "        - **kwargs: Other parameters are directly passed to tf.keras.optimizers.SGD,\n",
    "        such as learning_rate, momentum, nesterov, etc. \n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.weight_decay = weight_decay\n",
    "    def apply_gradients(self, grads_and_vars, name=None, experimental_aggregate_gradients=True):\n",
    "        super().apply_gradients(grads_and_vars, name, experimental_aggregate_gradients)\n",
    "        for grad, var in grads_and_vars:\n",
    "            if ('kernel' in var.name) and ('bn' not in var.name.lower()):\n",
    "                var.assign_sub(self.weight_decay * var)\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"weight_decay\": float(self.weight_decay), \n",
    "        })\n",
    "        return config\n",
    "optimizer = CustomWeightDecaySGD(\n",
    "    weight_decay=weight_decay,\n",
    "    learning_rate=lr_schedule,\n",
    "    momentum=0.9,\n",
    "    nesterov=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fc4dd03-6492-4ed1-852d-968e81e6dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=tf.keras.losses.CategoricalCrossentropy()\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e1f3333-9a1b-4adc-9ef5-6c037da9cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcf05ee3-ab09-4c46-a896-1d968c3e9cd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "390/390 - 40s - loss: 1.5427 - accuracy: 0.4429 - val_loss: 1.9395 - val_accuracy: 0.4546 - 40s/epoch - 102ms/step\n",
      "Epoch 2/200\n",
      "390/390 - 22s - loss: 1.0295 - accuracy: 0.6403 - val_loss: 1.5705 - val_accuracy: 0.5667 - 22s/epoch - 56ms/step\n",
      "Epoch 3/200\n",
      "390/390 - 22s - loss: 0.8367 - accuracy: 0.7148 - val_loss: 1.6181 - val_accuracy: 0.5931 - 22s/epoch - 57ms/step\n",
      "Epoch 4/200\n",
      "390/390 - 22s - loss: 0.7372 - accuracy: 0.7498 - val_loss: 1.0389 - val_accuracy: 0.6706 - 22s/epoch - 57ms/step\n",
      "Epoch 5/200\n",
      "390/390 - 23s - loss: 0.6650 - accuracy: 0.7756 - val_loss: 0.9519 - val_accuracy: 0.6912 - 23s/epoch - 58ms/step\n",
      "Epoch 6/200\n",
      "390/390 - 23s - loss: 0.5960 - accuracy: 0.7973 - val_loss: 0.6528 - val_accuracy: 0.7832 - 23s/epoch - 58ms/step\n",
      "Epoch 7/200\n",
      "390/390 - 23s - loss: 0.5257 - accuracy: 0.8228 - val_loss: 0.6163 - val_accuracy: 0.7889 - 23s/epoch - 58ms/step\n",
      "Epoch 8/200\n",
      "390/390 - 23s - loss: 0.4828 - accuracy: 0.8332 - val_loss: 0.7178 - val_accuracy: 0.7589 - 23s/epoch - 58ms/step\n",
      "Epoch 9/200\n",
      "390/390 - 23s - loss: 0.5390 - accuracy: 0.8202 - val_loss: 0.8785 - val_accuracy: 0.7405 - 23s/epoch - 58ms/step\n",
      "Epoch 10/200\n",
      "390/390 - 23s - loss: 0.4611 - accuracy: 0.8446 - val_loss: 0.7107 - val_accuracy: 0.7575 - 23s/epoch - 58ms/step\n",
      "Epoch 11/200\n",
      "390/390 - 23s - loss: 0.4115 - accuracy: 0.8604 - val_loss: 0.4883 - val_accuracy: 0.8401 - 23s/epoch - 58ms/step\n",
      "Epoch 12/200\n",
      "390/390 - 23s - loss: 0.3777 - accuracy: 0.8716 - val_loss: 0.5686 - val_accuracy: 0.8187 - 23s/epoch - 58ms/step\n",
      "Epoch 13/200\n",
      "390/390 - 23s - loss: 0.3562 - accuracy: 0.8781 - val_loss: 0.5802 - val_accuracy: 0.8150 - 23s/epoch - 58ms/step\n",
      "Epoch 14/200\n",
      "390/390 - 23s - loss: 0.3386 - accuracy: 0.8831 - val_loss: 0.6929 - val_accuracy: 0.7778 - 23s/epoch - 58ms/step\n",
      "Epoch 15/200\n",
      "390/390 - 22s - loss: 0.3225 - accuracy: 0.8905 - val_loss: 0.5213 - val_accuracy: 0.8217 - 22s/epoch - 57ms/step\n",
      "Epoch 16/200\n",
      "390/390 - 23s - loss: 0.3053 - accuracy: 0.8960 - val_loss: 0.4216 - val_accuracy: 0.8580 - 23s/epoch - 58ms/step\n",
      "Epoch 17/200\n",
      "390/390 - 23s - loss: 0.2986 - accuracy: 0.8963 - val_loss: 0.6984 - val_accuracy: 0.7884 - 23s/epoch - 59ms/step\n",
      "Epoch 18/200\n",
      "390/390 - 22s - loss: 0.2825 - accuracy: 0.9029 - val_loss: 0.5421 - val_accuracy: 0.8248 - 22s/epoch - 58ms/step\n",
      "Epoch 19/200\n",
      "390/390 - 22s - loss: 0.2769 - accuracy: 0.9055 - val_loss: 0.4174 - val_accuracy: 0.8637 - 22s/epoch - 57ms/step\n",
      "Epoch 20/200\n",
      "390/390 - 23s - loss: 0.2692 - accuracy: 0.9082 - val_loss: 0.4610 - val_accuracy: 0.8552 - 23s/epoch - 58ms/step\n",
      "Epoch 21/200\n",
      "390/390 - 22s - loss: 0.2559 - accuracy: 0.9119 - val_loss: 0.4267 - val_accuracy: 0.8593 - 22s/epoch - 58ms/step\n",
      "Epoch 22/200\n",
      "390/390 - 23s - loss: 0.2539 - accuracy: 0.9136 - val_loss: 0.7264 - val_accuracy: 0.7887 - 23s/epoch - 59ms/step\n",
      "Epoch 23/200\n",
      "390/390 - 23s - loss: 0.2462 - accuracy: 0.9158 - val_loss: 0.4225 - val_accuracy: 0.8683 - 23s/epoch - 58ms/step\n",
      "Epoch 24/200\n",
      "390/390 - 23s - loss: 0.2366 - accuracy: 0.9189 - val_loss: 0.5220 - val_accuracy: 0.8408 - 23s/epoch - 58ms/step\n",
      "Epoch 25/200\n",
      "390/390 - 23s - loss: 0.2316 - accuracy: 0.9200 - val_loss: 0.4223 - val_accuracy: 0.8629 - 23s/epoch - 58ms/step\n",
      "Epoch 26/200\n",
      "390/390 - 23s - loss: 0.2272 - accuracy: 0.9226 - val_loss: 0.6211 - val_accuracy: 0.8215 - 23s/epoch - 59ms/step\n",
      "Epoch 27/200\n",
      "390/390 - 22s - loss: 0.2220 - accuracy: 0.9252 - val_loss: 0.4686 - val_accuracy: 0.8491 - 22s/epoch - 58ms/step\n",
      "Epoch 28/200\n",
      "390/390 - 23s - loss: 0.2130 - accuracy: 0.9271 - val_loss: 0.4760 - val_accuracy: 0.8530 - 23s/epoch - 59ms/step\n",
      "Epoch 29/200\n",
      "390/390 - 23s - loss: 0.2065 - accuracy: 0.9285 - val_loss: 0.4407 - val_accuracy: 0.8621 - 23s/epoch - 58ms/step\n",
      "Epoch 30/200\n",
      "390/390 - 23s - loss: 0.2074 - accuracy: 0.9285 - val_loss: 0.5446 - val_accuracy: 0.8357 - 23s/epoch - 59ms/step\n",
      "Epoch 31/200\n",
      "390/390 - 23s - loss: 0.2022 - accuracy: 0.9310 - val_loss: 0.4212 - val_accuracy: 0.8685 - 23s/epoch - 58ms/step\n",
      "Epoch 32/200\n",
      "390/390 - 22s - loss: 0.1973 - accuracy: 0.9315 - val_loss: 0.4780 - val_accuracy: 0.8525 - 22s/epoch - 58ms/step\n",
      "Epoch 33/200\n",
      "390/390 - 23s - loss: 0.1898 - accuracy: 0.9341 - val_loss: 0.6223 - val_accuracy: 0.8162 - 23s/epoch - 60ms/step\n",
      "Epoch 34/200\n",
      "390/390 - 22s - loss: 0.1877 - accuracy: 0.9354 - val_loss: 0.4370 - val_accuracy: 0.8689 - 22s/epoch - 57ms/step\n",
      "Epoch 35/200\n",
      "390/390 - 23s - loss: 0.1886 - accuracy: 0.9350 - val_loss: 0.3819 - val_accuracy: 0.8800 - 23s/epoch - 58ms/step\n",
      "Epoch 36/200\n",
      "390/390 - 23s - loss: 0.1865 - accuracy: 0.9359 - val_loss: 0.4795 - val_accuracy: 0.8491 - 23s/epoch - 59ms/step\n",
      "Epoch 37/200\n",
      "390/390 - 22s - loss: 0.1838 - accuracy: 0.9378 - val_loss: 0.4320 - val_accuracy: 0.8682 - 22s/epoch - 57ms/step\n",
      "Epoch 38/200\n",
      "390/390 - 23s - loss: 0.1785 - accuracy: 0.9396 - val_loss: 0.5667 - val_accuracy: 0.8379 - 23s/epoch - 58ms/step\n",
      "Epoch 39/200\n",
      "390/390 - 23s - loss: 0.1739 - accuracy: 0.9400 - val_loss: 0.5732 - val_accuracy: 0.8288 - 23s/epoch - 58ms/step\n",
      "Epoch 40/200\n",
      "390/390 - 22s - loss: 0.1714 - accuracy: 0.9401 - val_loss: 0.4519 - val_accuracy: 0.8660 - 22s/epoch - 58ms/step\n",
      "Epoch 41/200\n",
      "390/390 - 23s - loss: 0.1691 - accuracy: 0.9423 - val_loss: 0.4351 - val_accuracy: 0.8678 - 23s/epoch - 58ms/step\n",
      "Epoch 42/200\n",
      "390/390 - 23s - loss: 0.1673 - accuracy: 0.9430 - val_loss: 0.5128 - val_accuracy: 0.8475 - 23s/epoch - 58ms/step\n",
      "Epoch 43/200\n",
      "390/390 - 23s - loss: 0.1669 - accuracy: 0.9429 - val_loss: 0.4529 - val_accuracy: 0.8590 - 23s/epoch - 58ms/step\n",
      "Epoch 44/200\n",
      "390/390 - 22s - loss: 0.1619 - accuracy: 0.9439 - val_loss: 0.4788 - val_accuracy: 0.8644 - 22s/epoch - 58ms/step\n",
      "Epoch 45/200\n",
      "390/390 - 22s - loss: 0.1705 - accuracy: 0.9414 - val_loss: 0.4500 - val_accuracy: 0.8628 - 22s/epoch - 57ms/step\n",
      "Epoch 46/200\n",
      "390/390 - 23s - loss: 0.1612 - accuracy: 0.9455 - val_loss: 0.6328 - val_accuracy: 0.8296 - 23s/epoch - 58ms/step\n",
      "Epoch 47/200\n",
      "390/390 - 23s - loss: 0.1596 - accuracy: 0.9451 - val_loss: 0.4876 - val_accuracy: 0.8537 - 23s/epoch - 58ms/step\n",
      "Epoch 48/200\n",
      "390/390 - 22s - loss: 0.1584 - accuracy: 0.9463 - val_loss: 0.4352 - val_accuracy: 0.8619 - 22s/epoch - 57ms/step\n",
      "Epoch 49/200\n",
      "390/390 - 22s - loss: 0.1564 - accuracy: 0.9477 - val_loss: 0.6515 - val_accuracy: 0.8133 - 22s/epoch - 57ms/step\n",
      "Epoch 50/200\n",
      "390/390 - 23s - loss: 0.1569 - accuracy: 0.9456 - val_loss: 0.4249 - val_accuracy: 0.8748 - 23s/epoch - 58ms/step\n",
      "Epoch 51/200\n",
      "390/390 - 22s - loss: 0.1522 - accuracy: 0.9469 - val_loss: 0.4040 - val_accuracy: 0.8785 - 22s/epoch - 57ms/step\n",
      "Epoch 52/200\n",
      "390/390 - 22s - loss: 0.1482 - accuracy: 0.9490 - val_loss: 0.5999 - val_accuracy: 0.8322 - 22s/epoch - 57ms/step\n",
      "Epoch 53/200\n",
      "390/390 - 22s - loss: 0.1496 - accuracy: 0.9494 - val_loss: 0.4084 - val_accuracy: 0.8780 - 22s/epoch - 57ms/step\n",
      "Epoch 54/200\n",
      "390/390 - 23s - loss: 0.1491 - accuracy: 0.9491 - val_loss: 0.5022 - val_accuracy: 0.8576 - 23s/epoch - 58ms/step\n",
      "Epoch 55/200\n",
      "390/390 - 23s - loss: 0.1473 - accuracy: 0.9491 - val_loss: 0.4604 - val_accuracy: 0.8625 - 23s/epoch - 59ms/step\n",
      "Epoch 56/200\n",
      "390/390 - 23s - loss: 0.1424 - accuracy: 0.9513 - val_loss: 0.4986 - val_accuracy: 0.8519 - 23s/epoch - 59ms/step\n",
      "Epoch 57/200\n",
      "390/390 - 22s - loss: 0.1459 - accuracy: 0.9495 - val_loss: 0.4031 - val_accuracy: 0.8783 - 22s/epoch - 58ms/step\n",
      "Epoch 58/200\n",
      "390/390 - 22s - loss: 0.1447 - accuracy: 0.9500 - val_loss: 0.5069 - val_accuracy: 0.8491 - 22s/epoch - 57ms/step\n",
      "Epoch 59/200\n",
      "390/390 - 23s - loss: 0.1437 - accuracy: 0.9508 - val_loss: 0.5324 - val_accuracy: 0.8522 - 23s/epoch - 58ms/step\n",
      "Epoch 60/200\n",
      "390/390 - 22s - loss: 0.1396 - accuracy: 0.9529 - val_loss: 0.4416 - val_accuracy: 0.8681 - 22s/epoch - 57ms/step\n",
      "Epoch 61/200\n",
      "390/390 - 23s - loss: 0.1368 - accuracy: 0.9525 - val_loss: 0.6449 - val_accuracy: 0.8197 - 23s/epoch - 58ms/step\n",
      "Epoch 62/200\n",
      "390/390 - 23s - loss: 0.1373 - accuracy: 0.9527 - val_loss: 0.4507 - val_accuracy: 0.8725 - 23s/epoch - 59ms/step\n",
      "Epoch 63/200\n",
      "390/390 - 23s - loss: 0.1317 - accuracy: 0.9549 - val_loss: 0.4141 - val_accuracy: 0.8795 - 23s/epoch - 58ms/step\n",
      "Epoch 64/200\n",
      "390/390 - 22s - loss: 0.1404 - accuracy: 0.9521 - val_loss: 0.5020 - val_accuracy: 0.8542 - 22s/epoch - 56ms/step\n",
      "Epoch 65/200\n",
      "390/390 - 23s - loss: 0.1333 - accuracy: 0.9542 - val_loss: 0.6072 - val_accuracy: 0.8382 - 23s/epoch - 58ms/step\n",
      "Epoch 66/200\n",
      "390/390 - 22s - loss: 0.1329 - accuracy: 0.9538 - val_loss: 0.4253 - val_accuracy: 0.8759 - 22s/epoch - 58ms/step\n",
      "Epoch 67/200\n",
      "390/390 - 22s - loss: 0.1322 - accuracy: 0.9555 - val_loss: 0.4800 - val_accuracy: 0.8600 - 22s/epoch - 57ms/step\n",
      "Epoch 68/200\n",
      "390/390 - 23s - loss: 0.1343 - accuracy: 0.9541 - val_loss: 0.5321 - val_accuracy: 0.8495 - 23s/epoch - 59ms/step\n",
      "Epoch 69/200\n",
      "390/390 - 23s - loss: 0.1307 - accuracy: 0.9552 - val_loss: 0.6971 - val_accuracy: 0.8033 - 23s/epoch - 59ms/step\n",
      "Epoch 70/200\n",
      "390/390 - 22s - loss: 0.1306 - accuracy: 0.9553 - val_loss: 0.4286 - val_accuracy: 0.8761 - 22s/epoch - 57ms/step\n",
      "Epoch 71/200\n",
      "390/390 - 23s - loss: 0.1298 - accuracy: 0.9556 - val_loss: 0.3746 - val_accuracy: 0.8887 - 23s/epoch - 58ms/step\n",
      "Epoch 72/200\n",
      "390/390 - 23s - loss: 0.1230 - accuracy: 0.9582 - val_loss: 0.4902 - val_accuracy: 0.8694 - 23s/epoch - 59ms/step\n",
      "Epoch 73/200\n",
      "390/390 - 22s - loss: 0.1235 - accuracy: 0.9578 - val_loss: 0.5306 - val_accuracy: 0.8544 - 22s/epoch - 57ms/step\n",
      "Epoch 74/200\n",
      "390/390 - 23s - loss: 0.1288 - accuracy: 0.9565 - val_loss: 0.6539 - val_accuracy: 0.8348 - 23s/epoch - 58ms/step\n",
      "Epoch 75/200\n",
      "390/390 - 22s - loss: 0.1286 - accuracy: 0.9566 - val_loss: 0.4762 - val_accuracy: 0.8599 - 22s/epoch - 57ms/step\n",
      "Epoch 76/200\n",
      "390/390 - 23s - loss: 0.1257 - accuracy: 0.9575 - val_loss: 0.5131 - val_accuracy: 0.8531 - 23s/epoch - 60ms/step\n",
      "Epoch 77/200\n",
      "390/390 - 23s - loss: 0.1248 - accuracy: 0.9569 - val_loss: 0.5618 - val_accuracy: 0.8468 - 23s/epoch - 58ms/step\n",
      "Epoch 78/200\n",
      "390/390 - 23s - loss: 0.1242 - accuracy: 0.9588 - val_loss: 0.5114 - val_accuracy: 0.8639 - 23s/epoch - 58ms/step\n",
      "Epoch 79/200\n",
      "390/390 - 23s - loss: 0.1231 - accuracy: 0.9579 - val_loss: 0.3910 - val_accuracy: 0.8834 - 23s/epoch - 58ms/step\n",
      "Epoch 80/200\n",
      "390/390 - 23s - loss: 0.1200 - accuracy: 0.9585 - val_loss: 0.4590 - val_accuracy: 0.8692 - 23s/epoch - 58ms/step\n",
      "Epoch 81/200\n",
      "390/390 - 23s - loss: 0.1212 - accuracy: 0.9582 - val_loss: 0.4635 - val_accuracy: 0.8721 - 23s/epoch - 59ms/step\n",
      "Epoch 82/200\n",
      "390/390 - 23s - loss: 0.1185 - accuracy: 0.9596 - val_loss: 0.5815 - val_accuracy: 0.8369 - 23s/epoch - 59ms/step\n",
      "Epoch 83/200\n",
      "390/390 - 22s - loss: 0.1235 - accuracy: 0.9585 - val_loss: 0.4886 - val_accuracy: 0.8665 - 22s/epoch - 57ms/step\n",
      "Epoch 84/200\n",
      "390/390 - 23s - loss: 0.1215 - accuracy: 0.9596 - val_loss: 0.8013 - val_accuracy: 0.7883 - 23s/epoch - 58ms/step\n",
      "Epoch 85/200\n",
      "390/390 - 22s - loss: 0.1154 - accuracy: 0.9611 - val_loss: 0.5282 - val_accuracy: 0.8559 - 22s/epoch - 57ms/step\n",
      "Epoch 86/200\n",
      "390/390 - 23s - loss: 0.1190 - accuracy: 0.9598 - val_loss: 0.4689 - val_accuracy: 0.8684 - 23s/epoch - 58ms/step\n",
      "Epoch 87/200\n",
      "390/390 - 23s - loss: 0.1129 - accuracy: 0.9613 - val_loss: 0.4672 - val_accuracy: 0.8706 - 23s/epoch - 58ms/step\n",
      "Epoch 88/200\n",
      "390/390 - 23s - loss: 0.1172 - accuracy: 0.9599 - val_loss: 0.4141 - val_accuracy: 0.8773 - 23s/epoch - 59ms/step\n",
      "Epoch 89/200\n",
      "390/390 - 23s - loss: 0.1185 - accuracy: 0.9596 - val_loss: 0.4882 - val_accuracy: 0.8669 - 23s/epoch - 59ms/step\n",
      "Epoch 90/200\n",
      "390/390 - 22s - loss: 0.1124 - accuracy: 0.9623 - val_loss: 0.3954 - val_accuracy: 0.8847 - 22s/epoch - 57ms/step\n",
      "Epoch 91/200\n",
      "390/390 - 23s - loss: 0.1158 - accuracy: 0.9606 - val_loss: 0.3694 - val_accuracy: 0.8919 - 23s/epoch - 58ms/step\n",
      "Epoch 92/200\n",
      "390/390 - 22s - loss: 0.1161 - accuracy: 0.9605 - val_loss: 0.6736 - val_accuracy: 0.8304 - 22s/epoch - 57ms/step\n",
      "Epoch 93/200\n",
      "390/390 - 22s - loss: 0.1144 - accuracy: 0.9610 - val_loss: 0.4117 - val_accuracy: 0.8819 - 22s/epoch - 57ms/step\n",
      "Epoch 94/200\n",
      "390/390 - 22s - loss: 0.1097 - accuracy: 0.9626 - val_loss: 0.5426 - val_accuracy: 0.8542 - 22s/epoch - 57ms/step\n",
      "Epoch 95/200\n",
      "390/390 - 23s - loss: 0.1118 - accuracy: 0.9620 - val_loss: 0.4726 - val_accuracy: 0.8672 - 23s/epoch - 58ms/step\n",
      "Epoch 96/200\n",
      "390/390 - 22s - loss: 0.1131 - accuracy: 0.9620 - val_loss: 0.4449 - val_accuracy: 0.8741 - 22s/epoch - 57ms/step\n",
      "Epoch 97/200\n",
      "390/390 - 22s - loss: 0.1047 - accuracy: 0.9650 - val_loss: 0.4515 - val_accuracy: 0.8746 - 22s/epoch - 57ms/step\n",
      "Epoch 98/200\n",
      "390/390 - 22s - loss: 0.1150 - accuracy: 0.9620 - val_loss: 0.4710 - val_accuracy: 0.8674 - 22s/epoch - 57ms/step\n",
      "Epoch 99/200\n",
      "390/390 - 23s - loss: 0.1056 - accuracy: 0.9633 - val_loss: 0.5090 - val_accuracy: 0.8632 - 23s/epoch - 58ms/step\n",
      "Epoch 100/200\n",
      "390/390 - 23s - loss: 0.1087 - accuracy: 0.9633 - val_loss: 0.4208 - val_accuracy: 0.8826 - 23s/epoch - 58ms/step\n",
      "Epoch 101/200\n",
      "390/390 - 23s - loss: 0.1082 - accuracy: 0.9637 - val_loss: 0.5674 - val_accuracy: 0.8556 - 23s/epoch - 59ms/step\n",
      "Epoch 102/200\n",
      "390/390 - 23s - loss: 0.1092 - accuracy: 0.9633 - val_loss: 0.3738 - val_accuracy: 0.8880 - 23s/epoch - 58ms/step\n",
      "Epoch 103/200\n",
      "390/390 - 22s - loss: 0.1076 - accuracy: 0.9633 - val_loss: 0.5585 - val_accuracy: 0.8453 - 22s/epoch - 57ms/step\n",
      "Epoch 104/200\n",
      "390/390 - 23s - loss: 0.1047 - accuracy: 0.9639 - val_loss: 0.4092 - val_accuracy: 0.8892 - 23s/epoch - 58ms/step\n",
      "Epoch 105/200\n",
      "390/390 - 22s - loss: 0.1048 - accuracy: 0.9637 - val_loss: 0.4646 - val_accuracy: 0.8740 - 22s/epoch - 57ms/step\n",
      "Epoch 106/200\n",
      "390/390 - 23s - loss: 0.1083 - accuracy: 0.9637 - val_loss: 0.5091 - val_accuracy: 0.8636 - 23s/epoch - 58ms/step\n",
      "Epoch 107/200\n",
      "390/390 - 23s - loss: 0.1028 - accuracy: 0.9643 - val_loss: 0.4280 - val_accuracy: 0.8786 - 23s/epoch - 59ms/step\n",
      "Epoch 108/200\n",
      "390/390 - 23s - loss: 0.1051 - accuracy: 0.9643 - val_loss: 0.3918 - val_accuracy: 0.8900 - 23s/epoch - 58ms/step\n",
      "Epoch 109/200\n",
      "390/390 - 22s - loss: 0.0986 - accuracy: 0.9665 - val_loss: 0.4844 - val_accuracy: 0.8653 - 22s/epoch - 57ms/step\n",
      "Epoch 110/200\n",
      "390/390 - 23s - loss: 0.1039 - accuracy: 0.9646 - val_loss: 0.4399 - val_accuracy: 0.8820 - 23s/epoch - 58ms/step\n",
      "Epoch 111/200\n",
      "390/390 - 22s - loss: 0.0971 - accuracy: 0.9665 - val_loss: 0.4471 - val_accuracy: 0.8776 - 22s/epoch - 57ms/step\n",
      "Epoch 112/200\n",
      "390/390 - 23s - loss: 0.0993 - accuracy: 0.9664 - val_loss: 0.4438 - val_accuracy: 0.8756 - 23s/epoch - 59ms/step\n",
      "Epoch 113/200\n",
      "390/390 - 22s - loss: 0.1052 - accuracy: 0.9651 - val_loss: 0.5346 - val_accuracy: 0.8523 - 22s/epoch - 57ms/step\n",
      "Epoch 114/200\n",
      "390/390 - 22s - loss: 0.1023 - accuracy: 0.9653 - val_loss: 0.3657 - val_accuracy: 0.8931 - 22s/epoch - 57ms/step\n",
      "Epoch 115/200\n",
      "390/390 - 22s - loss: 0.1016 - accuracy: 0.9664 - val_loss: 0.4132 - val_accuracy: 0.8837 - 22s/epoch - 58ms/step\n",
      "Epoch 116/200\n",
      "390/390 - 22s - loss: 0.1014 - accuracy: 0.9654 - val_loss: 0.3876 - val_accuracy: 0.8932 - 22s/epoch - 57ms/step\n",
      "Epoch 117/200\n",
      "390/390 - 23s - loss: 0.0980 - accuracy: 0.9678 - val_loss: 0.5148 - val_accuracy: 0.8634 - 23s/epoch - 59ms/step\n",
      "Epoch 118/200\n",
      "390/390 - 22s - loss: 0.0973 - accuracy: 0.9668 - val_loss: 0.4142 - val_accuracy: 0.8832 - 22s/epoch - 58ms/step\n",
      "Epoch 119/200\n",
      "390/390 - 23s - loss: 0.0944 - accuracy: 0.9678 - val_loss: 0.4199 - val_accuracy: 0.8860 - 23s/epoch - 59ms/step\n",
      "Epoch 120/200\n",
      "390/390 - 22s - loss: 0.1002 - accuracy: 0.9661 - val_loss: 0.4703 - val_accuracy: 0.8705 - 22s/epoch - 57ms/step\n",
      "Epoch 121/200\n",
      "390/390 - 23s - loss: 0.0958 - accuracy: 0.9670 - val_loss: 0.4067 - val_accuracy: 0.8812 - 23s/epoch - 58ms/step\n",
      "Epoch 122/200\n",
      "390/390 - 22s - loss: 0.0951 - accuracy: 0.9676 - val_loss: 0.4371 - val_accuracy: 0.8797 - 22s/epoch - 57ms/step\n",
      "Epoch 123/200\n",
      "390/390 - 22s - loss: 0.0946 - accuracy: 0.9685 - val_loss: 0.4055 - val_accuracy: 0.8938 - 22s/epoch - 57ms/step\n",
      "Epoch 124/200\n",
      "390/390 - 22s - loss: 0.0955 - accuracy: 0.9681 - val_loss: 0.4280 - val_accuracy: 0.8821 - 22s/epoch - 58ms/step\n",
      "Epoch 125/200\n",
      "390/390 - 23s - loss: 0.0889 - accuracy: 0.9698 - val_loss: 0.4458 - val_accuracy: 0.8820 - 23s/epoch - 59ms/step\n",
      "Epoch 126/200\n",
      "390/390 - 22s - loss: 0.0915 - accuracy: 0.9692 - val_loss: 0.3895 - val_accuracy: 0.8900 - 22s/epoch - 57ms/step\n",
      "Epoch 127/200\n",
      "390/390 - 22s - loss: 0.0903 - accuracy: 0.9700 - val_loss: 0.4812 - val_accuracy: 0.8724 - 22s/epoch - 58ms/step\n",
      "Epoch 128/200\n",
      "390/390 - 23s - loss: 0.0899 - accuracy: 0.9702 - val_loss: 0.4972 - val_accuracy: 0.8644 - 23s/epoch - 59ms/step\n",
      "Epoch 129/200\n",
      "390/390 - 22s - loss: 0.0922 - accuracy: 0.9699 - val_loss: 0.4690 - val_accuracy: 0.8784 - 22s/epoch - 57ms/step\n",
      "Epoch 130/200\n",
      "390/390 - 23s - loss: 0.0890 - accuracy: 0.9706 - val_loss: 0.6123 - val_accuracy: 0.8507 - 23s/epoch - 58ms/step\n",
      "Epoch 131/200\n",
      "390/390 - 22s - loss: 0.0920 - accuracy: 0.9690 - val_loss: 0.4383 - val_accuracy: 0.8832 - 22s/epoch - 57ms/step\n",
      "Epoch 132/200\n",
      "390/390 - 23s - loss: 0.0857 - accuracy: 0.9708 - val_loss: 0.4624 - val_accuracy: 0.8781 - 23s/epoch - 58ms/step\n",
      "Epoch 133/200\n",
      "390/390 - 22s - loss: 0.0863 - accuracy: 0.9713 - val_loss: 0.3453 - val_accuracy: 0.9000 - 22s/epoch - 57ms/step\n",
      "Epoch 134/200\n",
      "390/390 - 22s - loss: 0.0864 - accuracy: 0.9709 - val_loss: 0.4181 - val_accuracy: 0.8849 - 22s/epoch - 57ms/step\n",
      "Epoch 135/200\n",
      "390/390 - 23s - loss: 0.0867 - accuracy: 0.9710 - val_loss: 0.4741 - val_accuracy: 0.8705 - 23s/epoch - 58ms/step\n",
      "Epoch 136/200\n",
      "390/390 - 22s - loss: 0.0849 - accuracy: 0.9713 - val_loss: 0.5239 - val_accuracy: 0.8606 - 22s/epoch - 58ms/step\n",
      "Epoch 137/200\n",
      "390/390 - 23s - loss: 0.0837 - accuracy: 0.9726 - val_loss: 0.5172 - val_accuracy: 0.8593 - 23s/epoch - 58ms/step\n",
      "Epoch 138/200\n",
      "390/390 - 22s - loss: 0.0805 - accuracy: 0.9727 - val_loss: 0.4196 - val_accuracy: 0.8892 - 22s/epoch - 57ms/step\n",
      "Epoch 139/200\n",
      "390/390 - 23s - loss: 0.0799 - accuracy: 0.9727 - val_loss: 0.3848 - val_accuracy: 0.8929 - 23s/epoch - 59ms/step\n",
      "Epoch 140/200\n",
      "390/390 - 22s - loss: 0.0827 - accuracy: 0.9720 - val_loss: 0.3978 - val_accuracy: 0.8914 - 22s/epoch - 57ms/step\n",
      "Epoch 141/200\n",
      "390/390 - 23s - loss: 0.0845 - accuracy: 0.9713 - val_loss: 0.3784 - val_accuracy: 0.8933 - 23s/epoch - 58ms/step\n",
      "Epoch 142/200\n",
      "390/390 - 22s - loss: 0.0819 - accuracy: 0.9724 - val_loss: 0.3397 - val_accuracy: 0.9016 - 22s/epoch - 57ms/step\n",
      "Epoch 143/200\n",
      "390/390 - 23s - loss: 0.0797 - accuracy: 0.9735 - val_loss: 0.3452 - val_accuracy: 0.9041 - 23s/epoch - 59ms/step\n",
      "Epoch 144/200\n",
      "390/390 - 22s - loss: 0.0798 - accuracy: 0.9726 - val_loss: 0.3943 - val_accuracy: 0.8971 - 22s/epoch - 58ms/step\n",
      "Epoch 145/200\n",
      "390/390 - 22s - loss: 0.0784 - accuracy: 0.9739 - val_loss: 0.4420 - val_accuracy: 0.8820 - 22s/epoch - 57ms/step\n",
      "Epoch 146/200\n",
      "390/390 - 22s - loss: 0.0777 - accuracy: 0.9743 - val_loss: 0.4620 - val_accuracy: 0.8787 - 22s/epoch - 57ms/step\n",
      "Epoch 147/200\n",
      "390/390 - 23s - loss: 0.0770 - accuracy: 0.9748 - val_loss: 0.4566 - val_accuracy: 0.8719 - 23s/epoch - 58ms/step\n",
      "Epoch 148/200\n",
      "390/390 - 23s - loss: 0.0765 - accuracy: 0.9744 - val_loss: 0.3760 - val_accuracy: 0.8980 - 23s/epoch - 59ms/step\n",
      "Epoch 149/200\n",
      "390/390 - 22s - loss: 0.0803 - accuracy: 0.9731 - val_loss: 0.3918 - val_accuracy: 0.8926 - 22s/epoch - 57ms/step\n",
      "Epoch 150/200\n",
      "390/390 - 23s - loss: 0.0735 - accuracy: 0.9755 - val_loss: 0.6271 - val_accuracy: 0.8462 - 23s/epoch - 58ms/step\n",
      "Epoch 151/200\n",
      "390/390 - 22s - loss: 0.0709 - accuracy: 0.9760 - val_loss: 0.5463 - val_accuracy: 0.8572 - 22s/epoch - 58ms/step\n",
      "Epoch 152/200\n",
      "390/390 - 22s - loss: 0.0758 - accuracy: 0.9749 - val_loss: 0.3901 - val_accuracy: 0.8929 - 22s/epoch - 57ms/step\n",
      "Epoch 153/200\n",
      "390/390 - 23s - loss: 0.0742 - accuracy: 0.9756 - val_loss: 0.3905 - val_accuracy: 0.8959 - 23s/epoch - 59ms/step\n",
      "Epoch 154/200\n",
      "390/390 - 23s - loss: 0.0704 - accuracy: 0.9760 - val_loss: 0.4716 - val_accuracy: 0.8797 - 23s/epoch - 59ms/step\n",
      "Epoch 155/200\n",
      "390/390 - 23s - loss: 0.0709 - accuracy: 0.9767 - val_loss: 0.4453 - val_accuracy: 0.8824 - 23s/epoch - 58ms/step\n",
      "Epoch 156/200\n",
      "390/390 - 22s - loss: 0.0647 - accuracy: 0.9781 - val_loss: 0.4103 - val_accuracy: 0.8880 - 22s/epoch - 57ms/step\n",
      "Epoch 157/200\n",
      "390/390 - 22s - loss: 0.0655 - accuracy: 0.9774 - val_loss: 0.3995 - val_accuracy: 0.8941 - 22s/epoch - 57ms/step\n",
      "Epoch 158/200\n",
      "390/390 - 23s - loss: 0.0646 - accuracy: 0.9781 - val_loss: 0.4186 - val_accuracy: 0.8885 - 23s/epoch - 58ms/step\n",
      "Epoch 159/200\n",
      "390/390 - 22s - loss: 0.0644 - accuracy: 0.9787 - val_loss: 0.4088 - val_accuracy: 0.8853 - 22s/epoch - 57ms/step\n",
      "Epoch 160/200\n",
      "390/390 - 23s - loss: 0.0632 - accuracy: 0.9785 - val_loss: 0.5322 - val_accuracy: 0.8656 - 23s/epoch - 59ms/step\n",
      "Epoch 161/200\n",
      "390/390 - 22s - loss: 0.0624 - accuracy: 0.9794 - val_loss: 0.3595 - val_accuracy: 0.9017 - 22s/epoch - 58ms/step\n",
      "Epoch 162/200\n",
      "390/390 - 23s - loss: 0.0669 - accuracy: 0.9780 - val_loss: 0.4562 - val_accuracy: 0.8783 - 23s/epoch - 58ms/step\n",
      "Epoch 163/200\n",
      "390/390 - 23s - loss: 0.0601 - accuracy: 0.9802 - val_loss: 0.4216 - val_accuracy: 0.8910 - 23s/epoch - 58ms/step\n",
      "Epoch 164/200\n",
      "390/390 - 22s - loss: 0.0595 - accuracy: 0.9809 - val_loss: 0.3663 - val_accuracy: 0.9028 - 22s/epoch - 57ms/step\n",
      "Epoch 165/200\n",
      "390/390 - 22s - loss: 0.0577 - accuracy: 0.9807 - val_loss: 0.4278 - val_accuracy: 0.8874 - 22s/epoch - 58ms/step\n",
      "Epoch 166/200\n",
      "390/390 - 22s - loss: 0.0564 - accuracy: 0.9816 - val_loss: 0.3371 - val_accuracy: 0.9097 - 22s/epoch - 57ms/step\n",
      "Epoch 167/200\n",
      "390/390 - 23s - loss: 0.0549 - accuracy: 0.9818 - val_loss: 0.4213 - val_accuracy: 0.8921 - 23s/epoch - 59ms/step\n",
      "Epoch 168/200\n",
      "390/390 - 22s - loss: 0.0550 - accuracy: 0.9819 - val_loss: 0.3502 - val_accuracy: 0.9068 - 22s/epoch - 57ms/step\n",
      "Epoch 169/200\n",
      "390/390 - 23s - loss: 0.0498 - accuracy: 0.9846 - val_loss: 0.4792 - val_accuracy: 0.8781 - 23s/epoch - 58ms/step\n",
      "Epoch 170/200\n",
      "390/390 - 22s - loss: 0.0542 - accuracy: 0.9825 - val_loss: 0.3859 - val_accuracy: 0.8949 - 22s/epoch - 57ms/step\n",
      "Epoch 171/200\n",
      "390/390 - 22s - loss: 0.0499 - accuracy: 0.9840 - val_loss: 0.3954 - val_accuracy: 0.8975 - 22s/epoch - 58ms/step\n",
      "Epoch 172/200\n",
      "390/390 - 23s - loss: 0.0462 - accuracy: 0.9853 - val_loss: 0.4135 - val_accuracy: 0.8945 - 23s/epoch - 58ms/step\n",
      "Epoch 173/200\n",
      "390/390 - 23s - loss: 0.0481 - accuracy: 0.9845 - val_loss: 0.4745 - val_accuracy: 0.8764 - 23s/epoch - 59ms/step\n",
      "Epoch 174/200\n",
      "390/390 - 23s - loss: 0.0437 - accuracy: 0.9851 - val_loss: 0.4328 - val_accuracy: 0.8866 - 23s/epoch - 59ms/step\n",
      "Epoch 175/200\n",
      "390/390 - 23s - loss: 0.0437 - accuracy: 0.9852 - val_loss: 0.3278 - val_accuracy: 0.9135 - 23s/epoch - 58ms/step\n",
      "Epoch 176/200\n",
      "390/390 - 22s - loss: 0.0372 - accuracy: 0.9884 - val_loss: 0.3257 - val_accuracy: 0.9143 - 22s/epoch - 57ms/step\n",
      "Epoch 177/200\n",
      "390/390 - 23s - loss: 0.0411 - accuracy: 0.9871 - val_loss: 0.3837 - val_accuracy: 0.9012 - 23s/epoch - 59ms/step\n",
      "Epoch 178/200\n",
      "390/390 - 22s - loss: 0.0366 - accuracy: 0.9877 - val_loss: 0.4586 - val_accuracy: 0.8872 - 22s/epoch - 57ms/step\n",
      "Epoch 179/200\n",
      "390/390 - 22s - loss: 0.0368 - accuracy: 0.9884 - val_loss: 0.3769 - val_accuracy: 0.9031 - 22s/epoch - 57ms/step\n",
      "Epoch 180/200\n",
      "390/390 - 23s - loss: 0.0343 - accuracy: 0.9893 - val_loss: 0.3594 - val_accuracy: 0.9104 - 23s/epoch - 58ms/step\n",
      "Epoch 181/200\n",
      "390/390 - 23s - loss: 0.0320 - accuracy: 0.9899 - val_loss: 0.3390 - val_accuracy: 0.9137 - 23s/epoch - 59ms/step\n",
      "Epoch 182/200\n",
      "390/390 - 22s - loss: 0.0287 - accuracy: 0.9910 - val_loss: 0.3489 - val_accuracy: 0.9119 - 22s/epoch - 58ms/step\n",
      "Epoch 183/200\n",
      "390/390 - 22s - loss: 0.0267 - accuracy: 0.9915 - val_loss: 0.3755 - val_accuracy: 0.9057 - 22s/epoch - 57ms/step\n",
      "Epoch 184/200\n",
      "390/390 - 23s - loss: 0.0246 - accuracy: 0.9925 - val_loss: 0.3162 - val_accuracy: 0.9208 - 23s/epoch - 58ms/step\n",
      "Epoch 185/200\n",
      "390/390 - 22s - loss: 0.0243 - accuracy: 0.9928 - val_loss: 0.3151 - val_accuracy: 0.9198 - 22s/epoch - 57ms/step\n",
      "Epoch 186/200\n",
      "390/390 - 22s - loss: 0.0219 - accuracy: 0.9934 - val_loss: 0.3158 - val_accuracy: 0.9216 - 22s/epoch - 57ms/step\n",
      "Epoch 187/200\n",
      "390/390 - 23s - loss: 0.0197 - accuracy: 0.9943 - val_loss: 0.2961 - val_accuracy: 0.9254 - 23s/epoch - 58ms/step\n",
      "Epoch 188/200\n",
      "390/390 - 22s - loss: 0.0184 - accuracy: 0.9952 - val_loss: 0.3260 - val_accuracy: 0.9190 - 22s/epoch - 58ms/step\n",
      "Epoch 189/200\n",
      "390/390 - 22s - loss: 0.0170 - accuracy: 0.9958 - val_loss: 0.2902 - val_accuracy: 0.9293 - 22s/epoch - 57ms/step\n",
      "Epoch 190/200\n",
      "390/390 - 22s - loss: 0.0150 - accuracy: 0.9966 - val_loss: 0.3324 - val_accuracy: 0.9210 - 22s/epoch - 57ms/step\n",
      "Epoch 191/200\n",
      "390/390 - 23s - loss: 0.0144 - accuracy: 0.9968 - val_loss: 0.2986 - val_accuracy: 0.9278 - 23s/epoch - 58ms/step\n",
      "Epoch 192/200\n",
      "390/390 - 23s - loss: 0.0130 - accuracy: 0.9978 - val_loss: 0.3005 - val_accuracy: 0.9276 - 23s/epoch - 59ms/step\n",
      "Epoch 193/200\n",
      "390/390 - 22s - loss: 0.0140 - accuracy: 0.9976 - val_loss: 0.2925 - val_accuracy: 0.9282 - 22s/epoch - 57ms/step\n",
      "Epoch 194/200\n",
      "390/390 - 22s - loss: 0.0126 - accuracy: 0.9982 - val_loss: 0.2835 - val_accuracy: 0.9285 - 22s/epoch - 57ms/step\n",
      "Epoch 195/200\n",
      "390/390 - 23s - loss: 0.0139 - accuracy: 0.9984 - val_loss: 0.2892 - val_accuracy: 0.9267 - 23s/epoch - 58ms/step\n",
      "Epoch 196/200\n",
      "390/390 - 22s - loss: 0.0156 - accuracy: 0.9986 - val_loss: 0.2739 - val_accuracy: 0.9306 - 22s/epoch - 57ms/step\n",
      "Epoch 197/200\n",
      "390/390 - 23s - loss: 0.0168 - accuracy: 0.9988 - val_loss: 0.2700 - val_accuracy: 0.9322 - 23s/epoch - 58ms/step\n",
      "Epoch 198/200\n",
      "390/390 - 23s - loss: 0.0202 - accuracy: 0.9988 - val_loss: 0.2670 - val_accuracy: 0.9308 - 23s/epoch - 58ms/step\n",
      "Epoch 199/200\n",
      "390/390 - 23s - loss: 0.0245 - accuracy: 0.9987 - val_loss: 0.2657 - val_accuracy: 0.9311 - 23s/epoch - 58ms/step\n",
      "Epoch 200/200\n",
      "390/390 - 22s - loss: 0.0298 - accuracy: 0.9990 - val_loss: 0.2649 - val_accuracy: 0.9317 - 22s/epoch - 57ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x115ff290f28>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen.flow(x_train, y_train_onehot,batch_size=batch_size),\n",
    "                            steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=(x_test, y_test_onehot),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b0df93e-b32a-4540-863f-0244250e2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"VGG16_cifar10.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5039e57f-ccb3-4d6a-8655-20449778ecf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d05e2-786c-4517-9af4-111bf94e2867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eddbe2-b3f1-40d0-b0d2-f14870881941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

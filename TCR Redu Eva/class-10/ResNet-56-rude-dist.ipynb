{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dea4f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers,regularizers,metrics,optimizers\n",
    "import random\n",
    "import pandas as pd\n",
    "from scipy.linalg import sqrtm\n",
    "import pickle\n",
    "import logging\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\n",
    "import math\n",
    "import scipy.stats as st\n",
    "from scipy.special import comb\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import json\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c459c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "config=tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "config.gpu_options.allow_growth=True\n",
    "sess=tf.compat.v1.Session(config=config) \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047a8aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This algorithm is used to evaluate the structural redundancy of ResNet-56 \n",
    "and outputs the evaluation criteria of hidden layer redundancy as well as \n",
    "the entire redundancy evaluation criteria under each pruning parameter. \n",
    "Here, \"Lam\" refers to the pruning parameter set used in the evaluation \n",
    "algorithm, and \"repeats\" represents the number of times the pruning network \n",
    "is repeatedly fine-tuned.\"\"\"\n",
    "Lam=[0.775,0.725,0.675,0.625]\n",
    "repeats=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a42158-7dad-47c9-b86b-79b6d1ec3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "y_train_onehot=tf.keras.utils.to_categorical(y_train,num_classes=10)\n",
    "y_test_onehot=tf.keras.utils.to_categorical(y_test,num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1760ce31-8ae9-4c9b-9ae2-a1f51229e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_dist_ResNet_56.pkl', 'rb') as f:\n",
    "    [x_dist,y_dist]=pickle.load(f)\n",
    "y_dist=y_dist.reshape(len(y_dist),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b29e82-1535-4890-8962-851dfbb01ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 0.1\n",
    "weight_decay = 1e-4\n",
    "epochs = 200\n",
    "warmup_epochs = 5\n",
    "batch_size = 128\n",
    "image_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d66d49e7-17f5-4ca8-af13-e7e02c3a029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, base_lr, total_steps, warmup_steps, warmup_lr=0.0):\n",
    "        super().__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.warmup_lr = warmup_lr\n",
    "    def __call__(self, step):\n",
    "        if step is None:\n",
    "            step = tf.constant(0)\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        total_steps = tf.cast(self.total_steps, tf.float32)\n",
    "        warmup_percent_done = step / warmup_steps\n",
    "        learning_rate = tf.where(\n",
    "            step < warmup_steps,\n",
    "            self.warmup_lr + (self.base_lr - self.warmup_lr) * warmup_percent_done,\n",
    "            self.base_lr * 0.5 * (1.0 + tf.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps)))\n",
    "        )\n",
    "        return learning_rate\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"base_lr\": self.base_lr,\n",
    "            \"total_steps\": self.total_steps,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"warmup_lr\": self.warmup_lr,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a00d3648-b747-4730-8a76-75ce7c9a318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWeightDecaySGD(tf.keras.optimizers.SGD):\n",
    "    def __init__(self, weight_decay, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.weight_decay = weight_decay\n",
    "    def apply_gradients(self, grads_and_vars, name=None, experimental_aggregate_gradients=True):\n",
    "        super().apply_gradients(grads_and_vars, name, experimental_aggregate_gradients)\n",
    "        for grad, var in grads_and_vars:\n",
    "            if ('kernel' in var.name) and ('bn' not in var.name.lower()):\n",
    "                var.assign_sub(self.weight_decay * var)\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"weight_decay\": float(self.weight_decay),  # 确保是float\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0af3367-3405-42a9-8e90-f5c8df2a1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastNSaver(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, n=10):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.history = deque(maxlen=n)  \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_acc = logs.get(\"val_accuracy\")\n",
    "        if val_acc is not None:\n",
    "            weights = self.model.get_weights()\n",
    "            self.history.append((val_acc, weights))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if not self.history:\n",
    "            return\n",
    "        best_acc, best_weights = max(self.history, key=lambda x: x[0])\n",
    "        print(f\" Using best val_acc={best_acc:.4f} from last {self.n} epochs\")\n",
    "        self.model.set_weights(best_weights)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47d65cd3-569b-4d19-a999-d595b1b517bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Res():\n",
    "    model = tf.keras.models.load_model('Res56_cifar10.h5',custom_objects={\n",
    "        'CustomWeightDecaySGD': CustomWeightDecaySGD,\n",
    "        'WarmUpCosine': WarmUpCosine\n",
    "    })\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e2e2c6f-0487-4831-9852-adf0323c615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_relu(x, filters, kernel_size, strides=1):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding='same',use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    return tf.keras.layers.ReLU()(x)\n",
    "\n",
    "def residual_block(x, filter1, filter2, downsample=False):\n",
    "    shortcut = x\n",
    "    strides = 2 if downsample else 1\n",
    "    x = conv_bn_relu(x, filter1, 3, strides)\n",
    "    x = tf.keras.layers.Conv2D(filter2, 3, strides=1, padding='same',use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    if downsample:\n",
    "        shortcut = tf.keras.layers.Conv2D(filter2, 1, strides=strides, padding='same',use_bias=False)(shortcut)\n",
    "        shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
    "    x = tf.keras.layers.add([x, shortcut])\n",
    "    return tf.keras.layers.ReLU()(x)\n",
    "\n",
    "def Res_model(NN,input_shape=(32,32,3), num_classes=10):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = conv_bn_relu(inputs, NN[9], 3)\n",
    "    x = residual_block(x, NN[0], NN[9])\n",
    "    x = residual_block(x, NN[1], NN[9])\n",
    "    x = residual_block(x, NN[2], NN[9])\n",
    "    x = residual_block(x, NN[3], NN[9])\n",
    "    x = residual_block(x, NN[4], NN[9])\n",
    "    x = residual_block(x, NN[5], NN[9])\n",
    "    x = residual_block(x, NN[6], NN[9])\n",
    "    x = residual_block(x, NN[7], NN[9])\n",
    "    x = residual_block(x, NN[8], NN[9])\n",
    "    x = residual_block(x, NN[10], NN[19], downsample=True)\n",
    "    x = residual_block(x, NN[11], NN[19])\n",
    "    x = residual_block(x, NN[12], NN[19])\n",
    "    x = residual_block(x, NN[13], NN[19])\n",
    "    x = residual_block(x, NN[14], NN[19])\n",
    "    x = residual_block(x, NN[15], NN[19])\n",
    "    x = residual_block(x, NN[16], NN[19])\n",
    "    x = residual_block(x, NN[17], NN[19])\n",
    "    x = residual_block(x, NN[18], NN[19])\n",
    "    x = residual_block(x, NN[20], NN[29], downsample=True)\n",
    "    x = residual_block(x, NN[21], NN[29])\n",
    "    x = residual_block(x, NN[22], NN[29])\n",
    "    x = residual_block(x, NN[23],NN[29])\n",
    "    x = residual_block(x, NN[24],NN[29])\n",
    "    x = residual_block(x, NN[25], NN[29])\n",
    "    x = residual_block(x, NN[26], NN[29])\n",
    "    x = residual_block(x, NN[27],NN[29])\n",
    "    x = residual_block(x, NN[28],NN[29])\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes,activation='softmax')(x)\n",
    "    return tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78515397",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_Res()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c71b4f71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   432         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 32, 32, 16)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['batch_normalization_2[0][0]',  \n",
      "                                                                  're_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 32, 32, 16)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 32, 32, 16)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 16)   0           ['batch_normalization_4[0][0]',  \n",
      "                                                                  're_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 32, 32, 16)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 32, 32, 16)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 32, 16)   0           ['batch_normalization_6[0][0]',  \n",
      "                                                                  're_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 32, 32, 16)   0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 32, 32, 16)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 32, 32, 16)   0           ['batch_normalization_8[0][0]',  \n",
      "                                                                  're_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)                 (None, 32, 32, 16)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu_8[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)                 (None, 32, 32, 16)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 16)   2304        ['re_lu_9[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 32, 16)  64          ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 32, 32, 16)   0           ['batch_normalization_10[0][0]', \n",
      "                                                                  're_lu_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)                (None, 32, 32, 16)   0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 16)   2304        ['re_lu_10[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 16)  64          ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)                (None, 32, 32, 16)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 16)   2304        ['re_lu_11[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 16)  64          ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 32, 32, 16)   0           ['batch_normalization_12[0][0]', \n",
      "                                                                  're_lu_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)                (None, 32, 32, 16)   0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 16)   2304        ['re_lu_12[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 32, 32, 16)  64          ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)                (None, 32, 32, 16)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 32, 32, 16)   2304        ['re_lu_13[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 32, 32, 16)  64          ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 32, 32, 16)   0           ['batch_normalization_14[0][0]', \n",
      "                                                                  're_lu_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)                (None, 32, 32, 16)   0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 32, 32, 16)   2304        ['re_lu_14[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 32, 32, 16)  64          ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)                (None, 32, 32, 16)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 32, 32, 16)   2304        ['re_lu_15[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 32, 32, 16)  64          ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 32, 32, 16)   0           ['batch_normalization_16[0][0]', \n",
      "                                                                  're_lu_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)                (None, 32, 32, 16)   0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 32, 32, 16)   2304        ['re_lu_16[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 32, 32, 16)  64          ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)                (None, 32, 32, 16)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 32, 32, 16)   2304        ['re_lu_17[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 32, 32, 16)  64          ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 32, 32, 16)   0           ['batch_normalization_18[0][0]', \n",
      "                                                                  're_lu_16[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_18 (ReLU)                (None, 32, 32, 16)   0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 16, 16, 32)   4608        ['re_lu_18[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 16, 16, 32)  128         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_19 (ReLU)                (None, 16, 16, 32)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_19[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 16, 16, 32)   512         ['re_lu_18[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 16, 16, 32)  128         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 16, 16, 32)  128         ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 16, 16, 32)   0           ['batch_normalization_20[0][0]', \n",
      "                                                                  'batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_20 (ReLU)                (None, 16, 16, 32)   0           ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_20[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 16, 16, 32)  128         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_21 (ReLU)                (None, 16, 16, 32)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_21[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 16, 16, 32)  128         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 16, 16, 32)   0           ['batch_normalization_23[0][0]', \n",
      "                                                                  're_lu_20[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_22 (ReLU)                (None, 16, 16, 32)   0           ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_22[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 16, 16, 32)  128         ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_23 (ReLU)                (None, 16, 16, 32)   0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_23[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 16, 16, 32)  128         ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 16, 16, 32)   0           ['batch_normalization_25[0][0]', \n",
      "                                                                  're_lu_22[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_24 (ReLU)                (None, 16, 16, 32)   0           ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_24[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 16, 16, 32)  128         ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_25 (ReLU)                (None, 16, 16, 32)   0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_25[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 16, 16, 32)  128         ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 16, 16, 32)   0           ['batch_normalization_27[0][0]', \n",
      "                                                                  're_lu_24[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_26 (ReLU)                (None, 16, 16, 32)   0           ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_26[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 16, 16, 32)  128         ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_27 (ReLU)                (None, 16, 16, 32)   0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_27[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 16, 16, 32)  128         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 16, 16, 32)   0           ['batch_normalization_29[0][0]', \n",
      "                                                                  're_lu_26[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_28 (ReLU)                (None, 16, 16, 32)   0           ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_28[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 16, 16, 32)  128         ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_29 (ReLU)                (None, 16, 16, 32)   0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_29[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 16, 16, 32)  128         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 16, 16, 32)   0           ['batch_normalization_31[0][0]', \n",
      "                                                                  're_lu_28[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_30 (ReLU)                (None, 16, 16, 32)   0           ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_30[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 16, 16, 32)  128         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_31 (ReLU)                (None, 16, 16, 32)   0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_31[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 16, 16, 32)  128         ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 16, 16, 32)   0           ['batch_normalization_33[0][0]', \n",
      "                                                                  're_lu_30[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_32 (ReLU)                (None, 16, 16, 32)   0           ['add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_32[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 16, 16, 32)  128         ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_33 (ReLU)                (None, 16, 16, 32)   0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_33[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 16, 16, 32)  128         ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 16, 16, 32)   0           ['batch_normalization_35[0][0]', \n",
      "                                                                  're_lu_32[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_34 (ReLU)                (None, 16, 16, 32)   0           ['add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_34[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 16, 16, 32)  128         ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_35 (ReLU)                (None, 16, 16, 32)   0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_35[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 16, 16, 32)  128         ['conv2d_37[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16, 16, 32)   0           ['batch_normalization_37[0][0]', \n",
      "                                                                  're_lu_34[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_36 (ReLU)                (None, 16, 16, 32)   0           ['add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 8, 8, 64)     18432       ['re_lu_36[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 8, 8, 64)    256         ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_37 (ReLU)                (None, 8, 8, 64)     0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_37[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 8, 8, 64)     2048        ['re_lu_36[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 8, 8, 64)    256         ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 8, 8, 64)    256         ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_39[0][0]', \n",
      "                                                                  'batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_38 (ReLU)                (None, 8, 8, 64)     0           ['add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_38[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 8, 8, 64)    256         ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_39 (ReLU)                (None, 8, 8, 64)     0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_39[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 8, 8, 64)    256         ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_42[0][0]', \n",
      "                                                                  're_lu_38[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_40 (ReLU)                (None, 8, 8, 64)     0           ['add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_40[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 8, 8, 64)    256         ['conv2d_43[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_41 (ReLU)                (None, 8, 8, 64)     0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_41[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 8, 8, 64)    256         ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_44[0][0]', \n",
      "                                                                  're_lu_40[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_42 (ReLU)                (None, 8, 8, 64)     0           ['add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_42[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 8, 8, 64)    256         ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_43 (ReLU)                (None, 8, 8, 64)     0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_43[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 8, 8, 64)    256         ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_46[0][0]', \n",
      "                                                                  're_lu_42[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_44 (ReLU)                (None, 8, 8, 64)     0           ['add_21[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_44[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 8, 8, 64)    256         ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_45 (ReLU)                (None, 8, 8, 64)     0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_45[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 8, 8, 64)    256         ['conv2d_48[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_48[0][0]', \n",
      "                                                                  're_lu_44[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_46 (ReLU)                (None, 8, 8, 64)     0           ['add_22[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_46[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 8, 8, 64)    256         ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_47 (ReLU)                (None, 8, 8, 64)     0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_47[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 8, 8, 64)    256         ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_50[0][0]', \n",
      "                                                                  're_lu_46[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_48 (ReLU)                (None, 8, 8, 64)     0           ['add_23[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_48[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 8, 8, 64)    256         ['conv2d_51[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_49 (ReLU)                (None, 8, 8, 64)     0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_49[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 8, 8, 64)    256         ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_52[0][0]', \n",
      "                                                                  're_lu_48[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_50 (ReLU)                (None, 8, 8, 64)     0           ['add_24[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_50[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 8, 8, 64)    256         ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_51 (ReLU)                (None, 8, 8, 64)     0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_51[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 8, 8, 64)    256         ['conv2d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_54[0][0]', \n",
      "                                                                  're_lu_50[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_52 (ReLU)                (None, 8, 8, 64)     0           ['add_25[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_52[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 8, 8, 64)    256         ['conv2d_55[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_53 (ReLU)                (None, 8, 8, 64)     0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_53[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 8, 8, 64)    256         ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_56[0][0]', \n",
      "                                                                  're_lu_52[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_54 (ReLU)                (None, 8, 8, 64)     0           ['add_26[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 64)          0           ['re_lu_54[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           650         ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 860,026\n",
      "Trainable params: 855,770\n",
      "Non-trainable params: 4,256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28d0706c-94e4-48ee-a9f5-3306d5e84463",
   "metadata": {},
   "outputs": [],
   "source": [
    "G=[[4],[11],[18],[25],[32],[39],[46],[53],[60],[1,7,14,21,28,35,42,49,56,63],\n",
    "   [67],[76],[83],[90],[97],[104],[111],[118],[125],[70,71,79,86,93,100,107,114,121,128],\n",
    "   [132],[141],[148],[155],[162],[169],[176],[183],[190],[135,136,144,151,158,165,172,179,186,193]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "906a2ac4-2607-4a63-9db4-8182bd3a3257",
   "metadata": {},
   "outputs": [],
   "source": [
    "P=[0,3,\n",
    "   6,10,13,17,20,24,27,31,34,38,41,45,48,52,55,59,62,66,\n",
    "   69,75,78,82,85,89,92,96,99,103,106,110,113,117,120,124,127,131,\n",
    "   134,140,143,147,150,154,157,161,164,168,171,175,178,182,185,189,192,196]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "564ea471-4e46-47bc-9d1f-e0d6df7de5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def JW(m, M):\n",
    "    \"\"\"\n",
    "    Compute the binary MD-LP J_w_value.\n",
    "\n",
    "    Args:\n",
    "        m: 1-D tensor of shape [d], the mean of the Minkowski difference of a \n",
    "           binary classification dataset.\n",
    "        M: 2-D tensor of shape [d, N], binary classification dataset Minkowski \n",
    "           difference set.\n",
    "           \n",
    "    Key idea:\n",
    "        - Calculate the approximate solution m_weighted for the optimal weights \n",
    "          in the MD-LP.\n",
    "        - Calculate the MD-LP based on the approximately optimal weights, and \n",
    "          perform a left truncation at 0.5. \n",
    "    Returns:\n",
    "        Binary MD-LP value.\n",
    "    \"\"\"\n",
    "    row_norm_sq = tf.reduce_sum(tf.square(M), axis=1)  \n",
    "    reciprocal_norm = tf.where(row_norm_sq != 0,\n",
    "                               tf.math.reciprocal(row_norm_sq),\n",
    "                               tf.zeros_like(row_norm_sq))  \n",
    "    m_weighted = m * reciprocal_norm  \n",
    "    m_weighted = tf.reshape(m_weighted, [1, -1])  \n",
    "    mM = tf.matmul(m_weighted, M)\n",
    "    L1 = tf.reduce_sum(mM)\n",
    "    L_1 = tf.reduce_sum(tf.abs(mM))\n",
    "    J_w_value = tf.abs(L1) / (L_1 + 1e-8)\n",
    "    J_w_value = tf.maximum(J_w_value, 0.5)\n",
    "    return J_w_value\n",
    "def W(X, Y, k, n_c=10):\n",
    "    \"\"\"\n",
    "    This function is used to calculate the top k largest binary classification \n",
    "    problems MD-LP used in the multi-classification problem calculation. Here, \n",
    "    the binary classification problems are obtained by combining each pair of \n",
    "    categories of the multi-classification problem.\n",
    "    Args:\n",
    "        X: Tensor/array of shape [b, l, w]. Channel output.\n",
    "        Y: Tensor/array of labels of shape [b]. Data labels.\n",
    "        k: Number of the largest binary MD-LP to keep.\n",
    "        n_c: Number of classes.\n",
    "    Returns:\n",
    "        JK_list: Tensor of shape [k], the top-k MD-LP.\n",
    "    \"\"\"\n",
    "    b, l, w = X.shape\n",
    "    X = tf.reshape(X, [b, l*w])   # flatten\n",
    "    J_list = []\n",
    "    for i, j in itertools.combinations(range(n_c), 2):\n",
    "        mask_1 = tf.reshape(tf.equal(Y, i), [-1])\n",
    "        mask_2 = tf.reshape(tf.equal(Y, j), [-1])\n",
    "        X1 = tf.boolean_mask(X, mask_1)\n",
    "        X2 = tf.boolean_mask(X, mask_2)\n",
    "        n1 = tf.shape(X1)[0]\n",
    "        n2 = tf.shape(X2)[0]\n",
    "        m_i = tf.reduce_sum(X1, axis=0) * tf.cast(n2, tf.float32) - tf.reduce_sum(X2, axis=0) * tf.cast(n1, tf.float32)\n",
    "        M_i = tf.reshape(X1[:, None, :] - X2[None, :, :], [-1, l*w])\n",
    "        M_i = tf.transpose(M_i)\n",
    "        J = JW(m_i, M_i)\n",
    "        J_list.append(J)\n",
    "    J_list = tf.stack(J_list)\n",
    "    JK_list , JK_inde = tf.math.top_k(J_list,k)\n",
    "    return JK_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "779631d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1_channel(x_L,y,prune_rate, nnn, alpha=2.5):\n",
    "    \"\"\"\n",
    "    This function computes TCR measure of each channel in convolutional layer.\n",
    "    \n",
    "    Given the output of a convolutional layer, this function will execute:\n",
    "    - Treating each channel independently and computing a multi-class MD-LP \n",
    "      via function W;\n",
    "    - By applying nonlinear transformation, a TCR measure is constructed \n",
    "      to enhance the separability of MD-LP.\n",
    "    \n",
    "    Key Args:\n",
    "    x_L (Tensor):\n",
    "        Output of a convolutional hidden layer, with shape \n",
    "        [batch_size, height, width, channels].\n",
    "    y (Tensor):\n",
    "        Ground-truth labels corresponding to the input samples.\n",
    "    alpha (float, optional):\n",
    "        LP transformation parameter. Used to enhance the separability \n",
    "        of the MD-LP close to 1.\n",
    "    \n",
    "    Returns:\n",
    "    jw:\n",
    "        TCR measure of each channel.\n",
    "    \"\"\"\n",
    "    a, b, d, c = x_L.shape\n",
    "    jw = tf.zeros([c], dtype=tf.float32)\n",
    "    alpha = tf.cast(alpha, tf.float32)\n",
    "    for j in tf.range(c):\n",
    "        N_tf = W(x_L[:,:,:,j], y, nnn)\n",
    "        jw_j = tf.norm(N_tf) / tf.sqrt(float(nnn))\n",
    "        jw_j = (tf.exp(alpha * (2*jw_j-1)) - 1.0) / (tf.exp(alpha) - 1.0)\n",
    "        jw = tf.tensor_scatter_nd_update(jw, [[j]], [jw_j])\n",
    "    return jw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2acfaba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_channel(x_LG, y, prune_rate, nnn=45, esp=1e-8):\n",
    "    \"\"\"\n",
    "    This function computes the structural redundancy evaluation criterion R_L \n",
    "    and determines the set of retained channel indices `channel_i_label` used \n",
    "    by the pruning algorithm for a layer group.\n",
    "    \n",
    "    Given the output of a convolutional layer, this function will execute:\n",
    "    - By analyzing the propensity calculation of TCR measure, an evaluation \n",
    "      criterion for evaluating the redundancy of convolutional layers is derived.\n",
    "    - Based on the TCR measure, the pruning threshold is calculated and the \n",
    "      channels that remain after pruning are selected.\n",
    "    \n",
    "    Key Args:\n",
    "    x_LG (a list of Tensors):\n",
    "        Output of a group of convolutional hidden layers.\n",
    "    y (Tensor):\n",
    "        Ground-truth labels corresponding to the input samples.\n",
    "    prune_rate (float):\n",
    "        Pruning parameter. Used to control the strictness of pruning.\n",
    "    \n",
    "    Returns:\n",
    "    channel_i_label (ndarray):\n",
    "        Indices of channels retained after pruning.\n",
    "    R_L (float):\n",
    "        Structural redundancy evaluation criterion of the layer group,\n",
    "    \"\"\"\n",
    "    a, b, d, c = x_LG[-1].shape\n",
    "    L1_list = []\n",
    "    for i in range(len(x_LG)):\n",
    "        L1_i = L1_channel(x_LG[i], y, prune_rate, nnn)\n",
    "        L1_list.append(L1_i)\n",
    "    L1 = tf.stack(L1_list, axis=0)  # shape: (len(x_LG), c)\n",
    "    jw = tf.reduce_mean(L1, axis=0) # shape: (c,)\n",
    "    jw_min = tf.maximum(tf.reduce_min(jw) - esp, 0.0)\n",
    "    jw_max = tf.reduce_max(jw)\n",
    "    me = tf.sqrt(tf.reduce_mean(tf.square(jw - jw_min)))\n",
    "    jd = jw_min + prune_rate * me\n",
    "    mean = tf.maximum(tf.reduce_mean(jw) - esp, 0.0)\n",
    "    R_L = tf.reduce_mean(tf.sign(jw - mean))\n",
    "    channel_i_label = tf.where(jw >= jd)[:,0]\n",
    "    return channel_i_label.numpy(), R_L.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62555afb-03be-491e-a73b-8a314b2941ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Group_1(x_L,w,S):\n",
    "    \"\"\"This function is used to collect the key hidden layer outputs of the first block \n",
    "    in ResNet. Here, x_L1 is used to update the list of output from the ResNet \n",
    "    convolutional layers, and x_L2 is used to provide the hidden layer outputs required \n",
    "    for layer group pruning.\"\"\"\n",
    "    wb=w[0]\n",
    "    bn=w[1]\n",
    "    x_L1=[]\n",
    "    x_L2=[]\n",
    "    x_1=layer_xL(\"conv2d\",x_L,w=wb[0])\n",
    "    x_1=layer_xL(\"batch_normalization\",x_1,w=bn[0])\n",
    "    x_1=layer_xL(\"activation\",x_1)\n",
    "    x_L2.append(deepcopy(x_1))\n",
    "    #x_1=layer_xL(\"maxpooling\",x_1)\n",
    "    x_L1.append(deepcopy(x_1))\n",
    "\n",
    "    for i in range(9):\n",
    "        \n",
    "        x_2=layer_xL(\"conv2d\",x_1,w=wb[2*i+1])\n",
    "        x_2=layer_xL(\"batch_normalization\",x_2,w=bn[2*i+1])\n",
    "        x_2=layer_xL(\"activation\",x_2)\n",
    "        x_L1.append(deepcopy(x_2))\n",
    "        \n",
    "        x_2=layer_xL(\"conv2d\",x_2,w=wb[2*i+2])\n",
    "        x_2=layer_xL(\"batch_normalization\",x_2,w=bn[2*i+2])\n",
    "        x_1=layer_xL(\"add\",[x_2,x_1])\n",
    "        x_1=layer_xL(\"activation\",x_1)\n",
    "        x_L1.append(deepcopy(x_1))\n",
    "        x_L2.append(deepcopy(x_1))\n",
    "    return x_L1,x_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b224b60b-5f03-4f1d-9d30-287e9dcc8aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Group_L(x_L,w,S):\n",
    "    \"\"\"This function is used to collect the key hidden layer outputs of the other block \n",
    "    in ResNet. Here, x_L1 is used to update the list of output from the ResNet \n",
    "    convolutional layers, and x_L2 is used to provide the hidden layer outputs required \n",
    "    for layer group pruning.\"\"\"\n",
    "    wb=w[0]\n",
    "    bn=w[1]\n",
    "    x_L1=[]\n",
    "    x_L2=[]\n",
    "    x_1=layer_xL(\"conv2d\",x_L,w=wb[0],S=2)\n",
    "    x_1=layer_xL(\"batch_normalization\",x_1,w=bn[0])\n",
    "    x_1=layer_xL(\"activation\",x_1)\n",
    "    x_L1.append(deepcopy(x_1))\n",
    "            \n",
    "    x_1=layer_xL(\"conv2d\",x_1,w=wb[1])\n",
    "    x_2=layer_xL(\"conv2d\",x_L,w=wb[2],S=2)\n",
    "    x_1=layer_xL(\"batch_normalization\",x_1,w=bn[1])\n",
    "    x_2=layer_xL(\"batch_normalization\",x_2,w=bn[2])\n",
    "    x_1=layer_xL(\"add\",[x_1,x_2])\n",
    "    x_1=layer_xL(\"activation\",x_1)\n",
    "    x_L1.append(deepcopy(x_1))\n",
    "    x_L2.append(deepcopy(x_1))\n",
    "\n",
    "    for i in range(8):\n",
    "        \n",
    "        x_2=layer_xL(\"conv2d\",x_1,w=wb[2*i+3])\n",
    "        x_2=layer_xL(\"batch_normalization\",x_2,w=bn[2*i+3])\n",
    "        x_2=layer_xL(\"activation\",x_2)\n",
    "        x_L1.append(deepcopy(x_2))\n",
    "    \n",
    "        x_2=layer_xL(\"conv2d\",x_2,w=wb[2*i+4])\n",
    "        x_2=layer_xL(\"batch_normalization\",x_2,w=bn[2*i+4])\n",
    "        x_1=layer_xL(\"add\",[x_2,x_1])\n",
    "        x_1=layer_xL(\"activation\",x_1)\n",
    "        x_L1.append(deepcopy(x_1))\n",
    "        x_L2.append(deepcopy(x_1))\n",
    "    return x_L1,x_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b348156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_xL(layer_name,x_L,w=None,F=False,S=1):\n",
    "    \"\"\"This function is used to obtain the outputs of various hidden layers or layer groups in the \n",
    "    network, which is utilized for updating the outputs of hidden layers or for performing layer \n",
    "    group pruning calculations.\"\"\"\n",
    "    if \"conv2d\" in layer_name:\n",
    "        weight=w[0]\n",
    "        strides=[1,S,S,1]\n",
    "        x_L1=tf.nn.conv2d(x_L,weight,strides=strides,padding=\"SAME\")\n",
    "        #x_L1=tf.nn.bias_add(x_L1,bias)\n",
    "        return x_L1\n",
    "    if \"first\" in layer_name:\n",
    "        x_L1,x_L2=Group_1(x_L,w,S)\n",
    "        if F==True:\n",
    "            return x_L2\n",
    "        else:\n",
    "            return x_L1\n",
    "    if \"group\" in layer_name:\n",
    "        x_L1,x_L2=Group_L(x_L,w,S)\n",
    "        if F==True:\n",
    "            return x_L2\n",
    "        else:\n",
    "            return x_L1\n",
    "    if \"batch_normalization\" in layer_name:\n",
    "        gamma,beta,mean,var=w\n",
    "        x_L1=tf.nn.batch_normalization(x_L,mean=mean,\n",
    "                                          variance=var,\n",
    "                                          offset=beta,\n",
    "                                          scale=gamma,variance_epsilon=1e-5)\n",
    "        return x_L1\n",
    "    if \"activation\" in layer_name:\n",
    "        x_L1=tf.nn.relu(x_L)\n",
    "        return x_L1\n",
    "    if \"add\" in layer_name:\n",
    "        x1,x2=x_L\n",
    "        x_L1=tf.math.add(x1,x2)\n",
    "        return x_L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "355a0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_block(a,b,G,weight_list,x_LG,First=False,Group=False,R=False,F=False):\n",
    "    \"\"\"This function is used to update the list of hidden layer outputs \n",
    "    of ResNet after function pruning. For single-hidden-layer pruning, \n",
    "    it only needs to update the output of the convolutional layer within \n",
    "    the block. However, for layer group pruning, since it involves multiple \n",
    "    blocks, the output of all convolutional layers in these blocks needs \n",
    "    to be updated.\"\"\"\n",
    "    if Group==False:\n",
    "        if R==True:\n",
    "            x_1=x_LG[b]\n",
    "            layer_l=G[a][0]\n",
    "            wc=weight_list[layer_l]\n",
    "            wb=weight_list[layer_l+1]\n",
    "            x_2=layer_xL(\"conv2d\",x_1,wc,S=2)\n",
    "            x_2=layer_xL(\"batch_normalization\",x_2,wb)\n",
    "            x_2=layer_xL(\"activation\",x_2)\n",
    "            x_LG[b+1]=x_2\n",
    "            layer_l=G[a+9][0]\n",
    "            wc=weight_list[layer_l]\n",
    "            wb=weight_list[layer_l+2]\n",
    "            x_2=layer_xL(\"conv2d\",x_2,wc)\n",
    "            x_2=layer_xL(\"batch_normalization\",x_2,wb)\n",
    "            layer_l=G[a+9][1]\n",
    "            wc=weight_list[layer_l]\n",
    "            wb=weight_list[layer_l+2]\n",
    "            x_1=layer_xL(\"conv2d\",x_1,wc,S=2)\n",
    "            x_1=layer_xL(\"batch_normalization\",x_1,wb)\n",
    "            x_1=layer_xL(\"add\",[x_2,x_1])\n",
    "            x_1=layer_xL(\"activation\",x_1)\n",
    "            x_LG[b+2]=x_1\n",
    "            return x_LG\n",
    "        if R==False:\n",
    "            x_1=x_LG[b]\n",
    "            layer_l=G[a][0]\n",
    "            wc=weight_list[layer_l]\n",
    "            wb=weight_list[layer_l+1]\n",
    "            x_2=layer_xL(\"conv2d\",x_1,wc)\n",
    "            x_2=layer_xL(\"batch_normalization\",x_2,wb)\n",
    "            x_2=layer_xL(\"activation\",x_2)\n",
    "            x_LG[b+1]=x_2\n",
    "            if First==True:\n",
    "                layer_l=G[a+9][1]\n",
    "            else:\n",
    "                aaa=int(((a+1)//10+1)*10)\n",
    "                layer_l=G[aaa-1][a%10+1]\n",
    "            wc=weight_list[layer_l]\n",
    "            wb=weight_list[layer_l+1]\n",
    "            x_2=layer_xL(\"conv2d\",x_2,wc)\n",
    "            x_2=layer_xL(\"batch_normalization\",x_2,wb)\n",
    "            x_1=layer_xL(\"add\",[x_2,x_1])\n",
    "            x_1=layer_xL(\"activation\",x_1)\n",
    "            x_LG[b+2]=x_1\n",
    "            return x_LG\n",
    "    if Group==True:\n",
    "        if First==True:\n",
    "            label=[G[a][0],G[a-9][0],G[a][1],G[a-8][0],G[a][2],G[a-7][0],G[a][3],G[a-6][0],G[a][4],G[a-5][0],G[a][5],G[a-4][0],G[a][6]\n",
    "                  ,G[a-3][0],G[a][7],G[a-2][0],G[a][8],G[a-1][0],G[a][9]]\n",
    "            w1=[]\n",
    "            w2=[]\n",
    "            x_1=x_LG[b]\n",
    "            for i in range(19):\n",
    "                w1.append(weight_list[label[i]])\n",
    "                w2.append(weight_list[label[i]+1])\n",
    "            w=[w1,w2]\n",
    "            x_1=layer_xL(\"first\",x_1,w,F=False,S=1)\n",
    "            for i in range(19):\n",
    "                x_LG[b+i+1]=x_1[i]\n",
    "            return x_LG\n",
    "        else:\n",
    "            label=[G[a-9][0],G[a][0],G[a][1],G[a-8][0],G[a][2],G[a-7][0],G[a][3],G[a-6][0],G[a][4],G[a-5][0],G[a][5],G[a-4][0],G[a][6]\n",
    "                  ,G[a-3][0],G[a][7],G[a-2][0],G[a][8],G[a-1][0],G[a][9]]\n",
    "            w1=[]\n",
    "            w2=[]\n",
    "            x_1=x_LG[b]\n",
    "            l=[1,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "            for i in range(19):\n",
    "                w1.append(weight_list[label[i]])\n",
    "                w2.append(weight_list[label[i]+l[i]])\n",
    "            w=[w1,w2]\n",
    "            x_1=layer_xL(\"group\",x_1,w)\n",
    "            for i in range(18):\n",
    "                x_LG[b+i+1]=x_1[i]\n",
    "            return x_LG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4ac50a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(a,b,G,x_LG,weight_list,First=False):\n",
    "    \"\"\"This function is used to provide the required output list \n",
    "    for 'prune_channel' function when performing layer group pruning.\"\"\"\n",
    "    x_l=x_LG[b]\n",
    "    if First==True:\n",
    "        label=[G[a][0],G[a-9][0],G[a][1],G[a-8][0],G[a][2],G[a-7][0],G[a][3],G[a-6][0],G[a][4],G[a-5][0],G[a][5],G[a-4][0],G[a][6]\n",
    "               ,G[a-3][0],G[a][7],G[a-2][0],G[a][8],G[a-1][0],G[a][9]]\n",
    "    else:\n",
    "        label=[G[a-9][0],G[a][0],G[a][1],G[a-8][0],G[a][2],G[a-7][0],G[a][3],G[a-6][0],G[a][4],G[a-5][0],G[a][5],G[a-4][0],G[a][6]\n",
    "               ,G[a-3][0],G[a][7],G[a-2][0],G[a][8],G[a-1][0],G[a][9]]\n",
    "    w1=[]\n",
    "    w2=[]\n",
    "    for i in range(19):\n",
    "        w1.append(weight_list[label[i]])\n",
    "    l=[1,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "    if First==True:\n",
    "        for i in range(19):\n",
    "            w2.append(weight_list[label[i]+1])\n",
    "        w=[w1,w2]\n",
    "        x_L=layer_xL('first',x_l,w,F=True)\n",
    "    else:\n",
    "        for i in range(19):\n",
    "            w2.append(weight_list[label[i]+l[i]])\n",
    "        w=[w1,w2]\n",
    "        x_L=layer_xL('group',x_l,w,F=True)\n",
    "    return x_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f4a2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model,G,P,x,y,prune_rate,q=198):\n",
    "    \"\"\"\n",
    "    Structured Channel Group Pruning Function Based on MD-LP (Channel-wise Pruning) \n",
    "    \n",
    "    This function is the main function for pruning in ResNet-56. It achieves \n",
    "    the pruning of layer groups by using the provided convolution layer groups G.\n",
    "    The main process of this function is as follows:\n",
    "    - Pruning Preparation: Based on the hidden layer positions provided by P, \n",
    "      construct the input/output lists for the pruning-required convolutional layers, \n",
    "      as well as the list of convolution kernel parameters and BN layer parameters.\n",
    "    - Layer Group Pruning: In accordance with the sequence in G, the pruning_channel \n",
    "      function is used to perform pruning successively, resulting in the channels \n",
    "      that are retained after pruning, which are labeled as channel_new_label.\n",
    "    - Output/Parameter Update: Based on the channel_new_label, the parameters of the \n",
    "      convolutional layers included in the group, as well as the parameters of the \n",
    "      convolutional layers whose outputs are used as inputs, are updated. And the \n",
    "      input/output lists of the convolutional layers are updated according to the \n",
    "      updated parameters.\n",
    "    \n",
    "    Input:\n",
    "    model : Original Keras ResNet-18\n",
    "    x : Network input samples (used for forward propagation and channel evaluation)\n",
    "    y : Sample labels (used for metric calculation in prune_channel)\n",
    "    prune_rate : Pruning parameter\n",
    "    G : The grouped list of network convolution layers, based on the ReNet network \n",
    "    structure, divides the hidden layers of ResNet. It is used to ensure that the \n",
    "    output results of the network hidden layers within the same group after pruning \n",
    "    can still maintain the same size and can be added together.\n",
    "    \n",
    "    Output:\n",
    "    weight_list: List of weights for each convolutional / BN layer after pruning.\n",
    "    channel_label: Record of the number of retained channels for each layer group.   \n",
    "    \"\"\"\n",
    "    layer_outputs =[layer.output for layer in model.layers] \n",
    "    weight_list=[]\n",
    "    # =========================\n",
    "    # Collect the input/output of the convolutional layers, and the parameters\n",
    "    # of the convolutional layers and BN layers in the ResNet network.\n",
    "    # =========================\n",
    "    for i in range(q+1):\n",
    "        layer=model.layers[i]\n",
    "        if \"conv\" in layer.name:\n",
    "            w=layer.get_weights()\n",
    "            weight_list.append(w)\n",
    "            #print(i)\n",
    "        elif \"dense\" in layer.name:\n",
    "            w,b=layer.get_weights()\n",
    "            weight_list.append([w,b])\n",
    "        elif \"batch_normalization\" in layer.name:\n",
    "            g,b,m,v=layer.get_weights()\n",
    "            weight_list.append([g,b,m,v])\n",
    "        else:\n",
    "            weight_list.append(None)\n",
    "    x_LG=[]\n",
    "    for i in range(len(P)):\n",
    "        activation_model = tf.keras.models.Model(inputs=model.input,outputs=layer_outputs[P[i]])\n",
    "        layer_x=activation_model.predict(x)\n",
    "        x_LG.append(layer_x)\n",
    "    channel_label=[]\n",
    "    b=1\n",
    "    for i in range(len(G)):\n",
    "        if len(G[i])==1:\n",
    "            # =========================\n",
    "            # According to the prune_channel function, the single convolution layer in G is pruned, \n",
    "            # resulting in the retained channels after pruning. Based on the pruning results, the \n",
    "            # network parameters and network input/output are updated.\n",
    "            # =========================\n",
    "            if (b-1)%18==0:\n",
    "                a=G[i][0]\n",
    "                channel_new_label,r_L=prune_channel([x_LG[b+1]],y,prune_rate)\n",
    "                print(len(channel_new_label),0)\n",
    "                #print(weight_list[a][0].shape)\n",
    "                weight_list[a][0]=weight_list[a][0][:,:,:,channel_new_label]\n",
    "                #weight_list[a][1]=weight_list[a][1][channel_new_label]\n",
    "                for j in range(4):\n",
    "                    weight_list[a+1][j]=weight_list[a+1][j][channel_new_label]\n",
    "                if i==0:\n",
    "                    a1=G[i+9][1]\n",
    "                    weight_list[a1][0]=weight_list[a1][0][:,:,channel_new_label,:]\n",
    "                    x_LG=x_block(i,b,G,weight_list,x_LG,First=True)\n",
    "                else:\n",
    "                    a1=G[i+9][0]\n",
    "                    weight_list[a1][0]=weight_list[a1][0][:,:,channel_new_label,:]\n",
    "                    x_LG=x_block(i,b,G,weight_list,x_LG,First=False,R=True)\n",
    "                channel_label.append(len(channel_new_label))\n",
    "                b+=2\n",
    "                continue\n",
    "            else:\n",
    "                a=G[i][0]\n",
    "                aaa=int(((i+1)//10+1)*10)\n",
    "                channel_new_label,r_L=prune_channel([x_LG[b+1]],y,prune_rate)\n",
    "                print(len(channel_new_label),1)\n",
    "                weight_list[a][0]=weight_list[a][0][:,:,:,channel_new_label]\n",
    "                #weight_list[a][1]=weight_list[a][1][channel_new_label]\n",
    "                for j in range(4):\n",
    "                    weight_list[a+1][j]=weight_list[a+1][j][channel_new_label]\n",
    "                channel_label.append(len(channel_new_label))\n",
    "                a1=G[aaa-1][i%10+1]\n",
    "                weight_list[a1][0]=weight_list[a1][0][:,:,channel_new_label,:]\n",
    "                x_LG=x_block(i,b,G,weight_list,x_LG)\n",
    "                if (b-1)%18==16:\n",
    "                    if i==8:\n",
    "                        b-=17\n",
    "                    else:\n",
    "                        b-=16\n",
    "                else:\n",
    "                    b+=2\n",
    "                continue\n",
    "        if len(G[i])>1:\n",
    "            # =========================\n",
    "            # According to the prune_channel function, the layer group in G with multiple convolutional \n",
    "            # layers is pruned to obtain a unified set of pruned channels. Based on the pruning results, \n",
    "            # the network parameters and network input/output are updated.\n",
    "            # =========================\n",
    "            if i==9:\n",
    "                x_LP=get_x(i,b,G,x_LG,weight_list,First=True)\n",
    "            else:\n",
    "                x_LP=get_x(i,b,G,x_LG,weight_list)\n",
    "            channel_new_label,r_L=prune_channel(x_LP,y,prune_rate)\n",
    "            print(len(channel_new_label),2)\n",
    "            for g in G[i]:\n",
    "                weight_list[g][0]=weight_list[g][0][:,:,:,channel_new_label]\n",
    "                #weight_list[g][1]=weight_list[g][1][channel_new_label]\n",
    "            for j in range(8):\n",
    "                weight_list[G[i-8+j][0]][0]=weight_list[G[i-8+j][0]][0][:,:,channel_new_label,:]\n",
    "            if i==9:\n",
    "                for g in G[i]:\n",
    "                    for j in range(4):\n",
    "                        weight_list[g+1][j]=weight_list[g+1][j][channel_new_label]\n",
    "                weight_list[G[i-9][0]][0]=weight_list[G[i-9][0]][0][:,:,channel_new_label,:]\n",
    "            else:\n",
    "                l=[2,2,1,1,1,1,1,1,1,1]\n",
    "                for g in range(10):\n",
    "                    for j in range(4):\n",
    "                        weight_list[G[i][g]+l[g]][j]=weight_list[G[i][g]+l[g]][j][channel_new_label]\n",
    "            if i!=len(G)-1:\n",
    "                weight_list[G[i+1][0]][0]=weight_list[G[i+1][0]][0][:,:,channel_new_label,:]\n",
    "                weight_list[G[i+10][1]][0]=weight_list[G[i+10][1]][0][:,:,channel_new_label,:]\n",
    "            if i==9:\n",
    "                x_LG=x_block(i,b,G,weight_list,x_LG,First=True,R=False,Group=True)\n",
    "                b+=19\n",
    "            else:\n",
    "                x_LG=x_block(i,b,G,weight_list,x_LG,First=False,R=False,Group=True)\n",
    "                b+=18\n",
    "            channel_label.append(len(channel_new_label))\n",
    "            continue\n",
    "    weight_list[q][0]=weight_list[q][0][channel_new_label]\n",
    "    return weight_list,channel_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2428f648-7af9-4278-8686-5024da702c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pr(model,weight_list,channel_label):\n",
    "    \"\"\"This function is used to construct a pruned network by using \n",
    "    the given pruned network structure and parameters.\"\"\"\n",
    "    model_p=Res_model(channel_label)\n",
    "    for i in range(len(weight_list)):\n",
    "        if weight_list[i]!=None:\n",
    "            w = [ww for ww in weight_list[i]]\n",
    "            model_p.layers[i].set_weights(w)\n",
    "    return model_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c451b93e-3b25-44cf-b461-e4edd32e51c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d9dd391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain(model,x_train,y_train,x_test,y_test):\n",
    "    \"\"\"This function is used to fine-tune the pruned network \n",
    "    using the same method as the original network training.\"\"\"\n",
    "    total_steps = epochs * (x_train.shape[0] // batch_size)\n",
    "    warmup_steps = warmup_epochs * (x_train.shape[0] // batch_size)\n",
    "    lr_schedule = WarmUpCosine(initial_lr, total_steps, warmup_steps)\n",
    "    optimizer = CustomWeightDecaySGD(weight_decay=weight_decay,learning_rate=lr_schedule,momentum=0.9,nesterov=True)\n",
    "    loss_fn=tf.keras.losses.CategoricalCrossentropy()\n",
    "    model.compile(optimizer=optimizer,loss=loss_fn,metrics=['accuracy'])\n",
    "    saver = LastNSaver(n=20)\n",
    "    model.fit(datagen.flow(x_train, y_train_onehot,batch_size=batch_size),\n",
    "                            steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=(x_test, y_test_onehot),verbose=2,callbacks=[saver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34c12c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"These functions are used to calculate the FLOPs \n",
    "and the number of parameters of the network.\"\"\"\n",
    "def conv_flops_params(layer, input_shape):\n",
    "    h_in, w_in, cin = input_shape[1:]\n",
    "    h_out, w_out, cout = layer.output_shape[1:]\n",
    "    k_h, k_w = layer.kernel_size\n",
    "    flops = h_out * w_out * cin * cout * k_h * k_w\n",
    "    params = cin * cout * k_h * k_w\n",
    "    if layer.use_bias:\n",
    "        params += cout\n",
    "    return flops, params, (h_out, w_out, cout)\n",
    "def dense_flops_params(layer, input_shape):\n",
    "    cin = input_shape[-1]\n",
    "    cout = layer.units\n",
    "    flops = cin * cout\n",
    "    params = cin * cout\n",
    "    if layer.use_bias:\n",
    "        params += cout\n",
    "    return flops, params, (cout,)\n",
    "def compute_flops_params(model, input_shape=(32, 32, 3)):\n",
    "    total_flops = 0\n",
    "    total_params = 0\n",
    "    dummy_input = tf.zeros((1, *input_shape))\n",
    "    _ = model(dummy_input)\n",
    "    current_shape = (1, *input_shape)\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            flops, params, out_shape = conv_flops_params(layer, current_shape)\n",
    "            total_flops += flops\n",
    "            total_params += params\n",
    "            current_shape = (1, *out_shape)\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            flops, params, out_shape = dense_flops_params(layer, current_shape)\n",
    "            total_flops += flops\n",
    "            total_params += params\n",
    "            current_shape = (1, *out_shape)\n",
    "    return total_flops, total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c45c7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_layers(model,x,y,R=P[1:]):\n",
    "    \"\"\"This function is used to obtain the structural redundancy \n",
    "    criterion of each block in the ResNet-18 network.\"\"\"\n",
    "    layer_outputs =[layer.output for layer in model.layers] \n",
    "    R_L=[]\n",
    "    channel_label=[]\n",
    "    for i in range(len(R)):\n",
    "        print('start')\n",
    "        activation_model = tf.keras.models.Model(inputs=model.input,outputs=layer_outputs[R[i]])\n",
    "        x_L=activation_model.predict(x)\n",
    "        #x_L=layer_x[R[i]]\n",
    "        #print(x_L)\n",
    "        channel_new_label,r_L=prune_channel([x_L],y,0,nnn=15)\n",
    "        r_L=float(r_L)\n",
    "        print('finish')\n",
    "        R_L.append(r_L)\n",
    "        print(r_L)\n",
    "    R_L=np.array(R_L)\n",
    "    LLL=[1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2]\n",
    "    RR_L=[]\n",
    "    iii=0\n",
    "    for k in range(len(LLL)):\n",
    "        if LLL[k]==1:\n",
    "            RR_L.append(R_L[0])\n",
    "        if LLL[k]==2:\n",
    "            RR_L.append(R_L[iii:iii+2].sum()/2)\n",
    "        iii+=LLL[k]\n",
    "    return R_L,RR_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "590d84b3-a696-4d89-aea1-bfbc1f329efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_G(model):\n",
    "    \"\"\"This function is used to obtain the number of channels of the layer group.\"\"\"\n",
    "    C=[]\n",
    "    for i in range(len(G)):\n",
    "        CG=0\n",
    "        for g in G[i]:\n",
    "            a,b,d,c=model.layers[g].output.shape\n",
    "            CG+=c\n",
    "        C.append(CG)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2592c954",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 14ms/step - loss: 0.2072 - accuracy: 0.9352\n",
      "126009984\n"
     ]
    }
   ],
   "source": [
    "P_list=[]\n",
    "E_list=[]\n",
    "F_list=[]\n",
    "#RP_list=[]\n",
    "#RRP_list=[]\n",
    "C_list=[]\n",
    "flops,par=compute_flops_params(model)\n",
    "loss, acc = model.evaluate(x_test, y_test_onehot)\n",
    "C_0=channel_G(model)\n",
    "print(flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fedc74ac-8c3e-45d0-b7e7-7c172ca91b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_FILE = \"training_ResNet56_log.json\"\n",
    "def load_progress():\n",
    "    if os.path.exists(SAVE_FILE):\n",
    "        with open(SAVE_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"results\": [], \n",
    "            \"RR_L\": [],\n",
    "            \"P_list\": [],\n",
    "            \"E_list\": [],\n",
    "            \"F_list\": [],\n",
    "            \"C_list\": [],\n",
    "            \"last_lam_idx\": 0,\n",
    "            \"last_repeat\": 0,\n",
    "            \"RL_exist\": 0,\n",
    "            \"Cri_exist\": 0}\n",
    "def save_progress(progress):\n",
    "    with open(SAVE_FILE, \"w\") as f:\n",
    "        json.dump(progress, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53af22f6-df8c-4957-a688-41c83f207426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "progress = load_progress()\n",
    "start_lr_idx = progress[\"last_lam_idx\"]\n",
    "start_repeat = progress[\"last_repeat\"]\n",
    "If_RL = progress[\"RL_exist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67eb5e7f-5c6f-4bea-aa93-2212a6cb0d0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if If_RL == 0:\n",
    "    model=load_Res()\n",
    "    R_L,RR_L=R_layers(model,x_dist,y_dist)\n",
    "    progress[\"RR_L\"].append(RR_L)\n",
    "    progress[\"RL_exist\"] = 1\n",
    "    save_progress(progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef2eeb9f-9f54-41bf-985f-35a247461e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.125, -0.0625, -0.1875, 0.0625, 0.1875, 0.1875, 0.5, 0.5, 0.4375, 0.4375, 0.25, 0.21875, 0.28125, 0.1875, 0.15625, 0.21875, 0.1875, 0.3125, 0.0, 0.09375, 0.109375, 0.0625, 0.140625, 0.125, 0.109375, 0.15625, 0.15625, 0.515625]]\n"
     ]
    }
   ],
   "source": [
    "print(progress[\"RR_L\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab6d02fe-8adb-4377-8bb1-eb3cf8d90ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe227ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " lambda: Lam=0.625, Repeat=2/3\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "72987892 126009984\n",
      "Epoch 1/200\n",
      "390/390 - 31s - loss: 0.8006 - accuracy: 0.7288 - val_loss: 4.3029 - val_accuracy: 0.2207 - 31s/epoch - 78ms/step\n",
      "Epoch 2/200\n",
      "390/390 - 23s - loss: 0.6843 - accuracy: 0.7665 - val_loss: 7.6280 - val_accuracy: 0.2061 - 23s/epoch - 60ms/step\n",
      "Epoch 3/200\n",
      "390/390 - 23s - loss: 0.6061 - accuracy: 0.7921 - val_loss: 1.7396 - val_accuracy: 0.5807 - 23s/epoch - 60ms/step\n",
      "Epoch 4/200\n",
      "390/390 - 23s - loss: 0.5521 - accuracy: 0.8112 - val_loss: 2.1272 - val_accuracy: 0.5255 - 23s/epoch - 60ms/step\n",
      "Epoch 5/200\n",
      "390/390 - 23s - loss: 0.5151 - accuracy: 0.8226 - val_loss: 1.5497 - val_accuracy: 0.6015 - 23s/epoch - 60ms/step\n",
      "Epoch 6/200\n",
      "390/390 - 24s - loss: 0.4699 - accuracy: 0.8389 - val_loss: 0.7444 - val_accuracy: 0.7551 - 24s/epoch - 61ms/step\n",
      "Epoch 7/200\n",
      "390/390 - 23s - loss: 0.4325 - accuracy: 0.8499 - val_loss: 0.6767 - val_accuracy: 0.7845 - 23s/epoch - 60ms/step\n",
      "Epoch 8/200\n",
      "390/390 - 23s - loss: 0.4064 - accuracy: 0.8601 - val_loss: 0.5123 - val_accuracy: 0.8296 - 23s/epoch - 60ms/step\n",
      "Epoch 9/200\n",
      "390/390 - 23s - loss: 0.3815 - accuracy: 0.8682 - val_loss: 0.5383 - val_accuracy: 0.8208 - 23s/epoch - 59ms/step\n",
      "Epoch 10/200\n",
      "390/390 - 23s - loss: 0.3569 - accuracy: 0.8775 - val_loss: 0.5937 - val_accuracy: 0.8133 - 23s/epoch - 60ms/step\n",
      "Epoch 11/200\n",
      "390/390 - 23s - loss: 0.3456 - accuracy: 0.8803 - val_loss: 0.8701 - val_accuracy: 0.7560 - 23s/epoch - 59ms/step\n",
      "Epoch 12/200\n",
      "390/390 - 23s - loss: 0.3313 - accuracy: 0.8851 - val_loss: 0.4962 - val_accuracy: 0.8384 - 23s/epoch - 60ms/step\n",
      "Epoch 13/200\n",
      "390/390 - 23s - loss: 0.3202 - accuracy: 0.8892 - val_loss: 0.6964 - val_accuracy: 0.7916 - 23s/epoch - 60ms/step\n",
      "Epoch 14/200\n",
      "390/390 - 23s - loss: 0.3111 - accuracy: 0.8923 - val_loss: 0.8670 - val_accuracy: 0.7515 - 23s/epoch - 60ms/step\n",
      "Epoch 15/200\n",
      "390/390 - 23s - loss: 0.3027 - accuracy: 0.8952 - val_loss: 0.4788 - val_accuracy: 0.8495 - 23s/epoch - 59ms/step\n",
      "Epoch 16/200\n",
      "390/390 - 23s - loss: 0.2960 - accuracy: 0.8969 - val_loss: 0.6560 - val_accuracy: 0.8045 - 23s/epoch - 60ms/step\n",
      "Epoch 17/200\n",
      "390/390 - 23s - loss: 0.2840 - accuracy: 0.9011 - val_loss: 0.6493 - val_accuracy: 0.8017 - 23s/epoch - 59ms/step\n",
      "Epoch 18/200\n",
      "390/390 - 23s - loss: 0.2831 - accuracy: 0.9015 - val_loss: 0.4829 - val_accuracy: 0.8483 - 23s/epoch - 58ms/step\n",
      "Epoch 19/200\n",
      "390/390 - 23s - loss: 0.2773 - accuracy: 0.9027 - val_loss: 0.6275 - val_accuracy: 0.8059 - 23s/epoch - 60ms/step\n",
      "Epoch 20/200\n",
      "390/390 - 24s - loss: 0.2714 - accuracy: 0.9053 - val_loss: 0.4541 - val_accuracy: 0.8496 - 24s/epoch - 61ms/step\n",
      "Epoch 21/200\n",
      "390/390 - 23s - loss: 0.2637 - accuracy: 0.9082 - val_loss: 0.4747 - val_accuracy: 0.8519 - 23s/epoch - 60ms/step\n",
      "Epoch 22/200\n",
      "390/390 - 23s - loss: 0.2601 - accuracy: 0.9087 - val_loss: 0.4475 - val_accuracy: 0.8503 - 23s/epoch - 59ms/step\n",
      "Epoch 23/200\n",
      "390/390 - 23s - loss: 0.2599 - accuracy: 0.9091 - val_loss: 0.4869 - val_accuracy: 0.8361 - 23s/epoch - 60ms/step\n",
      "Epoch 24/200\n",
      "390/390 - 23s - loss: 0.2506 - accuracy: 0.9117 - val_loss: 0.4351 - val_accuracy: 0.8601 - 23s/epoch - 60ms/step\n",
      "Epoch 25/200\n",
      "390/390 - 23s - loss: 0.2496 - accuracy: 0.9129 - val_loss: 0.4962 - val_accuracy: 0.8461 - 23s/epoch - 59ms/step\n",
      "Epoch 26/200\n",
      "390/390 - 23s - loss: 0.2426 - accuracy: 0.9160 - val_loss: 0.5949 - val_accuracy: 0.8311 - 23s/epoch - 60ms/step\n",
      "Epoch 27/200\n",
      "390/390 - 23s - loss: 0.2365 - accuracy: 0.9177 - val_loss: 0.6644 - val_accuracy: 0.8157 - 23s/epoch - 59ms/step\n",
      "Epoch 28/200\n",
      "390/390 - 23s - loss: 0.2426 - accuracy: 0.9151 - val_loss: 0.6274 - val_accuracy: 0.8230 - 23s/epoch - 60ms/step\n",
      "Epoch 29/200\n",
      "390/390 - 23s - loss: 0.2354 - accuracy: 0.9169 - val_loss: 0.5805 - val_accuracy: 0.8244 - 23s/epoch - 60ms/step\n",
      "Epoch 30/200\n",
      "390/390 - 23s - loss: 0.2373 - accuracy: 0.9164 - val_loss: 0.5676 - val_accuracy: 0.8221 - 23s/epoch - 59ms/step\n",
      "Epoch 31/200\n",
      "390/390 - 23s - loss: 0.2294 - accuracy: 0.9204 - val_loss: 0.4773 - val_accuracy: 0.8497 - 23s/epoch - 60ms/step\n",
      "Epoch 32/200\n",
      "390/390 - 23s - loss: 0.2283 - accuracy: 0.9195 - val_loss: 0.4565 - val_accuracy: 0.8516 - 23s/epoch - 60ms/step\n",
      "Epoch 33/200\n",
      "390/390 - 23s - loss: 0.2273 - accuracy: 0.9191 - val_loss: 0.4764 - val_accuracy: 0.8531 - 23s/epoch - 60ms/step\n",
      "Epoch 34/200\n",
      "390/390 - 23s - loss: 0.2262 - accuracy: 0.9212 - val_loss: 0.5342 - val_accuracy: 0.8378 - 23s/epoch - 60ms/step\n",
      "Epoch 35/200\n",
      "390/390 - 23s - loss: 0.2213 - accuracy: 0.9220 - val_loss: 0.6794 - val_accuracy: 0.7980 - 23s/epoch - 59ms/step\n",
      "Epoch 36/200\n",
      "390/390 - 24s - loss: 0.2228 - accuracy: 0.9215 - val_loss: 0.5502 - val_accuracy: 0.8287 - 24s/epoch - 60ms/step\n",
      "Epoch 37/200\n",
      "390/390 - 24s - loss: 0.2185 - accuracy: 0.9235 - val_loss: 0.6016 - val_accuracy: 0.8274 - 24s/epoch - 60ms/step\n",
      "Epoch 38/200\n",
      "390/390 - 23s - loss: 0.2123 - accuracy: 0.9264 - val_loss: 0.6887 - val_accuracy: 0.8041 - 23s/epoch - 60ms/step\n",
      "Epoch 39/200\n",
      "390/390 - 23s - loss: 0.2152 - accuracy: 0.9253 - val_loss: 0.4041 - val_accuracy: 0.8764 - 23s/epoch - 60ms/step\n",
      "Epoch 40/200\n",
      "390/390 - 23s - loss: 0.2139 - accuracy: 0.9248 - val_loss: 0.4617 - val_accuracy: 0.8559 - 23s/epoch - 60ms/step\n",
      "Epoch 41/200\n",
      "390/390 - 24s - loss: 0.2128 - accuracy: 0.9255 - val_loss: 0.5172 - val_accuracy: 0.8353 - 24s/epoch - 60ms/step\n",
      "Epoch 42/200\n",
      "390/390 - 23s - loss: 0.2099 - accuracy: 0.9257 - val_loss: 0.4922 - val_accuracy: 0.8469 - 23s/epoch - 60ms/step\n",
      "Epoch 43/200\n",
      "390/390 - 23s - loss: 0.2147 - accuracy: 0.9251 - val_loss: 0.5365 - val_accuracy: 0.8346 - 23s/epoch - 59ms/step\n",
      "Epoch 44/200\n",
      "390/390 - 23s - loss: 0.2047 - accuracy: 0.9286 - val_loss: 0.5986 - val_accuracy: 0.8091 - 23s/epoch - 60ms/step\n",
      "Epoch 45/200\n",
      "390/390 - 23s - loss: 0.2068 - accuracy: 0.9278 - val_loss: 0.5582 - val_accuracy: 0.8451 - 23s/epoch - 59ms/step\n",
      "Epoch 46/200\n",
      "390/390 - 23s - loss: 0.2014 - accuracy: 0.9286 - val_loss: 0.5433 - val_accuracy: 0.8386 - 23s/epoch - 59ms/step\n",
      "Epoch 47/200\n",
      "390/390 - 23s - loss: 0.1981 - accuracy: 0.9309 - val_loss: 0.4751 - val_accuracy: 0.8575 - 23s/epoch - 60ms/step\n",
      "Epoch 48/200\n",
      "390/390 - 23s - loss: 0.2043 - accuracy: 0.9278 - val_loss: 0.4634 - val_accuracy: 0.8563 - 23s/epoch - 59ms/step\n",
      "Epoch 49/200\n",
      "390/390 - 23s - loss: 0.1996 - accuracy: 0.9298 - val_loss: 0.6537 - val_accuracy: 0.8120 - 23s/epoch - 60ms/step\n",
      "Epoch 50/200\n",
      "390/390 - 23s - loss: 0.1971 - accuracy: 0.9309 - val_loss: 0.9478 - val_accuracy: 0.7739 - 23s/epoch - 59ms/step\n",
      "Epoch 51/200\n",
      "390/390 - 23s - loss: 0.1971 - accuracy: 0.9318 - val_loss: 0.4327 - val_accuracy: 0.8718 - 23s/epoch - 59ms/step\n",
      "Epoch 52/200\n",
      "390/390 - 23s - loss: 0.2005 - accuracy: 0.9287 - val_loss: 0.5322 - val_accuracy: 0.8459 - 23s/epoch - 59ms/step\n",
      "Epoch 53/200\n",
      "390/390 - 23s - loss: 0.1953 - accuracy: 0.9316 - val_loss: 0.7027 - val_accuracy: 0.8165 - 23s/epoch - 59ms/step\n",
      "Epoch 54/200\n",
      "390/390 - 23s - loss: 0.1919 - accuracy: 0.9326 - val_loss: 0.4857 - val_accuracy: 0.8555 - 23s/epoch - 59ms/step\n",
      "Epoch 55/200\n",
      "390/390 - 24s - loss: 0.1938 - accuracy: 0.9327 - val_loss: 0.4705 - val_accuracy: 0.8611 - 24s/epoch - 60ms/step\n",
      "Epoch 56/200\n",
      "390/390 - 23s - loss: 0.1936 - accuracy: 0.9333 - val_loss: 0.7282 - val_accuracy: 0.7913 - 23s/epoch - 60ms/step\n",
      "Epoch 57/200\n",
      "390/390 - 23s - loss: 0.1875 - accuracy: 0.9345 - val_loss: 0.4503 - val_accuracy: 0.8658 - 23s/epoch - 59ms/step\n",
      "Epoch 58/200\n",
      "390/390 - 24s - loss: 0.1899 - accuracy: 0.9327 - val_loss: 0.6021 - val_accuracy: 0.8275 - 24s/epoch - 60ms/step\n",
      "Epoch 59/200\n",
      "390/390 - 23s - loss: 0.1879 - accuracy: 0.9345 - val_loss: 0.4285 - val_accuracy: 0.8638 - 23s/epoch - 59ms/step\n",
      "Epoch 60/200\n",
      "390/390 - 23s - loss: 0.1858 - accuracy: 0.9352 - val_loss: 0.6219 - val_accuracy: 0.8216 - 23s/epoch - 59ms/step\n",
      "Epoch 61/200\n",
      "390/390 - 23s - loss: 0.1842 - accuracy: 0.9363 - val_loss: 0.5248 - val_accuracy: 0.8332 - 23s/epoch - 60ms/step\n",
      "Epoch 62/200\n",
      "390/390 - 23s - loss: 0.1888 - accuracy: 0.9350 - val_loss: 0.4737 - val_accuracy: 0.8526 - 23s/epoch - 60ms/step\n",
      "Epoch 63/200\n",
      "390/390 - 23s - loss: 0.1822 - accuracy: 0.9363 - val_loss: 0.4693 - val_accuracy: 0.8589 - 23s/epoch - 59ms/step\n",
      "Epoch 64/200\n",
      "390/390 - 23s - loss: 0.1819 - accuracy: 0.9370 - val_loss: 0.5305 - val_accuracy: 0.8392 - 23s/epoch - 60ms/step\n",
      "Epoch 65/200\n",
      "390/390 - 23s - loss: 0.1826 - accuracy: 0.9363 - val_loss: 0.4468 - val_accuracy: 0.8634 - 23s/epoch - 60ms/step\n",
      "Epoch 66/200\n",
      "390/390 - 23s - loss: 0.1839 - accuracy: 0.9367 - val_loss: 0.3941 - val_accuracy: 0.8766 - 23s/epoch - 60ms/step\n",
      "Epoch 67/200\n",
      "390/390 - 23s - loss: 0.1827 - accuracy: 0.9357 - val_loss: 0.4971 - val_accuracy: 0.8582 - 23s/epoch - 60ms/step\n",
      "Epoch 68/200\n",
      "390/390 - 24s - loss: 0.1798 - accuracy: 0.9365 - val_loss: 0.5323 - val_accuracy: 0.8382 - 24s/epoch - 61ms/step\n",
      "Epoch 69/200\n",
      "390/390 - 23s - loss: 0.1780 - accuracy: 0.9367 - val_loss: 0.4127 - val_accuracy: 0.8757 - 23s/epoch - 60ms/step\n",
      "Epoch 70/200\n",
      "390/390 - 23s - loss: 0.1803 - accuracy: 0.9376 - val_loss: 0.6135 - val_accuracy: 0.8332 - 23s/epoch - 59ms/step\n",
      "Epoch 71/200\n",
      "390/390 - 23s - loss: 0.1806 - accuracy: 0.9367 - val_loss: 0.4759 - val_accuracy: 0.8521 - 23s/epoch - 60ms/step\n",
      "Epoch 72/200\n",
      "390/390 - 23s - loss: 0.1782 - accuracy: 0.9372 - val_loss: 0.4679 - val_accuracy: 0.8710 - 23s/epoch - 59ms/step\n",
      "Epoch 73/200\n",
      "390/390 - 23s - loss: 0.1751 - accuracy: 0.9388 - val_loss: 0.5491 - val_accuracy: 0.8437 - 23s/epoch - 60ms/step\n",
      "Epoch 74/200\n",
      "390/390 - 24s - loss: 0.1759 - accuracy: 0.9383 - val_loss: 0.3919 - val_accuracy: 0.8795 - 24s/epoch - 60ms/step\n",
      "Epoch 75/200\n",
      "390/390 - 24s - loss: 0.1734 - accuracy: 0.9402 - val_loss: 0.6113 - val_accuracy: 0.8239 - 24s/epoch - 61ms/step\n",
      "Epoch 76/200\n",
      "390/390 - 23s - loss: 0.1732 - accuracy: 0.9393 - val_loss: 0.5347 - val_accuracy: 0.8401 - 23s/epoch - 59ms/step\n",
      "Epoch 77/200\n",
      "390/390 - 23s - loss: 0.1684 - accuracy: 0.9413 - val_loss: 0.4541 - val_accuracy: 0.8613 - 23s/epoch - 60ms/step\n",
      "Epoch 78/200\n",
      "390/390 - 23s - loss: 0.1730 - accuracy: 0.9398 - val_loss: 0.4419 - val_accuracy: 0.8652 - 23s/epoch - 60ms/step\n",
      "Epoch 79/200\n",
      "390/390 - 23s - loss: 0.1670 - accuracy: 0.9408 - val_loss: 0.4177 - val_accuracy: 0.8720 - 23s/epoch - 60ms/step\n",
      "Epoch 80/200\n",
      "390/390 - 24s - loss: 0.1711 - accuracy: 0.9411 - val_loss: 0.4648 - val_accuracy: 0.8578 - 24s/epoch - 60ms/step\n",
      "Epoch 81/200\n",
      "390/390 - 24s - loss: 0.1706 - accuracy: 0.9392 - val_loss: 0.4601 - val_accuracy: 0.8632 - 24s/epoch - 60ms/step\n",
      "Epoch 82/200\n",
      "390/390 - 24s - loss: 0.1636 - accuracy: 0.9428 - val_loss: 0.5323 - val_accuracy: 0.8527 - 24s/epoch - 60ms/step\n",
      "Epoch 83/200\n",
      "390/390 - 23s - loss: 0.1689 - accuracy: 0.9414 - val_loss: 0.4431 - val_accuracy: 0.8675 - 23s/epoch - 59ms/step\n",
      "Epoch 84/200\n",
      "390/390 - 23s - loss: 0.1619 - accuracy: 0.9433 - val_loss: 0.3837 - val_accuracy: 0.8820 - 23s/epoch - 60ms/step\n",
      "Epoch 85/200\n",
      "390/390 - 23s - loss: 0.1692 - accuracy: 0.9400 - val_loss: 0.4761 - val_accuracy: 0.8492 - 23s/epoch - 60ms/step\n",
      "Epoch 86/200\n",
      "390/390 - 23s - loss: 0.1595 - accuracy: 0.9440 - val_loss: 0.5921 - val_accuracy: 0.8291 - 23s/epoch - 59ms/step\n",
      "Epoch 87/200\n",
      "390/390 - 23s - loss: 0.1648 - accuracy: 0.9422 - val_loss: 0.4600 - val_accuracy: 0.8639 - 23s/epoch - 59ms/step\n",
      "Epoch 88/200\n",
      "390/390 - 24s - loss: 0.1649 - accuracy: 0.9411 - val_loss: 0.5413 - val_accuracy: 0.8468 - 24s/epoch - 60ms/step\n",
      "Epoch 89/200\n",
      "390/390 - 23s - loss: 0.1627 - accuracy: 0.9426 - val_loss: 0.4347 - val_accuracy: 0.8665 - 23s/epoch - 60ms/step\n",
      "Epoch 90/200\n",
      "390/390 - 23s - loss: 0.1645 - accuracy: 0.9432 - val_loss: 0.6182 - val_accuracy: 0.8194 - 23s/epoch - 59ms/step\n",
      "Epoch 91/200\n",
      "390/390 - 23s - loss: 0.1573 - accuracy: 0.9449 - val_loss: 0.4965 - val_accuracy: 0.8587 - 23s/epoch - 60ms/step\n",
      "Epoch 92/200\n",
      "390/390 - 23s - loss: 0.1607 - accuracy: 0.9435 - val_loss: 0.6165 - val_accuracy: 0.8166 - 23s/epoch - 60ms/step\n",
      "Epoch 93/200\n",
      "390/390 - 23s - loss: 0.1642 - accuracy: 0.9427 - val_loss: 0.4864 - val_accuracy: 0.8576 - 23s/epoch - 60ms/step\n",
      "Epoch 94/200\n",
      "390/390 - 23s - loss: 0.1583 - accuracy: 0.9447 - val_loss: 0.3809 - val_accuracy: 0.8809 - 23s/epoch - 59ms/step\n",
      "Epoch 95/200\n",
      "390/390 - 24s - loss: 0.1567 - accuracy: 0.9445 - val_loss: 0.5652 - val_accuracy: 0.8355 - 24s/epoch - 61ms/step\n",
      "Epoch 96/200\n",
      "390/390 - 23s - loss: 0.1544 - accuracy: 0.9458 - val_loss: 0.4329 - val_accuracy: 0.8721 - 23s/epoch - 60ms/step\n",
      "Epoch 97/200\n",
      "390/390 - 23s - loss: 0.1578 - accuracy: 0.9444 - val_loss: 0.6411 - val_accuracy: 0.8300 - 23s/epoch - 60ms/step\n",
      "Epoch 98/200\n",
      "390/390 - 23s - loss: 0.1563 - accuracy: 0.9454 - val_loss: 0.4463 - val_accuracy: 0.8736 - 23s/epoch - 60ms/step\n",
      "Epoch 99/200\n",
      "390/390 - 24s - loss: 0.1532 - accuracy: 0.9468 - val_loss: 0.4610 - val_accuracy: 0.8646 - 24s/epoch - 60ms/step\n",
      "Epoch 100/200\n",
      "390/390 - 23s - loss: 0.1507 - accuracy: 0.9466 - val_loss: 0.4713 - val_accuracy: 0.8611 - 23s/epoch - 60ms/step\n",
      "Epoch 101/200\n",
      "390/390 - 23s - loss: 0.1533 - accuracy: 0.9464 - val_loss: 0.6518 - val_accuracy: 0.8261 - 23s/epoch - 59ms/step\n",
      "Epoch 102/200\n",
      "390/390 - 23s - loss: 0.1525 - accuracy: 0.9462 - val_loss: 0.4471 - val_accuracy: 0.8646 - 23s/epoch - 60ms/step\n",
      "Epoch 103/200\n",
      "390/390 - 23s - loss: 0.1567 - accuracy: 0.9447 - val_loss: 0.5353 - val_accuracy: 0.8566 - 23s/epoch - 59ms/step\n",
      "Epoch 104/200\n",
      "390/390 - 23s - loss: 0.1485 - accuracy: 0.9479 - val_loss: 0.4313 - val_accuracy: 0.8678 - 23s/epoch - 60ms/step\n",
      "Epoch 105/200\n",
      "390/390 - 24s - loss: 0.1495 - accuracy: 0.9483 - val_loss: 0.4124 - val_accuracy: 0.8786 - 24s/epoch - 60ms/step\n",
      "Epoch 106/200\n",
      "390/390 - 23s - loss: 0.1478 - accuracy: 0.9471 - val_loss: 0.7718 - val_accuracy: 0.8021 - 23s/epoch - 60ms/step\n",
      "Epoch 107/200\n",
      "390/390 - 23s - loss: 0.1532 - accuracy: 0.9461 - val_loss: 0.5172 - val_accuracy: 0.8478 - 23s/epoch - 60ms/step\n",
      "Epoch 108/200\n",
      "390/390 - 24s - loss: 0.1491 - accuracy: 0.9480 - val_loss: 0.4513 - val_accuracy: 0.8635 - 24s/epoch - 61ms/step\n",
      "Epoch 109/200\n",
      "390/390 - 23s - loss: 0.1437 - accuracy: 0.9495 - val_loss: 0.4891 - val_accuracy: 0.8640 - 23s/epoch - 59ms/step\n",
      "Epoch 110/200\n",
      "390/390 - 23s - loss: 0.1460 - accuracy: 0.9481 - val_loss: 0.4866 - val_accuracy: 0.8593 - 23s/epoch - 60ms/step\n",
      "Epoch 111/200\n",
      "390/390 - 24s - loss: 0.1459 - accuracy: 0.9489 - val_loss: 0.4318 - val_accuracy: 0.8704 - 24s/epoch - 61ms/step\n",
      "Epoch 112/200\n",
      "390/390 - 23s - loss: 0.1414 - accuracy: 0.9499 - val_loss: 0.5003 - val_accuracy: 0.8536 - 23s/epoch - 59ms/step\n",
      "Epoch 113/200\n",
      "390/390 - 23s - loss: 0.1505 - accuracy: 0.9468 - val_loss: 0.5122 - val_accuracy: 0.8471 - 23s/epoch - 59ms/step\n",
      "Epoch 114/200\n",
      "390/390 - 23s - loss: 0.1423 - accuracy: 0.9498 - val_loss: 0.4710 - val_accuracy: 0.8599 - 23s/epoch - 60ms/step\n",
      "Epoch 115/200\n",
      "390/390 - 23s - loss: 0.1455 - accuracy: 0.9490 - val_loss: 0.4656 - val_accuracy: 0.8620 - 23s/epoch - 59ms/step\n",
      "Epoch 116/200\n",
      "390/390 - 23s - loss: 0.1430 - accuracy: 0.9499 - val_loss: 0.5071 - val_accuracy: 0.8548 - 23s/epoch - 60ms/step\n",
      "Epoch 117/200\n",
      "390/390 - 23s - loss: 0.1375 - accuracy: 0.9504 - val_loss: 0.4924 - val_accuracy: 0.8665 - 23s/epoch - 60ms/step\n",
      "Epoch 118/200\n",
      "390/390 - 23s - loss: 0.1421 - accuracy: 0.9503 - val_loss: 0.4234 - val_accuracy: 0.8790 - 23s/epoch - 60ms/step\n",
      "Epoch 119/200\n",
      "390/390 - 23s - loss: 0.1341 - accuracy: 0.9523 - val_loss: 0.3752 - val_accuracy: 0.8902 - 23s/epoch - 60ms/step\n",
      "Epoch 120/200\n",
      "390/390 - 23s - loss: 0.1434 - accuracy: 0.9492 - val_loss: 0.4244 - val_accuracy: 0.8742 - 23s/epoch - 60ms/step\n",
      "Epoch 121/200\n",
      "390/390 - 24s - loss: 0.1392 - accuracy: 0.9503 - val_loss: 0.3589 - val_accuracy: 0.8849 - 24s/epoch - 61ms/step\n",
      "Epoch 122/200\n",
      "390/390 - 23s - loss: 0.1389 - accuracy: 0.9509 - val_loss: 0.4352 - val_accuracy: 0.8713 - 23s/epoch - 60ms/step\n",
      "Epoch 123/200\n",
      "390/390 - 24s - loss: 0.1400 - accuracy: 0.9504 - val_loss: 0.4175 - val_accuracy: 0.8729 - 24s/epoch - 60ms/step\n",
      "Epoch 124/200\n",
      "390/390 - 23s - loss: 0.1316 - accuracy: 0.9543 - val_loss: 0.4269 - val_accuracy: 0.8807 - 23s/epoch - 60ms/step\n",
      "Epoch 125/200\n",
      "390/390 - 23s - loss: 0.1324 - accuracy: 0.9544 - val_loss: 0.4992 - val_accuracy: 0.8499 - 23s/epoch - 59ms/step\n",
      "Epoch 126/200\n",
      "390/390 - 23s - loss: 0.1389 - accuracy: 0.9498 - val_loss: 0.6977 - val_accuracy: 0.7974 - 23s/epoch - 60ms/step\n",
      "Epoch 127/200\n",
      "390/390 - 23s - loss: 0.1355 - accuracy: 0.9532 - val_loss: 0.4632 - val_accuracy: 0.8698 - 23s/epoch - 60ms/step\n",
      "Epoch 128/200\n",
      "390/390 - 24s - loss: 0.1343 - accuracy: 0.9527 - val_loss: 0.4056 - val_accuracy: 0.8794 - 24s/epoch - 60ms/step\n",
      "Epoch 129/200\n",
      "390/390 - 23s - loss: 0.1319 - accuracy: 0.9537 - val_loss: 0.3838 - val_accuracy: 0.8770 - 23s/epoch - 60ms/step\n",
      "Epoch 130/200\n",
      "390/390 - 23s - loss: 0.1317 - accuracy: 0.9537 - val_loss: 0.4315 - val_accuracy: 0.8782 - 23s/epoch - 59ms/step\n",
      "Epoch 131/200\n",
      "390/390 - 24s - loss: 0.1264 - accuracy: 0.9557 - val_loss: 0.3476 - val_accuracy: 0.8966 - 24s/epoch - 60ms/step\n",
      "Epoch 132/200\n",
      "390/390 - 23s - loss: 0.1296 - accuracy: 0.9539 - val_loss: 0.4204 - val_accuracy: 0.8799 - 23s/epoch - 60ms/step\n",
      "Epoch 133/200\n",
      "390/390 - 23s - loss: 0.1288 - accuracy: 0.9546 - val_loss: 0.5045 - val_accuracy: 0.8568 - 23s/epoch - 60ms/step\n",
      "Epoch 134/200\n",
      "390/390 - 23s - loss: 0.1303 - accuracy: 0.9537 - val_loss: 0.4602 - val_accuracy: 0.8667 - 23s/epoch - 59ms/step\n",
      "Epoch 135/200\n",
      "390/390 - 24s - loss: 0.1302 - accuracy: 0.9545 - val_loss: 0.4144 - val_accuracy: 0.8750 - 24s/epoch - 60ms/step\n",
      "Epoch 136/200\n",
      "390/390 - 23s - loss: 0.1241 - accuracy: 0.9571 - val_loss: 0.3682 - val_accuracy: 0.8914 - 23s/epoch - 60ms/step\n",
      "Epoch 137/200\n",
      "390/390 - 23s - loss: 0.1286 - accuracy: 0.9545 - val_loss: 0.3708 - val_accuracy: 0.8894 - 23s/epoch - 59ms/step\n",
      "Epoch 138/200\n",
      "390/390 - 23s - loss: 0.1213 - accuracy: 0.9572 - val_loss: 0.3422 - val_accuracy: 0.8976 - 23s/epoch - 60ms/step\n",
      "Epoch 139/200\n",
      "390/390 - 23s - loss: 0.1265 - accuracy: 0.9549 - val_loss: 0.4446 - val_accuracy: 0.8713 - 23s/epoch - 60ms/step\n",
      "Epoch 140/200\n",
      "390/390 - 23s - loss: 0.1188 - accuracy: 0.9575 - val_loss: 0.3489 - val_accuracy: 0.8912 - 23s/epoch - 59ms/step\n",
      "Epoch 141/200\n",
      "390/390 - 24s - loss: 0.1217 - accuracy: 0.9577 - val_loss: 0.4232 - val_accuracy: 0.8743 - 24s/epoch - 61ms/step\n",
      "Epoch 142/200\n",
      "390/390 - 23s - loss: 0.1205 - accuracy: 0.9588 - val_loss: 0.5306 - val_accuracy: 0.8593 - 23s/epoch - 59ms/step\n",
      "Epoch 143/200\n",
      "390/390 - 23s - loss: 0.1205 - accuracy: 0.9573 - val_loss: 0.3843 - val_accuracy: 0.8859 - 23s/epoch - 60ms/step\n",
      "Epoch 144/200\n",
      "390/390 - 23s - loss: 0.1177 - accuracy: 0.9581 - val_loss: 0.4821 - val_accuracy: 0.8689 - 23s/epoch - 59ms/step\n",
      "Epoch 145/200\n",
      "390/390 - 23s - loss: 0.1159 - accuracy: 0.9590 - val_loss: 0.4385 - val_accuracy: 0.8780 - 23s/epoch - 59ms/step\n",
      "Epoch 146/200\n",
      "390/390 - 23s - loss: 0.1177 - accuracy: 0.9590 - val_loss: 0.4626 - val_accuracy: 0.8680 - 23s/epoch - 60ms/step\n",
      "Epoch 147/200\n",
      "390/390 - 23s - loss: 0.1126 - accuracy: 0.9612 - val_loss: 0.5537 - val_accuracy: 0.8442 - 23s/epoch - 60ms/step\n",
      "Epoch 148/200\n",
      "390/390 - 23s - loss: 0.1132 - accuracy: 0.9608 - val_loss: 0.4413 - val_accuracy: 0.8726 - 23s/epoch - 60ms/step\n",
      "Epoch 149/200\n",
      "390/390 - 23s - loss: 0.1159 - accuracy: 0.9595 - val_loss: 0.4182 - val_accuracy: 0.8771 - 23s/epoch - 60ms/step\n",
      "Epoch 150/200\n",
      "390/390 - 23s - loss: 0.1092 - accuracy: 0.9622 - val_loss: 0.4610 - val_accuracy: 0.8701 - 23s/epoch - 60ms/step\n",
      "Epoch 151/200\n",
      "390/390 - 23s - loss: 0.1104 - accuracy: 0.9618 - val_loss: 0.4331 - val_accuracy: 0.8752 - 23s/epoch - 60ms/step\n",
      "Epoch 152/200\n",
      "390/390 - 23s - loss: 0.1080 - accuracy: 0.9629 - val_loss: 0.4100 - val_accuracy: 0.8826 - 23s/epoch - 59ms/step\n",
      "Epoch 153/200\n",
      "390/390 - 23s - loss: 0.1104 - accuracy: 0.9613 - val_loss: 0.3467 - val_accuracy: 0.8966 - 23s/epoch - 60ms/step\n",
      "Epoch 154/200\n",
      "390/390 - 23s - loss: 0.1055 - accuracy: 0.9631 - val_loss: 0.4594 - val_accuracy: 0.8693 - 23s/epoch - 60ms/step\n",
      "Epoch 155/200\n",
      "390/390 - 24s - loss: 0.1079 - accuracy: 0.9619 - val_loss: 0.4228 - val_accuracy: 0.8771 - 24s/epoch - 60ms/step\n",
      "Epoch 156/200\n",
      "390/390 - 23s - loss: 0.1045 - accuracy: 0.9631 - val_loss: 0.3653 - val_accuracy: 0.8900 - 23s/epoch - 59ms/step\n",
      "Epoch 157/200\n",
      "390/390 - 23s - loss: 0.1008 - accuracy: 0.9652 - val_loss: 0.3889 - val_accuracy: 0.8885 - 23s/epoch - 59ms/step\n",
      "Epoch 158/200\n",
      "390/390 - 23s - loss: 0.1022 - accuracy: 0.9653 - val_loss: 0.4849 - val_accuracy: 0.8675 - 23s/epoch - 59ms/step\n",
      "Epoch 159/200\n",
      "390/390 - 23s - loss: 0.0997 - accuracy: 0.9652 - val_loss: 0.4412 - val_accuracy: 0.8762 - 23s/epoch - 60ms/step\n",
      "Epoch 160/200\n",
      "390/390 - 23s - loss: 0.0953 - accuracy: 0.9663 - val_loss: 0.3825 - val_accuracy: 0.8927 - 23s/epoch - 60ms/step\n",
      "Epoch 161/200\n",
      "390/390 - 23s - loss: 0.0982 - accuracy: 0.9660 - val_loss: 0.6425 - val_accuracy: 0.8299 - 23s/epoch - 60ms/step\n",
      "Epoch 162/200\n",
      "390/390 - 23s - loss: 0.0949 - accuracy: 0.9680 - val_loss: 0.4461 - val_accuracy: 0.8734 - 23s/epoch - 60ms/step\n",
      "Epoch 163/200\n",
      "390/390 - 24s - loss: 0.0967 - accuracy: 0.9673 - val_loss: 0.5527 - val_accuracy: 0.8485 - 24s/epoch - 60ms/step\n",
      "Epoch 164/200\n",
      "390/390 - 23s - loss: 0.0915 - accuracy: 0.9692 - val_loss: 0.4080 - val_accuracy: 0.8828 - 23s/epoch - 60ms/step\n",
      "Epoch 165/200\n",
      "390/390 - 24s - loss: 0.0875 - accuracy: 0.9700 - val_loss: 0.5085 - val_accuracy: 0.8649 - 24s/epoch - 61ms/step\n",
      "Epoch 166/200\n",
      "390/390 - 23s - loss: 0.0850 - accuracy: 0.9707 - val_loss: 0.4482 - val_accuracy: 0.8758 - 23s/epoch - 60ms/step\n",
      "Epoch 167/200\n",
      "390/390 - 24s - loss: 0.0844 - accuracy: 0.9708 - val_loss: 0.4188 - val_accuracy: 0.8808 - 24s/epoch - 60ms/step\n",
      "Epoch 168/200\n",
      "390/390 - 23s - loss: 0.0854 - accuracy: 0.9706 - val_loss: 0.3632 - val_accuracy: 0.8964 - 23s/epoch - 60ms/step\n",
      "Epoch 169/200\n",
      "390/390 - 23s - loss: 0.0835 - accuracy: 0.9728 - val_loss: 0.3734 - val_accuracy: 0.8920 - 23s/epoch - 59ms/step\n",
      "Epoch 170/200\n",
      "390/390 - 23s - loss: 0.0784 - accuracy: 0.9737 - val_loss: 0.3820 - val_accuracy: 0.8904 - 23s/epoch - 59ms/step\n",
      "Epoch 171/200\n",
      "390/390 - 23s - loss: 0.0764 - accuracy: 0.9744 - val_loss: 0.3110 - val_accuracy: 0.9111 - 23s/epoch - 60ms/step\n",
      "Epoch 172/200\n",
      "390/390 - 23s - loss: 0.0779 - accuracy: 0.9734 - val_loss: 0.3310 - val_accuracy: 0.9025 - 23s/epoch - 59ms/step\n",
      "Epoch 173/200\n",
      "390/390 - 23s - loss: 0.0744 - accuracy: 0.9748 - val_loss: 0.3295 - val_accuracy: 0.9049 - 23s/epoch - 60ms/step\n",
      "Epoch 174/200\n",
      "390/390 - 23s - loss: 0.0711 - accuracy: 0.9763 - val_loss: 0.2967 - val_accuracy: 0.9115 - 23s/epoch - 59ms/step\n",
      "Epoch 175/200\n",
      "390/390 - 23s - loss: 0.0700 - accuracy: 0.9768 - val_loss: 0.3632 - val_accuracy: 0.8940 - 23s/epoch - 60ms/step\n",
      "Epoch 176/200\n",
      "390/390 - 23s - loss: 0.0665 - accuracy: 0.9778 - val_loss: 0.3217 - val_accuracy: 0.9123 - 23s/epoch - 60ms/step\n",
      "Epoch 177/200\n",
      "390/390 - 23s - loss: 0.0652 - accuracy: 0.9781 - val_loss: 0.3242 - val_accuracy: 0.9097 - 23s/epoch - 60ms/step\n",
      "Epoch 178/200\n",
      "390/390 - 23s - loss: 0.0601 - accuracy: 0.9804 - val_loss: 0.3520 - val_accuracy: 0.9013 - 23s/epoch - 60ms/step\n",
      "Epoch 179/200\n",
      "390/390 - 23s - loss: 0.0608 - accuracy: 0.9801 - val_loss: 0.4391 - val_accuracy: 0.8769 - 23s/epoch - 59ms/step\n",
      "Epoch 180/200\n",
      "390/390 - 24s - loss: 0.0552 - accuracy: 0.9823 - val_loss: 0.3440 - val_accuracy: 0.9021 - 24s/epoch - 60ms/step\n",
      "Epoch 181/200\n",
      "390/390 - 24s - loss: 0.0529 - accuracy: 0.9828 - val_loss: 0.3635 - val_accuracy: 0.9001 - 24s/epoch - 61ms/step\n",
      "Epoch 182/200\n",
      "390/390 - 23s - loss: 0.0483 - accuracy: 0.9846 - val_loss: 0.3128 - val_accuracy: 0.9097 - 23s/epoch - 59ms/step\n",
      "Epoch 183/200\n",
      "390/390 - 24s - loss: 0.0483 - accuracy: 0.9845 - val_loss: 0.3097 - val_accuracy: 0.9113 - 24s/epoch - 61ms/step\n",
      "Epoch 184/200\n",
      "390/390 - 23s - loss: 0.0455 - accuracy: 0.9863 - val_loss: 0.2998 - val_accuracy: 0.9169 - 23s/epoch - 60ms/step\n",
      "Epoch 185/200\n",
      "390/390 - 23s - loss: 0.0415 - accuracy: 0.9875 - val_loss: 0.2649 - val_accuracy: 0.9241 - 23s/epoch - 60ms/step\n",
      "Epoch 186/200\n",
      "390/390 - 23s - loss: 0.0397 - accuracy: 0.9887 - val_loss: 0.2809 - val_accuracy: 0.9196 - 23s/epoch - 60ms/step\n",
      "Epoch 187/200\n",
      "390/390 - 23s - loss: 0.0362 - accuracy: 0.9901 - val_loss: 0.2586 - val_accuracy: 0.9257 - 23s/epoch - 60ms/step\n",
      "Epoch 188/200\n",
      "390/390 - 24s - loss: 0.0343 - accuracy: 0.9905 - val_loss: 0.2897 - val_accuracy: 0.9186 - 24s/epoch - 60ms/step\n",
      "Epoch 189/200\n",
      "390/390 - 23s - loss: 0.0324 - accuracy: 0.9919 - val_loss: 0.2498 - val_accuracy: 0.9300 - 23s/epoch - 60ms/step\n",
      "Epoch 190/200\n",
      "390/390 - 23s - loss: 0.0307 - accuracy: 0.9923 - val_loss: 0.2799 - val_accuracy: 0.9207 - 23s/epoch - 59ms/step\n",
      "Epoch 191/200\n",
      "390/390 - 24s - loss: 0.0275 - accuracy: 0.9941 - val_loss: 0.2508 - val_accuracy: 0.9294 - 24s/epoch - 61ms/step\n",
      "Epoch 192/200\n",
      "390/390 - 23s - loss: 0.0295 - accuracy: 0.9930 - val_loss: 0.2513 - val_accuracy: 0.9272 - 23s/epoch - 60ms/step\n",
      "Epoch 193/200\n",
      "390/390 - 23s - loss: 0.0283 - accuracy: 0.9940 - val_loss: 0.2362 - val_accuracy: 0.9313 - 23s/epoch - 60ms/step\n",
      "Epoch 194/200\n",
      "390/390 - 23s - loss: 0.0286 - accuracy: 0.9944 - val_loss: 0.2488 - val_accuracy: 0.9278 - 23s/epoch - 60ms/step\n",
      "Epoch 195/200\n",
      "390/390 - 24s - loss: 0.0281 - accuracy: 0.9949 - val_loss: 0.2247 - val_accuracy: 0.9327 - 24s/epoch - 61ms/step\n",
      "Epoch 196/200\n",
      "390/390 - 23s - loss: 0.0299 - accuracy: 0.9947 - val_loss: 0.2288 - val_accuracy: 0.9334 - 23s/epoch - 60ms/step\n",
      "Epoch 197/200\n",
      "390/390 - 23s - loss: 0.0313 - accuracy: 0.9948 - val_loss: 0.2246 - val_accuracy: 0.9326 - 23s/epoch - 59ms/step\n",
      "Epoch 198/200\n",
      "390/390 - 23s - loss: 0.0335 - accuracy: 0.9953 - val_loss: 0.2241 - val_accuracy: 0.9331 - 23s/epoch - 59ms/step\n",
      "Epoch 199/200\n",
      "390/390 - 23s - loss: 0.0377 - accuracy: 0.9955 - val_loss: 0.2236 - val_accuracy: 0.9321 - 23s/epoch - 59ms/step\n",
      "Epoch 200/200\n",
      "390/390 - 23s - loss: 0.0444 - accuracy: 0.9943 - val_loss: 0.2229 - val_accuracy: 0.9323 - 23s/epoch - 60ms/step\n",
      " Using best val_acc=0.9334 from last 20 epochs\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2288 - accuracy: 0.9334\n",
      " Finished: Lam=0.625, Repeat=2, Acc=0.9334\n",
      "\n",
      " lambda: Lam=0.625, Repeat=3/3\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "72987892 126009984\n",
      "Epoch 1/200\n",
      "390/390 - 28s - loss: 0.7996 - accuracy: 0.7309 - val_loss: 12.4209 - val_accuracy: 0.2242 - 28s/epoch - 72ms/step\n",
      "Epoch 2/200\n",
      "390/390 - 23s - loss: 0.6839 - accuracy: 0.7649 - val_loss: 2.7467 - val_accuracy: 0.4379 - 23s/epoch - 59ms/step\n",
      "Epoch 3/200\n",
      "390/390 - 23s - loss: 0.5966 - accuracy: 0.7943 - val_loss: 1.3585 - val_accuracy: 0.6201 - 23s/epoch - 60ms/step\n",
      "Epoch 4/200\n",
      "390/390 - 23s - loss: 0.5547 - accuracy: 0.8097 - val_loss: 0.8865 - val_accuracy: 0.7086 - 23s/epoch - 60ms/step\n",
      "Epoch 5/200\n",
      "390/390 - 24s - loss: 0.5086 - accuracy: 0.8270 - val_loss: 0.9522 - val_accuracy: 0.7006 - 24s/epoch - 61ms/step\n",
      "Epoch 6/200\n",
      "390/390 - 24s - loss: 0.4676 - accuracy: 0.8383 - val_loss: 1.1413 - val_accuracy: 0.6853 - 24s/epoch - 61ms/step\n",
      "Epoch 7/200\n",
      "390/390 - 24s - loss: 0.4268 - accuracy: 0.8541 - val_loss: 0.8429 - val_accuracy: 0.7343 - 24s/epoch - 62ms/step\n",
      "Epoch 8/200\n",
      "390/390 - 24s - loss: 0.3995 - accuracy: 0.8614 - val_loss: 0.6024 - val_accuracy: 0.8053 - 24s/epoch - 61ms/step\n",
      "Epoch 9/200\n",
      "390/390 - 23s - loss: 0.3705 - accuracy: 0.8726 - val_loss: 0.5508 - val_accuracy: 0.8222 - 23s/epoch - 60ms/step\n",
      "Epoch 10/200\n",
      "390/390 - 23s - loss: 0.3555 - accuracy: 0.8762 - val_loss: 0.5475 - val_accuracy: 0.8263 - 23s/epoch - 60ms/step\n",
      "Epoch 11/200\n",
      "390/390 - 23s - loss: 0.3465 - accuracy: 0.8795 - val_loss: 0.5346 - val_accuracy: 0.8286 - 23s/epoch - 60ms/step\n",
      "Epoch 12/200\n",
      "390/390 - 24s - loss: 0.3297 - accuracy: 0.8860 - val_loss: 0.6931 - val_accuracy: 0.7913 - 24s/epoch - 61ms/step\n",
      "Epoch 13/200\n",
      "390/390 - 24s - loss: 0.3157 - accuracy: 0.8915 - val_loss: 0.6231 - val_accuracy: 0.8113 - 24s/epoch - 61ms/step\n",
      "Epoch 14/200\n",
      "390/390 - 24s - loss: 0.3103 - accuracy: 0.8929 - val_loss: 0.5703 - val_accuracy: 0.8228 - 24s/epoch - 60ms/step\n",
      "Epoch 15/200\n",
      "390/390 - 24s - loss: 0.3006 - accuracy: 0.8953 - val_loss: 0.5482 - val_accuracy: 0.8219 - 24s/epoch - 61ms/step\n",
      "Epoch 16/200\n",
      "390/390 - 23s - loss: 0.2937 - accuracy: 0.8980 - val_loss: 0.4700 - val_accuracy: 0.8495 - 23s/epoch - 60ms/step\n",
      "Epoch 17/200\n",
      "390/390 - 24s - loss: 0.2822 - accuracy: 0.9009 - val_loss: 0.4402 - val_accuracy: 0.8591 - 24s/epoch - 61ms/step\n",
      "Epoch 18/200\n",
      "390/390 - 23s - loss: 0.2812 - accuracy: 0.9027 - val_loss: 0.3776 - val_accuracy: 0.8700 - 23s/epoch - 60ms/step\n",
      "Epoch 19/200\n",
      "390/390 - 24s - loss: 0.2761 - accuracy: 0.9038 - val_loss: 0.7320 - val_accuracy: 0.7883 - 24s/epoch - 61ms/step\n",
      "Epoch 20/200\n",
      "390/390 - 24s - loss: 0.2691 - accuracy: 0.9060 - val_loss: 0.5909 - val_accuracy: 0.8268 - 24s/epoch - 61ms/step\n",
      "Epoch 21/200\n",
      "390/390 - 24s - loss: 0.2657 - accuracy: 0.9066 - val_loss: 0.5715 - val_accuracy: 0.8288 - 24s/epoch - 61ms/step\n",
      "Epoch 22/200\n",
      "390/390 - 24s - loss: 0.2548 - accuracy: 0.9105 - val_loss: 0.4478 - val_accuracy: 0.8572 - 24s/epoch - 60ms/step\n",
      "Epoch 23/200\n",
      "390/390 - 24s - loss: 0.2537 - accuracy: 0.9110 - val_loss: 0.5355 - val_accuracy: 0.8331 - 24s/epoch - 61ms/step\n",
      "Epoch 24/200\n",
      "390/390 - 24s - loss: 0.2538 - accuracy: 0.9123 - val_loss: 0.4578 - val_accuracy: 0.8591 - 24s/epoch - 61ms/step\n",
      "Epoch 25/200\n",
      "390/390 - 24s - loss: 0.2497 - accuracy: 0.9122 - val_loss: 0.4776 - val_accuracy: 0.8568 - 24s/epoch - 61ms/step\n",
      "Epoch 26/200\n",
      "390/390 - 24s - loss: 0.2440 - accuracy: 0.9160 - val_loss: 0.4818 - val_accuracy: 0.8536 - 24s/epoch - 62ms/step\n",
      "Epoch 27/200\n",
      "390/390 - 24s - loss: 0.2417 - accuracy: 0.9155 - val_loss: 0.5193 - val_accuracy: 0.8351 - 24s/epoch - 63ms/step\n",
      "Epoch 28/200\n",
      "390/390 - 24s - loss: 0.2359 - accuracy: 0.9164 - val_loss: 0.4500 - val_accuracy: 0.8569 - 24s/epoch - 60ms/step\n",
      "Epoch 29/200\n",
      "390/390 - 23s - loss: 0.2371 - accuracy: 0.9176 - val_loss: 0.6495 - val_accuracy: 0.8111 - 23s/epoch - 60ms/step\n",
      "Epoch 30/200\n",
      "390/390 - 24s - loss: 0.2315 - accuracy: 0.9199 - val_loss: 0.6553 - val_accuracy: 0.8017 - 24s/epoch - 62ms/step\n",
      "Epoch 31/200\n",
      "390/390 - 24s - loss: 0.2287 - accuracy: 0.9200 - val_loss: 0.5324 - val_accuracy: 0.8367 - 24s/epoch - 61ms/step\n",
      "Epoch 32/200\n",
      "390/390 - 23s - loss: 0.2290 - accuracy: 0.9200 - val_loss: 0.5877 - val_accuracy: 0.8295 - 23s/epoch - 60ms/step\n",
      "Epoch 33/200\n",
      "390/390 - 24s - loss: 0.2254 - accuracy: 0.9210 - val_loss: 0.5416 - val_accuracy: 0.8391 - 24s/epoch - 61ms/step\n",
      "Epoch 34/200\n",
      "390/390 - 24s - loss: 0.2220 - accuracy: 0.9225 - val_loss: 0.7292 - val_accuracy: 0.7893 - 24s/epoch - 61ms/step\n",
      "Epoch 35/200\n",
      "390/390 - 23s - loss: 0.2162 - accuracy: 0.9242 - val_loss: 0.5359 - val_accuracy: 0.8442 - 23s/epoch - 60ms/step\n",
      "Epoch 36/200\n",
      "390/390 - 24s - loss: 0.2140 - accuracy: 0.9237 - val_loss: 0.6828 - val_accuracy: 0.8146 - 24s/epoch - 61ms/step\n",
      "Epoch 37/200\n",
      "390/390 - 23s - loss: 0.2208 - accuracy: 0.9222 - val_loss: 0.6721 - val_accuracy: 0.8084 - 23s/epoch - 59ms/step\n",
      "Epoch 38/200\n",
      "390/390 - 24s - loss: 0.2123 - accuracy: 0.9247 - val_loss: 0.5017 - val_accuracy: 0.8531 - 24s/epoch - 61ms/step\n",
      "Epoch 39/200\n",
      "390/390 - 24s - loss: 0.2159 - accuracy: 0.9248 - val_loss: 0.5223 - val_accuracy: 0.8432 - 24s/epoch - 61ms/step\n",
      "Epoch 40/200\n",
      "390/390 - 23s - loss: 0.2151 - accuracy: 0.9251 - val_loss: 0.4261 - val_accuracy: 0.8629 - 23s/epoch - 60ms/step\n",
      "Epoch 41/200\n",
      "390/390 - 23s - loss: 0.2142 - accuracy: 0.9249 - val_loss: 0.4450 - val_accuracy: 0.8572 - 23s/epoch - 60ms/step\n",
      "Epoch 42/200\n",
      "390/390 - 24s - loss: 0.2103 - accuracy: 0.9273 - val_loss: 0.5715 - val_accuracy: 0.8345 - 24s/epoch - 60ms/step\n",
      "Epoch 43/200\n",
      "390/390 - 24s - loss: 0.2067 - accuracy: 0.9280 - val_loss: 0.4543 - val_accuracy: 0.8601 - 24s/epoch - 62ms/step\n",
      "Epoch 44/200\n",
      "390/390 - 24s - loss: 0.2069 - accuracy: 0.9275 - val_loss: 0.5556 - val_accuracy: 0.8352 - 24s/epoch - 61ms/step\n",
      "Epoch 45/200\n",
      "390/390 - 23s - loss: 0.2012 - accuracy: 0.9287 - val_loss: 0.4036 - val_accuracy: 0.8762 - 23s/epoch - 60ms/step\n",
      "Epoch 46/200\n",
      "390/390 - 23s - loss: 0.2017 - accuracy: 0.9299 - val_loss: 0.4129 - val_accuracy: 0.8620 - 23s/epoch - 60ms/step\n",
      "Epoch 47/200\n",
      "390/390 - 24s - loss: 0.1993 - accuracy: 0.9310 - val_loss: 0.4944 - val_accuracy: 0.8554 - 24s/epoch - 62ms/step\n",
      "Epoch 48/200\n",
      "390/390 - 24s - loss: 0.2038 - accuracy: 0.9280 - val_loss: 0.4780 - val_accuracy: 0.8578 - 24s/epoch - 60ms/step\n",
      "Epoch 49/200\n",
      "390/390 - 24s - loss: 0.1947 - accuracy: 0.9306 - val_loss: 1.0291 - val_accuracy: 0.7459 - 24s/epoch - 62ms/step\n",
      "Epoch 50/200\n",
      "390/390 - 23s - loss: 0.2004 - accuracy: 0.9297 - val_loss: 0.5252 - val_accuracy: 0.8501 - 23s/epoch - 59ms/step\n",
      "Epoch 51/200\n",
      "390/390 - 23s - loss: 0.1954 - accuracy: 0.9317 - val_loss: 0.4129 - val_accuracy: 0.8676 - 23s/epoch - 60ms/step\n",
      "Epoch 52/200\n",
      "390/390 - 23s - loss: 0.1926 - accuracy: 0.9317 - val_loss: 0.4432 - val_accuracy: 0.8660 - 23s/epoch - 60ms/step\n",
      "Epoch 53/200\n",
      "390/390 - 23s - loss: 0.1964 - accuracy: 0.9310 - val_loss: 0.5556 - val_accuracy: 0.8395 - 23s/epoch - 60ms/step\n",
      "Epoch 54/200\n",
      "390/390 - 23s - loss: 0.1944 - accuracy: 0.9313 - val_loss: 0.6658 - val_accuracy: 0.8055 - 23s/epoch - 60ms/step\n",
      "Epoch 55/200\n",
      "390/390 - 23s - loss: 0.1930 - accuracy: 0.9315 - val_loss: 0.4650 - val_accuracy: 0.8530 - 23s/epoch - 60ms/step\n",
      "Epoch 56/200\n",
      "390/390 - 23s - loss: 0.1917 - accuracy: 0.9318 - val_loss: 0.5598 - val_accuracy: 0.8396 - 23s/epoch - 60ms/step\n",
      "Epoch 57/200\n",
      "390/390 - 23s - loss: 0.1900 - accuracy: 0.9333 - val_loss: 0.5849 - val_accuracy: 0.8268 - 23s/epoch - 59ms/step\n",
      "Epoch 58/200\n",
      "390/390 - 24s - loss: 0.1934 - accuracy: 0.9323 - val_loss: 0.5468 - val_accuracy: 0.8430 - 24s/epoch - 61ms/step\n",
      "Epoch 59/200\n",
      "390/390 - 24s - loss: 0.1806 - accuracy: 0.9359 - val_loss: 0.6091 - val_accuracy: 0.8310 - 24s/epoch - 61ms/step\n",
      "Epoch 60/200\n",
      "390/390 - 24s - loss: 0.1899 - accuracy: 0.9330 - val_loss: 0.5063 - val_accuracy: 0.8532 - 24s/epoch - 60ms/step\n",
      "Epoch 61/200\n",
      "390/390 - 24s - loss: 0.1858 - accuracy: 0.9343 - val_loss: 0.4732 - val_accuracy: 0.8578 - 24s/epoch - 61ms/step\n",
      "Epoch 62/200\n",
      "390/390 - 24s - loss: 0.1847 - accuracy: 0.9341 - val_loss: 0.5526 - val_accuracy: 0.8397 - 24s/epoch - 60ms/step\n",
      "Epoch 63/200\n",
      "390/390 - 24s - loss: 0.1859 - accuracy: 0.9348 - val_loss: 0.4639 - val_accuracy: 0.8609 - 24s/epoch - 62ms/step\n",
      "Epoch 64/200\n",
      "390/390 - 24s - loss: 0.1824 - accuracy: 0.9364 - val_loss: 0.4344 - val_accuracy: 0.8684 - 24s/epoch - 60ms/step\n",
      "Epoch 65/200\n",
      "390/390 - 24s - loss: 0.1777 - accuracy: 0.9383 - val_loss: 0.4686 - val_accuracy: 0.8618 - 24s/epoch - 61ms/step\n",
      "Epoch 66/200\n",
      "390/390 - 24s - loss: 0.1818 - accuracy: 0.9356 - val_loss: 0.5098 - val_accuracy: 0.8424 - 24s/epoch - 62ms/step\n",
      "Epoch 67/200\n",
      "390/390 - 24s - loss: 0.1803 - accuracy: 0.9368 - val_loss: 0.4931 - val_accuracy: 0.8489 - 24s/epoch - 61ms/step\n",
      "Epoch 68/200\n",
      "390/390 - 23s - loss: 0.1801 - accuracy: 0.9365 - val_loss: 0.8882 - val_accuracy: 0.7744 - 23s/epoch - 60ms/step\n",
      "Epoch 69/200\n",
      "390/390 - 23s - loss: 0.1805 - accuracy: 0.9359 - val_loss: 0.4182 - val_accuracy: 0.8731 - 23s/epoch - 60ms/step\n",
      "Epoch 70/200\n",
      "390/390 - 23s - loss: 0.1752 - accuracy: 0.9393 - val_loss: 0.4790 - val_accuracy: 0.8607 - 23s/epoch - 60ms/step\n",
      "Epoch 71/200\n",
      "390/390 - 24s - loss: 0.1774 - accuracy: 0.9378 - val_loss: 0.7748 - val_accuracy: 0.7965 - 24s/epoch - 63ms/step\n",
      "Epoch 72/200\n",
      "390/390 - 24s - loss: 0.1778 - accuracy: 0.9376 - val_loss: 0.4323 - val_accuracy: 0.8653 - 24s/epoch - 61ms/step\n",
      "Epoch 73/200\n",
      "390/390 - 24s - loss: 0.1713 - accuracy: 0.9395 - val_loss: 0.4743 - val_accuracy: 0.8586 - 24s/epoch - 61ms/step\n",
      "Epoch 74/200\n",
      "390/390 - 23s - loss: 0.1749 - accuracy: 0.9395 - val_loss: 0.6475 - val_accuracy: 0.8209 - 23s/epoch - 60ms/step\n",
      "Epoch 75/200\n",
      "390/390 - 24s - loss: 0.1731 - accuracy: 0.9400 - val_loss: 0.3765 - val_accuracy: 0.8835 - 24s/epoch - 61ms/step\n",
      "Epoch 76/200\n",
      "390/390 - 23s - loss: 0.1741 - accuracy: 0.9383 - val_loss: 0.4414 - val_accuracy: 0.8649 - 23s/epoch - 60ms/step\n",
      "Epoch 77/200\n",
      "390/390 - 23s - loss: 0.1733 - accuracy: 0.9393 - val_loss: 0.4993 - val_accuracy: 0.8563 - 23s/epoch - 60ms/step\n",
      "Epoch 78/200\n",
      "390/390 - 23s - loss: 0.1720 - accuracy: 0.9404 - val_loss: 0.3814 - val_accuracy: 0.8798 - 23s/epoch - 59ms/step\n",
      "Epoch 79/200\n",
      "390/390 - 24s - loss: 0.1711 - accuracy: 0.9394 - val_loss: 0.8295 - val_accuracy: 0.7966 - 24s/epoch - 62ms/step\n",
      "Epoch 80/200\n",
      "390/390 - 24s - loss: 0.1652 - accuracy: 0.9411 - val_loss: 0.5043 - val_accuracy: 0.8504 - 24s/epoch - 61ms/step\n",
      "Epoch 81/200\n",
      "390/390 - 24s - loss: 0.1664 - accuracy: 0.9420 - val_loss: 0.4296 - val_accuracy: 0.8719 - 24s/epoch - 61ms/step\n",
      "Epoch 82/200\n",
      "390/390 - 23s - loss: 0.1714 - accuracy: 0.9398 - val_loss: 0.3764 - val_accuracy: 0.8856 - 23s/epoch - 59ms/step\n",
      "Epoch 83/200\n",
      "390/390 - 24s - loss: 0.1655 - accuracy: 0.9418 - val_loss: 0.4693 - val_accuracy: 0.8662 - 24s/epoch - 60ms/step\n",
      "Epoch 84/200\n",
      "390/390 - 24s - loss: 0.1647 - accuracy: 0.9417 - val_loss: 0.5890 - val_accuracy: 0.8197 - 24s/epoch - 60ms/step\n",
      "Epoch 85/200\n",
      "390/390 - 24s - loss: 0.1637 - accuracy: 0.9425 - val_loss: 0.4526 - val_accuracy: 0.8669 - 24s/epoch - 60ms/step\n",
      "Epoch 86/200\n",
      "390/390 - 23s - loss: 0.1654 - accuracy: 0.9423 - val_loss: 0.6148 - val_accuracy: 0.8315 - 23s/epoch - 60ms/step\n",
      "Epoch 87/200\n",
      "390/390 - 24s - loss: 0.1655 - accuracy: 0.9418 - val_loss: 0.3667 - val_accuracy: 0.8851 - 24s/epoch - 61ms/step\n",
      "Epoch 88/200\n",
      "390/390 - 23s - loss: 0.1633 - accuracy: 0.9431 - val_loss: 0.4389 - val_accuracy: 0.8673 - 23s/epoch - 60ms/step\n",
      "Epoch 89/200\n",
      "390/390 - 24s - loss: 0.1605 - accuracy: 0.9437 - val_loss: 0.4732 - val_accuracy: 0.8619 - 24s/epoch - 62ms/step\n",
      "Epoch 90/200\n",
      "390/390 - 24s - loss: 0.1629 - accuracy: 0.9432 - val_loss: 0.4697 - val_accuracy: 0.8564 - 24s/epoch - 61ms/step\n",
      "Epoch 91/200\n",
      "390/390 - 24s - loss: 0.1624 - accuracy: 0.9428 - val_loss: 0.4440 - val_accuracy: 0.8669 - 24s/epoch - 61ms/step\n",
      "Epoch 92/200\n",
      "390/390 - 24s - loss: 0.1582 - accuracy: 0.9437 - val_loss: 0.4304 - val_accuracy: 0.8648 - 24s/epoch - 61ms/step\n",
      "Epoch 93/200\n",
      "390/390 - 24s - loss: 0.1620 - accuracy: 0.9433 - val_loss: 0.5382 - val_accuracy: 0.8436 - 24s/epoch - 61ms/step\n",
      "Epoch 94/200\n",
      "390/390 - 24s - loss: 0.1595 - accuracy: 0.9444 - val_loss: 0.5031 - val_accuracy: 0.8565 - 24s/epoch - 61ms/step\n",
      "Epoch 95/200\n",
      "390/390 - 23s - loss: 0.1564 - accuracy: 0.9444 - val_loss: 0.4610 - val_accuracy: 0.8691 - 23s/epoch - 60ms/step\n",
      "Epoch 96/200\n",
      "390/390 - 23s - loss: 0.1612 - accuracy: 0.9439 - val_loss: 0.3592 - val_accuracy: 0.8917 - 23s/epoch - 59ms/step\n",
      "Epoch 97/200\n",
      "390/390 - 24s - loss: 0.1554 - accuracy: 0.9452 - val_loss: 0.4396 - val_accuracy: 0.8665 - 24s/epoch - 60ms/step\n",
      "Epoch 98/200\n",
      "390/390 - 23s - loss: 0.1543 - accuracy: 0.9457 - val_loss: 0.3874 - val_accuracy: 0.8882 - 23s/epoch - 59ms/step\n",
      "Epoch 99/200\n",
      "390/390 - 23s - loss: 0.1557 - accuracy: 0.9452 - val_loss: 0.6901 - val_accuracy: 0.8183 - 23s/epoch - 60ms/step\n",
      "Epoch 100/200\n",
      "390/390 - 24s - loss: 0.1593 - accuracy: 0.9436 - val_loss: 0.6431 - val_accuracy: 0.8172 - 24s/epoch - 61ms/step\n",
      "Epoch 101/200\n",
      "390/390 - 24s - loss: 0.1480 - accuracy: 0.9484 - val_loss: 0.6043 - val_accuracy: 0.8298 - 24s/epoch - 61ms/step\n",
      "Epoch 102/200\n",
      "390/390 - 23s - loss: 0.1498 - accuracy: 0.9476 - val_loss: 0.4681 - val_accuracy: 0.8692 - 23s/epoch - 59ms/step\n",
      "Epoch 103/200\n",
      "390/390 - 24s - loss: 0.1554 - accuracy: 0.9448 - val_loss: 0.4373 - val_accuracy: 0.8693 - 24s/epoch - 63ms/step\n",
      "Epoch 104/200\n",
      "390/390 - 24s - loss: 0.1517 - accuracy: 0.9463 - val_loss: 0.5086 - val_accuracy: 0.8516 - 24s/epoch - 61ms/step\n",
      "Epoch 105/200\n",
      "390/390 - 23s - loss: 0.1507 - accuracy: 0.9473 - val_loss: 0.4964 - val_accuracy: 0.8595 - 23s/epoch - 60ms/step\n",
      "Epoch 106/200\n",
      "390/390 - 24s - loss: 0.1473 - accuracy: 0.9482 - val_loss: 0.4247 - val_accuracy: 0.8733 - 24s/epoch - 61ms/step\n",
      "Epoch 107/200\n",
      "390/390 - 24s - loss: 0.1522 - accuracy: 0.9459 - val_loss: 0.4290 - val_accuracy: 0.8660 - 24s/epoch - 61ms/step\n",
      "Epoch 108/200\n",
      "390/390 - 23s - loss: 0.1497 - accuracy: 0.9481 - val_loss: 0.5108 - val_accuracy: 0.8495 - 23s/epoch - 60ms/step\n",
      "Epoch 109/200\n",
      "390/390 - 23s - loss: 0.1473 - accuracy: 0.9473 - val_loss: 0.4860 - val_accuracy: 0.8604 - 23s/epoch - 59ms/step\n",
      "Epoch 110/200\n",
      "390/390 - 24s - loss: 0.1414 - accuracy: 0.9496 - val_loss: 0.5205 - val_accuracy: 0.8490 - 24s/epoch - 60ms/step\n",
      "Epoch 111/200\n",
      "390/390 - 24s - loss: 0.1466 - accuracy: 0.9477 - val_loss: 0.4203 - val_accuracy: 0.8731 - 24s/epoch - 61ms/step\n",
      "Epoch 112/200\n",
      "390/390 - 24s - loss: 0.1429 - accuracy: 0.9504 - val_loss: 0.3425 - val_accuracy: 0.8938 - 24s/epoch - 61ms/step\n",
      "Epoch 113/200\n",
      "390/390 - 24s - loss: 0.1432 - accuracy: 0.9487 - val_loss: 0.4603 - val_accuracy: 0.8632 - 24s/epoch - 61ms/step\n",
      "Epoch 114/200\n",
      "390/390 - 23s - loss: 0.1468 - accuracy: 0.9475 - val_loss: 0.5163 - val_accuracy: 0.8512 - 23s/epoch - 60ms/step\n",
      "Epoch 115/200\n",
      "390/390 - 24s - loss: 0.1412 - accuracy: 0.9495 - val_loss: 0.4753 - val_accuracy: 0.8608 - 24s/epoch - 60ms/step\n",
      "Epoch 116/200\n",
      "390/390 - 24s - loss: 0.1409 - accuracy: 0.9504 - val_loss: 0.3985 - val_accuracy: 0.8809 - 24s/epoch - 61ms/step\n",
      "Epoch 117/200\n",
      "390/390 - 23s - loss: 0.1388 - accuracy: 0.9514 - val_loss: 0.4313 - val_accuracy: 0.8726 - 23s/epoch - 60ms/step\n",
      "Epoch 118/200\n",
      "390/390 - 23s - loss: 0.1417 - accuracy: 0.9501 - val_loss: 0.7246 - val_accuracy: 0.8080 - 23s/epoch - 60ms/step\n",
      "Epoch 119/200\n",
      "390/390 - 24s - loss: 0.1434 - accuracy: 0.9509 - val_loss: 0.5535 - val_accuracy: 0.8459 - 24s/epoch - 61ms/step\n",
      "Epoch 120/200\n",
      "390/390 - 23s - loss: 0.1366 - accuracy: 0.9522 - val_loss: 0.4567 - val_accuracy: 0.8669 - 23s/epoch - 60ms/step\n",
      "Epoch 121/200\n",
      "390/390 - 23s - loss: 0.1430 - accuracy: 0.9501 - val_loss: 0.4450 - val_accuracy: 0.8696 - 23s/epoch - 59ms/step\n",
      "Epoch 122/200\n",
      "390/390 - 24s - loss: 0.1355 - accuracy: 0.9529 - val_loss: 0.3780 - val_accuracy: 0.8888 - 24s/epoch - 61ms/step\n",
      "Epoch 123/200\n",
      "390/390 - 24s - loss: 0.1337 - accuracy: 0.9528 - val_loss: 0.3789 - val_accuracy: 0.8854 - 24s/epoch - 62ms/step\n",
      "Epoch 124/200\n",
      "390/390 - 24s - loss: 0.1380 - accuracy: 0.9513 - val_loss: 0.4314 - val_accuracy: 0.8719 - 24s/epoch - 61ms/step\n",
      "Epoch 125/200\n",
      "390/390 - 23s - loss: 0.1331 - accuracy: 0.9525 - val_loss: 0.4084 - val_accuracy: 0.8789 - 23s/epoch - 60ms/step\n",
      "Epoch 126/200\n",
      "390/390 - 23s - loss: 0.1349 - accuracy: 0.9523 - val_loss: 0.3932 - val_accuracy: 0.8824 - 23s/epoch - 60ms/step\n",
      "Epoch 127/200\n",
      "390/390 - 24s - loss: 0.1342 - accuracy: 0.9531 - val_loss: 0.5063 - val_accuracy: 0.8573 - 24s/epoch - 61ms/step\n",
      "Epoch 128/200\n",
      "390/390 - 23s - loss: 0.1315 - accuracy: 0.9533 - val_loss: 0.4205 - val_accuracy: 0.8800 - 23s/epoch - 60ms/step\n",
      "Epoch 129/200\n",
      "390/390 - 24s - loss: 0.1306 - accuracy: 0.9543 - val_loss: 0.4857 - val_accuracy: 0.8616 - 24s/epoch - 62ms/step\n",
      "Epoch 130/200\n",
      "390/390 - 23s - loss: 0.1322 - accuracy: 0.9543 - val_loss: 0.3642 - val_accuracy: 0.8930 - 23s/epoch - 60ms/step\n",
      "Epoch 131/200\n",
      "390/390 - 23s - loss: 0.1274 - accuracy: 0.9561 - val_loss: 0.3709 - val_accuracy: 0.8867 - 23s/epoch - 59ms/step\n",
      "Epoch 132/200\n",
      "390/390 - 24s - loss: 0.1294 - accuracy: 0.9550 - val_loss: 0.3677 - val_accuracy: 0.8931 - 24s/epoch - 62ms/step\n",
      "Epoch 133/200\n",
      "390/390 - 24s - loss: 0.1280 - accuracy: 0.9547 - val_loss: 0.4296 - val_accuracy: 0.8775 - 24s/epoch - 61ms/step\n",
      "Epoch 134/200\n",
      "390/390 - 23s - loss: 0.1302 - accuracy: 0.9533 - val_loss: 0.6246 - val_accuracy: 0.8326 - 23s/epoch - 60ms/step\n",
      "Epoch 135/200\n",
      "390/390 - 24s - loss: 0.1264 - accuracy: 0.9548 - val_loss: 0.3490 - val_accuracy: 0.8945 - 24s/epoch - 61ms/step\n",
      "Epoch 136/200\n",
      "390/390 - 24s - loss: 0.1276 - accuracy: 0.9557 - val_loss: 0.5112 - val_accuracy: 0.8552 - 24s/epoch - 61ms/step\n",
      "Epoch 137/200\n",
      "390/390 - 24s - loss: 0.1234 - accuracy: 0.9570 - val_loss: 0.5553 - val_accuracy: 0.8504 - 24s/epoch - 61ms/step\n",
      "Epoch 138/200\n",
      "390/390 - 24s - loss: 0.1224 - accuracy: 0.9578 - val_loss: 0.3440 - val_accuracy: 0.9002 - 24s/epoch - 60ms/step\n",
      "Epoch 139/200\n",
      "390/390 - 24s - loss: 0.1226 - accuracy: 0.9569 - val_loss: 0.7395 - val_accuracy: 0.8094 - 24s/epoch - 62ms/step\n",
      "Epoch 140/200\n",
      "390/390 - 23s - loss: 0.1224 - accuracy: 0.9569 - val_loss: 0.4393 - val_accuracy: 0.8723 - 23s/epoch - 60ms/step\n",
      "Epoch 141/200\n",
      "390/390 - 24s - loss: 0.1180 - accuracy: 0.9585 - val_loss: 0.4310 - val_accuracy: 0.8705 - 24s/epoch - 61ms/step\n",
      "Epoch 142/200\n",
      "390/390 - 24s - loss: 0.1218 - accuracy: 0.9580 - val_loss: 0.4299 - val_accuracy: 0.8760 - 24s/epoch - 61ms/step\n",
      "Epoch 143/200\n",
      "390/390 - 24s - loss: 0.1180 - accuracy: 0.9582 - val_loss: 0.4695 - val_accuracy: 0.8691 - 24s/epoch - 60ms/step\n",
      "Epoch 144/200\n",
      "390/390 - 24s - loss: 0.1181 - accuracy: 0.9584 - val_loss: 0.3900 - val_accuracy: 0.8847 - 24s/epoch - 60ms/step\n",
      "Epoch 145/200\n",
      "390/390 - 24s - loss: 0.1174 - accuracy: 0.9596 - val_loss: 0.9600 - val_accuracy: 0.7522 - 24s/epoch - 62ms/step\n",
      "Epoch 146/200\n",
      "390/390 - 24s - loss: 0.1181 - accuracy: 0.9588 - val_loss: 0.4465 - val_accuracy: 0.8738 - 24s/epoch - 62ms/step\n",
      "Epoch 147/200\n",
      "390/390 - 23s - loss: 0.1160 - accuracy: 0.9602 - val_loss: 0.4074 - val_accuracy: 0.8835 - 23s/epoch - 60ms/step\n",
      "Epoch 148/200\n",
      "390/390 - 23s - loss: 0.1139 - accuracy: 0.9607 - val_loss: 0.3754 - val_accuracy: 0.8912 - 23s/epoch - 60ms/step\n",
      "Epoch 149/200\n",
      "390/390 - 24s - loss: 0.1090 - accuracy: 0.9623 - val_loss: 0.4519 - val_accuracy: 0.8717 - 24s/epoch - 61ms/step\n",
      "Epoch 150/200\n",
      "390/390 - 23s - loss: 0.1149 - accuracy: 0.9601 - val_loss: 0.4193 - val_accuracy: 0.8851 - 23s/epoch - 60ms/step\n",
      "Epoch 151/200\n",
      "390/390 - 24s - loss: 0.1102 - accuracy: 0.9611 - val_loss: 0.3863 - val_accuracy: 0.8886 - 24s/epoch - 61ms/step\n",
      "Epoch 152/200\n",
      "390/390 - 24s - loss: 0.1077 - accuracy: 0.9614 - val_loss: 0.3906 - val_accuracy: 0.8885 - 24s/epoch - 62ms/step\n",
      "Epoch 153/200\n",
      "390/390 - 24s - loss: 0.1074 - accuracy: 0.9621 - val_loss: 0.4682 - val_accuracy: 0.8732 - 24s/epoch - 62ms/step\n",
      "Epoch 154/200\n",
      "390/390 - 24s - loss: 0.1105 - accuracy: 0.9613 - val_loss: 0.4205 - val_accuracy: 0.8812 - 24s/epoch - 60ms/step\n",
      "Epoch 155/200\n",
      "390/390 - 24s - loss: 0.1060 - accuracy: 0.9635 - val_loss: 0.3122 - val_accuracy: 0.9034 - 24s/epoch - 62ms/step\n",
      "Epoch 156/200\n",
      "390/390 - 23s - loss: 0.1010 - accuracy: 0.9644 - val_loss: 0.4993 - val_accuracy: 0.8657 - 23s/epoch - 60ms/step\n",
      "Epoch 157/200\n",
      "390/390 - 23s - loss: 0.1014 - accuracy: 0.9648 - val_loss: 0.3261 - val_accuracy: 0.9003 - 23s/epoch - 60ms/step\n",
      "Epoch 158/200\n",
      "390/390 - 24s - loss: 0.1043 - accuracy: 0.9637 - val_loss: 0.5617 - val_accuracy: 0.8513 - 24s/epoch - 62ms/step\n",
      "Epoch 159/200\n",
      "390/390 - 23s - loss: 0.0993 - accuracy: 0.9654 - val_loss: 0.3479 - val_accuracy: 0.8968 - 23s/epoch - 60ms/step\n",
      "Epoch 160/200\n",
      "390/390 - 24s - loss: 0.0997 - accuracy: 0.9657 - val_loss: 0.4049 - val_accuracy: 0.8794 - 24s/epoch - 61ms/step\n",
      "Epoch 161/200\n",
      "390/390 - 23s - loss: 0.0944 - accuracy: 0.9673 - val_loss: 0.4327 - val_accuracy: 0.8800 - 23s/epoch - 58ms/step\n",
      "Epoch 162/200\n",
      "390/390 - 24s - loss: 0.0872 - accuracy: 0.9698 - val_loss: 0.4556 - val_accuracy: 0.8722 - 24s/epoch - 63ms/step\n",
      "Epoch 163/200\n",
      "390/390 - 23s - loss: 0.0973 - accuracy: 0.9669 - val_loss: 0.3858 - val_accuracy: 0.8878 - 23s/epoch - 60ms/step\n",
      "Epoch 164/200\n",
      "390/390 - 24s - loss: 0.0973 - accuracy: 0.9659 - val_loss: 0.3372 - val_accuracy: 0.8990 - 24s/epoch - 61ms/step\n",
      "Epoch 165/200\n",
      "390/390 - 23s - loss: 0.0887 - accuracy: 0.9697 - val_loss: 0.3781 - val_accuracy: 0.8941 - 23s/epoch - 60ms/step\n",
      "Epoch 166/200\n",
      "390/390 - 24s - loss: 0.0882 - accuracy: 0.9695 - val_loss: 0.3329 - val_accuracy: 0.8991 - 24s/epoch - 61ms/step\n",
      "Epoch 167/200\n",
      "390/390 - 24s - loss: 0.0872 - accuracy: 0.9693 - val_loss: 0.3710 - val_accuracy: 0.8938 - 24s/epoch - 60ms/step\n",
      "Epoch 168/200\n",
      "390/390 - 24s - loss: 0.0844 - accuracy: 0.9709 - val_loss: 0.5799 - val_accuracy: 0.8491 - 24s/epoch - 61ms/step\n",
      "Epoch 169/200\n",
      "390/390 - 24s - loss: 0.0788 - accuracy: 0.9732 - val_loss: 0.3166 - val_accuracy: 0.9066 - 24s/epoch - 61ms/step\n",
      "Epoch 170/200\n",
      "390/390 - 24s - loss: 0.0843 - accuracy: 0.9712 - val_loss: 0.3352 - val_accuracy: 0.9018 - 24s/epoch - 60ms/step\n",
      "Epoch 171/200\n",
      "390/390 - 24s - loss: 0.0733 - accuracy: 0.9755 - val_loss: 0.3575 - val_accuracy: 0.8965 - 24s/epoch - 61ms/step\n",
      "Epoch 172/200\n",
      "390/390 - 24s - loss: 0.0766 - accuracy: 0.9742 - val_loss: 0.4229 - val_accuracy: 0.8838 - 24s/epoch - 62ms/step\n",
      "Epoch 173/200\n",
      "390/390 - 24s - loss: 0.0737 - accuracy: 0.9754 - val_loss: 0.4022 - val_accuracy: 0.8887 - 24s/epoch - 60ms/step\n",
      "Epoch 174/200\n",
      "390/390 - 23s - loss: 0.0755 - accuracy: 0.9740 - val_loss: 0.3308 - val_accuracy: 0.9024 - 23s/epoch - 60ms/step\n",
      "Epoch 175/200\n",
      "390/390 - 24s - loss: 0.0702 - accuracy: 0.9766 - val_loss: 0.3664 - val_accuracy: 0.8966 - 24s/epoch - 61ms/step\n",
      "Epoch 176/200\n",
      "390/390 - 24s - loss: 0.0666 - accuracy: 0.9786 - val_loss: 0.3042 - val_accuracy: 0.9080 - 24s/epoch - 61ms/step\n",
      "Epoch 177/200\n",
      "390/390 - 24s - loss: 0.0637 - accuracy: 0.9789 - val_loss: 0.3448 - val_accuracy: 0.8998 - 24s/epoch - 61ms/step\n",
      "Epoch 178/200\n",
      "390/390 - 23s - loss: 0.0604 - accuracy: 0.9797 - val_loss: 0.3773 - val_accuracy: 0.8960 - 23s/epoch - 59ms/step\n",
      "Epoch 179/200\n",
      "390/390 - 23s - loss: 0.0577 - accuracy: 0.9809 - val_loss: 0.3611 - val_accuracy: 0.8944 - 23s/epoch - 60ms/step\n",
      "Epoch 180/200\n",
      "390/390 - 23s - loss: 0.0536 - accuracy: 0.9824 - val_loss: 0.2975 - val_accuracy: 0.9141 - 23s/epoch - 60ms/step\n",
      "Epoch 181/200\n",
      "390/390 - 23s - loss: 0.0535 - accuracy: 0.9826 - val_loss: 0.3018 - val_accuracy: 0.9098 - 23s/epoch - 59ms/step\n",
      "Epoch 182/200\n",
      "390/390 - 23s - loss: 0.0480 - accuracy: 0.9853 - val_loss: 0.2592 - val_accuracy: 0.9236 - 23s/epoch - 60ms/step\n",
      "Epoch 183/200\n",
      "390/390 - 23s - loss: 0.0477 - accuracy: 0.9852 - val_loss: 0.3290 - val_accuracy: 0.9054 - 23s/epoch - 59ms/step\n",
      "Epoch 184/200\n",
      "390/390 - 23s - loss: 0.0434 - accuracy: 0.9871 - val_loss: 0.2868 - val_accuracy: 0.9167 - 23s/epoch - 59ms/step\n",
      "Epoch 185/200\n",
      "390/390 - 23s - loss: 0.0395 - accuracy: 0.9880 - val_loss: 0.2809 - val_accuracy: 0.9182 - 23s/epoch - 59ms/step\n",
      "Epoch 186/200\n",
      "390/390 - 24s - loss: 0.0384 - accuracy: 0.9889 - val_loss: 0.2769 - val_accuracy: 0.9189 - 24s/epoch - 62ms/step\n",
      "Epoch 187/200\n",
      "390/390 - 23s - loss: 0.0378 - accuracy: 0.9893 - val_loss: 0.2336 - val_accuracy: 0.9290 - 23s/epoch - 59ms/step\n",
      "Epoch 188/200\n",
      "390/390 - 23s - loss: 0.0339 - accuracy: 0.9908 - val_loss: 0.2511 - val_accuracy: 0.9279 - 23s/epoch - 60ms/step\n",
      "Epoch 189/200\n",
      "390/390 - 23s - loss: 0.0334 - accuracy: 0.9913 - val_loss: 0.2316 - val_accuracy: 0.9315 - 23s/epoch - 59ms/step\n",
      "Epoch 190/200\n",
      "390/390 - 23s - loss: 0.0303 - accuracy: 0.9925 - val_loss: 0.2450 - val_accuracy: 0.9277 - 23s/epoch - 59ms/step\n",
      "Epoch 191/200\n",
      "390/390 - 23s - loss: 0.0281 - accuracy: 0.9939 - val_loss: 0.2322 - val_accuracy: 0.9297 - 23s/epoch - 60ms/step\n",
      "Epoch 192/200\n",
      "390/390 - 23s - loss: 0.0275 - accuracy: 0.9939 - val_loss: 0.2246 - val_accuracy: 0.9320 - 23s/epoch - 60ms/step\n",
      "Epoch 193/200\n",
      "390/390 - 23s - loss: 0.0287 - accuracy: 0.9936 - val_loss: 0.2166 - val_accuracy: 0.9359 - 23s/epoch - 60ms/step\n",
      "Epoch 194/200\n",
      "390/390 - 23s - loss: 0.0274 - accuracy: 0.9947 - val_loss: 0.2241 - val_accuracy: 0.9325 - 23s/epoch - 59ms/step\n",
      "Epoch 195/200\n",
      "390/390 - 23s - loss: 0.0272 - accuracy: 0.9951 - val_loss: 0.2033 - val_accuracy: 0.9385 - 23s/epoch - 60ms/step\n",
      "Epoch 196/200\n",
      "390/390 - 23s - loss: 0.0283 - accuracy: 0.9957 - val_loss: 0.2086 - val_accuracy: 0.9358 - 23s/epoch - 59ms/step\n",
      "Epoch 197/200\n",
      "390/390 - 23s - loss: 0.0307 - accuracy: 0.9955 - val_loss: 0.2091 - val_accuracy: 0.9349 - 23s/epoch - 60ms/step\n",
      "Epoch 198/200\n",
      "390/390 - 24s - loss: 0.0328 - accuracy: 0.9961 - val_loss: 0.2064 - val_accuracy: 0.9352 - 24s/epoch - 61ms/step\n",
      "Epoch 199/200\n",
      "390/390 - 23s - loss: 0.0382 - accuracy: 0.9953 - val_loss: 0.2063 - val_accuracy: 0.9340 - 23s/epoch - 59ms/step\n",
      "Epoch 200/200\n",
      "390/390 - 23s - loss: 0.0431 - accuracy: 0.9953 - val_loss: 0.2064 - val_accuracy: 0.9346 - 23s/epoch - 60ms/step\n",
      " Using best val_acc=0.9385 from last 20 epochs\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2033 - accuracy: 0.9385\n",
      " Finished: Lam=0.625, Repeat=3, Acc=0.9385\n"
     ]
    }
   ],
   "source": [
    "for lam_idx in range(start_lr_idx, len(Lam)):\n",
    "    lam = Lam[lam_idx]\n",
    "    for rep in range(start_repeat, repeats):\n",
    "        print(f\"\\n lambda: Lam={lam}, Repeat={rep+1}/{repeats}\")\n",
    "        if progress[\"Cri_exist\"] == 0:\n",
    "            model=load_Res()\n",
    "            weight_list,channel_label=prune_model(model,G,P,x_dist,y_dist,lam)\n",
    "            model_p=model_pr(model,weight_list,channel_label)\n",
    "            flops_p,par_p=compute_flops_params(model_p)\n",
    "            P_=par_p/par\n",
    "            F=flops_p/flops\n",
    "            C_P=channel_G(model_p)\n",
    "            print(flops_p,flops)\n",
    "            progress[\"P_list\"].append(P_)\n",
    "            progress[\"F_list\"].append([flops_p,F])\n",
    "            progress[\"C_list\"].append([C_P])\n",
    "            progress[\"Cri_exist\"] = 1\n",
    "            save_progress(progress)\n",
    "            model_p.save(\"Res_56_pruned.h5\")\n",
    "        else:\n",
    "            model_p=model_p=tf.keras.models.load_model('Res_56_pruned.h5',custom_objects={\n",
    "                'CustomWeightDecaySGD': CustomWeightDecaySGD,\n",
    "                'WarmUpCosine': WarmUpCosine})\n",
    "            flops_p,par_p=compute_flops_params(model_p)\n",
    "            F=flops_p/flops\n",
    "            print(flops_p,flops)\n",
    "        retrain(model_p,x_train,y_train_onehot,x_test,y_test_onehot)\n",
    "        loss_p, acc_p = model_p.evaluate(x_test, y_test_onehot)\n",
    "        print(f\" Finished: Lam={lam}, Repeat={rep+1}, Acc={acc_p:.4f}\")\n",
    "        progress[\"results\"].append(acc_p)\n",
    "        progress[\"last_lam_idx\"] = lam_idx\n",
    "        progress[\"last_repeat\"] = rep+1\n",
    "        save_progress(progress)\n",
    "    progress[\"E_list\"].append(sum(progress[\"results\"])/(repeats*acc))\n",
    "    progress[\"results\"]=[]\n",
    "    progress[\"Cri_exist\"] = 0\n",
    "    progress[\"last_lam_idx\"] = lam_idx + 1\n",
    "    progress[\"last_repeat\"] = 0\n",
    "    start_repeat=0\n",
    "    save_progress(progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de845a2-e4eb-4039-a3a2-89d7c32b8bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e771175-039c-4512-8332-c4f97afa638c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9275, 0.9302, 0.93]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.9275,0.9302,0.9300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49f92d2d-9ec6-43af-afc2-02025b65be17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9286, 0.9287, 0.9287]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.9286,0.9287,0.9287]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc632ca8-99ff-48ef-840b-1b4b5c4073a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9306, 0.9317, 0.9326]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.9306,0.9317,0.9326]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ffbbbba2-1534-482c-a445-6647c2407c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.934]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.9340,0.9334,0.9385]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86821218-8dc1-4797-a334-b4e20ee11e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7476f886-b1ef-4f28-94c9-1a16a968ba28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4440212440608191,\n",
       " 0.4638450532389465,\n",
       " 0.5292948854548903,\n",
       " 0.6431655805000503]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress[\"P_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6c3d482-233a-4a19-b9e6-6dfa3fef871d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[51460688, 0.4083857990173223],\n",
       " [54241990, 0.43045787546485204],\n",
       " [63426148, 0.5033422431035306],\n",
       " [72987892, 0.5792230875928054]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress[\"F_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40cd7051-b279-4c02-bfc2-b7c020862437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9936199276671188,\n",
       " 0.9930139810881442,\n",
       " 0.9961862246643075,\n",
       " 1.0001069467456194]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress[\"E_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3396098-7ad4-4b99-9ede-988f3250650c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[10,\n",
       "   8,\n",
       "   9,\n",
       "   13,\n",
       "   10,\n",
       "   11,\n",
       "   14,\n",
       "   14,\n",
       "   12,\n",
       "   70,\n",
       "   24,\n",
       "   23,\n",
       "   24,\n",
       "   26,\n",
       "   21,\n",
       "   24,\n",
       "   23,\n",
       "   24,\n",
       "   22,\n",
       "   210,\n",
       "   43,\n",
       "   44,\n",
       "   41,\n",
       "   38,\n",
       "   47,\n",
       "   52,\n",
       "   56,\n",
       "   43,\n",
       "   45,\n",
       "   400]],\n",
       " [[10,\n",
       "   9,\n",
       "   10,\n",
       "   13,\n",
       "   11,\n",
       "   11,\n",
       "   14,\n",
       "   14,\n",
       "   13,\n",
       "   80,\n",
       "   24,\n",
       "   24,\n",
       "   25,\n",
       "   27,\n",
       "   23,\n",
       "   26,\n",
       "   26,\n",
       "   24,\n",
       "   23,\n",
       "   190,\n",
       "   45,\n",
       "   47,\n",
       "   49,\n",
       "   42,\n",
       "   51,\n",
       "   57,\n",
       "   58,\n",
       "   49,\n",
       "   48,\n",
       "   390]],\n",
       " [[10,\n",
       "   9,\n",
       "   10,\n",
       "   13,\n",
       "   11,\n",
       "   11,\n",
       "   14,\n",
       "   15,\n",
       "   13,\n",
       "   90,\n",
       "   24,\n",
       "   26,\n",
       "   26,\n",
       "   27,\n",
       "   25,\n",
       "   27,\n",
       "   27,\n",
       "   24,\n",
       "   24,\n",
       "   230,\n",
       "   47,\n",
       "   47,\n",
       "   50,\n",
       "   45,\n",
       "   51,\n",
       "   59,\n",
       "   60,\n",
       "   51,\n",
       "   50,\n",
       "   420]],\n",
       " [[10,\n",
       "   9,\n",
       "   10,\n",
       "   13,\n",
       "   11,\n",
       "   12,\n",
       "   14,\n",
       "   15,\n",
       "   13,\n",
       "   90,\n",
       "   24,\n",
       "   27,\n",
       "   27,\n",
       "   27,\n",
       "   26,\n",
       "   28,\n",
       "   29,\n",
       "   26,\n",
       "   26,\n",
       "   260,\n",
       "   50,\n",
       "   52,\n",
       "   52,\n",
       "   48,\n",
       "   52,\n",
       "   60,\n",
       "   60,\n",
       "   52,\n",
       "   52,\n",
       "   500]]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress[\"C_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf24c86-89b6-470f-9c70-8b83977b6ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

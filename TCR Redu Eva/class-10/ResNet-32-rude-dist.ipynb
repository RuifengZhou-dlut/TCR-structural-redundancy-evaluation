{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dea4f2e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers,regularizers,metrics,optimizers\n",
    "import random\n",
    "import pandas as pd\n",
    "from scipy.linalg import sqrtm\n",
    "import pickle\n",
    "import logging\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\n",
    "import math\n",
    "import scipy.stats as st\n",
    "from scipy.special import comb\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import json\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c459c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "config=tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "config.gpu_options.allow_growth=True\n",
    "sess=tf.compat.v1.Session(config=config) \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047a8aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This algorithm is used to evaluate the structural redundancy of ResNet-32\n",
    "and outputs the evaluation criteria of hidden layer redundancy as well as \n",
    "the entire redundancy evaluation criteria under each pruning parameter. \n",
    "Here, \"Lam\" refers to the pruning parameter set used in the evaluation \n",
    "algorithm, and \"repeats\" represents the number of times the pruning network \n",
    "is repeatedly fine-tuned.\"\"\"\n",
    "Lam=[0.725,0.7,0.65,0.6]\n",
    "repeats=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a42158-7dad-47c9-b86b-79b6d1ec3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "y_train_onehot=tf.keras.utils.to_categorical(y_train,num_classes=10)\n",
    "y_test_onehot=tf.keras.utils.to_categorical(y_test,num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1760ce31-8ae9-4c9b-9ae2-a1f51229e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_dist_ResNet_32.pkl', 'rb') as f:\n",
    "    [x_dist,y_dist]=pickle.load(f)\n",
    "y_dist=y_dist.reshape(len(y_dist),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b29e82-1535-4890-8962-851dfbb01ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 0.1\n",
    "weight_decay = 1e-4\n",
    "epochs = 200\n",
    "warmup_epochs = 5\n",
    "batch_size = 128\n",
    "image_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d66d49e7-17f5-4ca8-af13-e7e02c3a029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, base_lr, total_steps, warmup_steps, warmup_lr=0.0):\n",
    "        super().__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.warmup_lr = warmup_lr\n",
    "    def __call__(self, step):\n",
    "        if step is None:\n",
    "            step = tf.constant(0)\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        total_steps = tf.cast(self.total_steps, tf.float32)\n",
    "        warmup_percent_done = step / warmup_steps\n",
    "        learning_rate = tf.where(\n",
    "            step < warmup_steps,\n",
    "            self.warmup_lr + (self.base_lr - self.warmup_lr) * warmup_percent_done,\n",
    "            self.base_lr * 0.5 * (1.0 + tf.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps)))\n",
    "        )\n",
    "        return learning_rate\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"base_lr\": self.base_lr,\n",
    "            \"total_steps\": self.total_steps,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"warmup_lr\": self.warmup_lr,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a00d3648-b747-4730-8a76-75ce7c9a318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWeightDecaySGD(tf.keras.optimizers.SGD):\n",
    "    def __init__(self, weight_decay, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.weight_decay = weight_decay\n",
    "    def apply_gradients(self, grads_and_vars, name=None, experimental_aggregate_gradients=True):\n",
    "        super().apply_gradients(grads_and_vars, name, experimental_aggregate_gradients)\n",
    "        for grad, var in grads_and_vars:\n",
    "            if ('kernel' in var.name) and ('bn' not in var.name.lower()):\n",
    "                var.assign_sub(self.weight_decay * var)\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"weight_decay\": float(self.weight_decay),  # 确保是float\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "544db265-6951-42cf-9123-5a9412f00d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastNSaver(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, n=10):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.history = deque(maxlen=n)  # 存最近N次 (val_acc, weights)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_acc = logs.get(\"val_accuracy\")\n",
    "        if val_acc is not None:\n",
    "            # 保存 (val_acc, 当前权重)\n",
    "            weights = self.model.get_weights()\n",
    "            self.history.append((val_acc, weights))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        # 在最后N次中选最优\n",
    "        if not self.history:\n",
    "            return\n",
    "        best_acc, best_weights = max(self.history, key=lambda x: x[0])\n",
    "        print(f\" Using best val_acc={best_acc:.4f} from last {self.n} epochs\")\n",
    "        self.model.set_weights(best_weights)  # 恢复最佳权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47d65cd3-569b-4d19-a999-d595b1b517bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Res():\n",
    "    model = tf.keras.models.load_model('Res32_cifar10.h5',custom_objects={\n",
    "        'CustomWeightDecaySGD': CustomWeightDecaySGD,\n",
    "        'WarmUpCosine': WarmUpCosine\n",
    "    })\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e2e2c6f-0487-4831-9852-adf0323c615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_relu(x, filters, kernel_size, strides=1):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding='same',use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    return tf.keras.layers.ReLU()(x)\n",
    "\n",
    "def residual_block(x, filter1, filter2, downsample=False):\n",
    "    shortcut = x\n",
    "    strides = 2 if downsample else 1\n",
    "    x = conv_bn_relu(x, filter1, 3, strides)\n",
    "    x = tf.keras.layers.Conv2D(filter2, 3, strides=1, padding='same',use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    if downsample:\n",
    "        shortcut = tf.keras.layers.Conv2D(filter2, 1, strides=strides, padding='same',use_bias=False)(shortcut)\n",
    "        shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
    "    x = tf.keras.layers.add([x, shortcut])\n",
    "    return tf.keras.layers.ReLU()(x)\n",
    "\n",
    "\n",
    "def Res_model(NN,input_shape=(32,32,3), num_classes=10):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = conv_bn_relu(inputs, NN[5], 3)\n",
    "    x = residual_block(x, NN[0], NN[5])\n",
    "    x = residual_block(x, NN[1], NN[5])\n",
    "    x = residual_block(x, NN[2], NN[5])\n",
    "    x = residual_block(x, NN[3], NN[5])\n",
    "    x = residual_block(x, NN[4], NN[5])\n",
    "    x = residual_block(x, NN[6], NN[11], downsample=True)\n",
    "    x = residual_block(x, NN[7], NN[11])\n",
    "    x = residual_block(x, NN[8], NN[11])\n",
    "    x = residual_block(x, NN[9], NN[11])\n",
    "    x = residual_block(x, NN[10], NN[11])\n",
    "    x = residual_block(x, NN[12], NN[17], downsample=True)\n",
    "    x = residual_block(x, NN[13], NN[17])\n",
    "    x = residual_block(x, NN[14], NN[17])\n",
    "    x = residual_block(x, NN[15],NN[17])\n",
    "    x = residual_block(x, NN[16],NN[17])\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes,activation='softmax')(x)\n",
    "    return tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78515397",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_Res()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c71b4f71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   432         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 32, 32, 16)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['batch_normalization_2[0][0]',  \n",
      "                                                                  're_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 32, 32, 16)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 32, 32, 16)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 16)   0           ['batch_normalization_4[0][0]',  \n",
      "                                                                  're_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 32, 32, 16)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 32, 32, 16)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 32, 16)   0           ['batch_normalization_6[0][0]',  \n",
      "                                                                  're_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 32, 32, 16)   0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 32, 32, 16)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 32, 32, 16)   0           ['batch_normalization_8[0][0]',  \n",
      "                                                                  're_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)                 (None, 32, 32, 16)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 16)   2304        ['re_lu_8[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)                 (None, 32, 32, 16)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 16)   2304        ['re_lu_9[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 32, 16)  64          ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 32, 32, 16)   0           ['batch_normalization_10[0][0]', \n",
      "                                                                  're_lu_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)                (None, 32, 32, 16)   0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 16, 16, 32)   4608        ['re_lu_10[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16, 16, 32)  128         ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)                (None, 16, 16, 32)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_11[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 16, 16, 32)   512         ['re_lu_10[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 32)  128         ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 16, 16, 32)  128         ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 16, 16, 32)   0           ['batch_normalization_12[0][0]', \n",
      "                                                                  'batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)                (None, 16, 16, 32)   0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_12[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 16, 16, 32)  128         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)                (None, 16, 16, 32)   0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_13[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 16, 16, 32)  128         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 16, 16, 32)   0           ['batch_normalization_15[0][0]', \n",
      "                                                                  're_lu_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)                (None, 16, 16, 32)   0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_14[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16, 16, 32)  128         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)                (None, 16, 16, 32)   0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_15[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 16, 16, 32)  128         ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 16, 16, 32)   0           ['batch_normalization_17[0][0]', \n",
      "                                                                  're_lu_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)                (None, 16, 16, 32)   0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_16[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 16, 16, 32)  128         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)                (None, 16, 16, 32)   0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_17[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 16, 16, 32)  128         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 16, 16, 32)   0           ['batch_normalization_19[0][0]', \n",
      "                                                                  're_lu_16[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_18 (ReLU)                (None, 16, 16, 32)   0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_18[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 16, 16, 32)  128         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_19 (ReLU)                (None, 16, 16, 32)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 16, 16, 32)   9216        ['re_lu_19[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 16, 16, 32)  128         ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 16, 16, 32)   0           ['batch_normalization_21[0][0]', \n",
      "                                                                  're_lu_18[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_20 (ReLU)                (None, 16, 16, 32)   0           ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 8, 8, 64)     18432       ['re_lu_20[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 8, 8, 64)    256         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_21 (ReLU)                (None, 8, 8, 64)     0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_21[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 8, 8, 64)     2048        ['re_lu_20[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 8, 8, 64)    256         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 8, 8, 64)    256         ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_23[0][0]', \n",
      "                                                                  'batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_22 (ReLU)                (None, 8, 8, 64)     0           ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_22[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 8, 8, 64)    256         ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_23 (ReLU)                (None, 8, 8, 64)     0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_23[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 8, 8, 64)    256         ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_26[0][0]', \n",
      "                                                                  're_lu_22[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_24 (ReLU)                (None, 8, 8, 64)     0           ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_24[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 8, 8, 64)    256         ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_25 (ReLU)                (None, 8, 8, 64)     0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_25[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 8, 8, 64)    256         ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_28[0][0]', \n",
      "                                                                  're_lu_24[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_26 (ReLU)                (None, 8, 8, 64)     0           ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_26[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 8, 8, 64)    256         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_27 (ReLU)                (None, 8, 8, 64)     0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_27[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 8, 8, 64)    256         ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_30[0][0]', \n",
      "                                                                  're_lu_26[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_28 (ReLU)                (None, 8, 8, 64)     0           ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_28[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 8, 8, 64)    256         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_29 (ReLU)                (None, 8, 8, 64)     0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 8, 8, 64)     36864       ['re_lu_29[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 8, 8, 64)    256         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_32[0][0]', \n",
      "                                                                  're_lu_28[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_30 (ReLU)                (None, 8, 8, 64)     0           ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 64)          0           ['re_lu_30[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           650         ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 469,370\n",
      "Trainable params: 466,906\n",
      "Non-trainable params: 2,464\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "564ea471-4e46-47bc-9d1f-e0d6df7de5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def JW(m, M):\n",
    "    \"\"\"\n",
    "    Compute the binary MD-LP J_w_value.\n",
    "\n",
    "    Args:\n",
    "        m: 1-D tensor of shape [d], the mean of the Minkowski difference of a \n",
    "           binary classification dataset.\n",
    "        M: 2-D tensor of shape [d, N], binary classification dataset Minkowski \n",
    "           difference set.\n",
    "           \n",
    "    Key idea:\n",
    "        - Calculate the approximate solution m_weighted for the optimal weights \n",
    "          in the MD-LP.\n",
    "        - Calculate the MD-LP based on the approximately optimal weights, and \n",
    "          perform a left truncation at 0.5. \n",
    "    Returns:\n",
    "        Binary MD-LP value.\n",
    "    \"\"\"\n",
    "    row_norm_sq = tf.reduce_sum(tf.square(M), axis=1)  \n",
    "    reciprocal_norm = tf.where(row_norm_sq != 0,\n",
    "                               tf.math.reciprocal(row_norm_sq),\n",
    "                               tf.zeros_like(row_norm_sq))  \n",
    "    m_weighted = m * reciprocal_norm  \n",
    "    m_weighted = tf.reshape(m_weighted, [1, -1])  \n",
    "    mM = tf.matmul(m_weighted, M)\n",
    "    L1 = tf.reduce_sum(mM)\n",
    "    L_1 = tf.reduce_sum(tf.abs(mM))\n",
    "    J_w_value = tf.abs(L1) / (L_1 + 1e-8)\n",
    "    J_w_value = tf.maximum(J_w_value, 0.5)\n",
    "    return J_w_value\n",
    "def W(X, Y, k, n_c=10):\n",
    "    \"\"\"\n",
    "    This function is used to calculate the top k largest binary classification \n",
    "    problems MD-LP used in the multi-classification problem calculation. Here, \n",
    "    the binary classification problems are obtained by combining each pair of \n",
    "    categories of the multi-classification problem.\n",
    "    Args:\n",
    "        X: Tensor/array of shape [b, l, w]. Channel output.\n",
    "        Y: Tensor/array of labels of shape [b]. Data labels.\n",
    "        k: Number of the largest binary MD-LP to keep.\n",
    "        n_c: Number of classes.\n",
    "    Returns:\n",
    "        JK_list: Tensor of shape [k], the top-k MD-LP.\n",
    "    \"\"\"\n",
    "    b, l, w = X.shape\n",
    "    X = tf.reshape(X, [b, l*w])   # flatten\n",
    "    J_list = []\n",
    "    for i, j in itertools.combinations(range(n_c), 2):\n",
    "        mask_1 = tf.reshape(tf.equal(Y, i), [-1])\n",
    "        mask_2 = tf.reshape(tf.equal(Y, j), [-1])\n",
    "        X1 = tf.boolean_mask(X, mask_1)\n",
    "        X2 = tf.boolean_mask(X, mask_2)\n",
    "        n1 = tf.shape(X1)[0]\n",
    "        n2 = tf.shape(X2)[0]\n",
    "        m_i = tf.reduce_sum(X1, axis=0) * tf.cast(n2, tf.float32) - tf.reduce_sum(X2, axis=0) * tf.cast(n1, tf.float32)\n",
    "        M_i = tf.reshape(X1[:, None, :] - X2[None, :, :], [-1, l*w])\n",
    "        M_i = tf.transpose(M_i)\n",
    "        J = JW(m_i, M_i)\n",
    "        J_list.append(J)\n",
    "    J_list = tf.stack(J_list)\n",
    "    JK_list , JK_inde = tf.math.top_k(J_list,k)\n",
    "    return JK_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "779631d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1_channel(x_L,y,prune_rate, nnn, alpha=2.5):\n",
    "    \"\"\"\n",
    "    This function computes TCR measure of each channel in convolutional layer.\n",
    "    \n",
    "    Given the output of a convolutional layer, this function will execute:\n",
    "    - Treating each channel independently and computing a multi-class MD-LP \n",
    "      via function W;\n",
    "    - By applying nonlinear transformation, a TCR measure is constructed \n",
    "      to enhance the separability of MD-LP.\n",
    "    \n",
    "    Key Args:\n",
    "    x_L (Tensor):\n",
    "        Output of a convolutional hidden layer, with shape \n",
    "        [batch_size, height, width, channels].\n",
    "    y (Tensor):\n",
    "        Ground-truth labels corresponding to the input samples.\n",
    "    alpha (float, optional):\n",
    "        LP transformation parameter. Used to enhance the separability \n",
    "        of the MD-LP close to 1.\n",
    "    \n",
    "    Returns:\n",
    "    jw:\n",
    "        TCR measure of each channel.\n",
    "    \"\"\"\n",
    "    a, b, d, c = x_L.shape\n",
    "    jw = tf.zeros([c], dtype=tf.float32)\n",
    "    alpha = tf.cast(alpha, tf.float32)\n",
    "    for j in tf.range(c):\n",
    "        N_tf = W(x_L[:,:,:,j], y, nnn)\n",
    "        jw_j = tf.norm(N_tf) / tf.sqrt(float(nnn))\n",
    "        jw_j = (tf.exp(alpha * (2*jw_j-1)) - 1.0) / (tf.exp(alpha) - 1.0)\n",
    "        jw = tf.tensor_scatter_nd_update(jw, [[j]], [jw_j])\n",
    "    return jw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2acfaba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_channel(x_LG, y, prune_rate, nnn=45, esp=1e-8):\n",
    "    \"\"\"\n",
    "    This function computes the structural redundancy evaluation criterion R_L \n",
    "    and determines the set of retained channel indices `channel_i_label` used \n",
    "    by the pruning algorithm for a layer group.\n",
    "    \n",
    "    Given the output of a convolutional layer, this function will execute:\n",
    "    - By analyzing the propensity calculation of TCR measure, an evaluation \n",
    "      criterion for evaluating the redundancy of convolutional layers is derived.\n",
    "    - Based on the TCR measure, the pruning threshold is calculated and the \n",
    "      channels that remain after pruning are selected.\n",
    "    \n",
    "    Key Args:\n",
    "    x_LG (a list of Tensors):\n",
    "        Output of a group of convolutional hidden layers.\n",
    "    y (Tensor):\n",
    "        Ground-truth labels corresponding to the input samples.\n",
    "    prune_rate (float):\n",
    "        Pruning parameter. Used to control the strictness of pruning.\n",
    "    \n",
    "    Returns:\n",
    "    channel_i_label (ndarray):\n",
    "        Indices of channels retained after pruning.\n",
    "    R_L (float):\n",
    "        Structural redundancy evaluation criterion of the layer group,\n",
    "    \"\"\"\n",
    "    a, b, d, c = x_LG[-1].shape\n",
    "    L1_list = []\n",
    "    for i in range(len(x_LG)):\n",
    "        L1_i = L1_channel(x_LG[i], y, prune_rate, nnn)\n",
    "        L1_list.append(L1_i)\n",
    "    L1 = tf.stack(L1_list, axis=0)  # shape: (len(x_LG), c)\n",
    "    jw = tf.reduce_mean(L1, axis=0) # shape: (c,)\n",
    "    jw_min = tf.maximum(tf.reduce_min(jw) - esp, 0.0)\n",
    "    jw_max = tf.reduce_max(jw)\n",
    "    me = tf.sqrt(tf.reduce_mean(tf.square(jw - jw_min)))\n",
    "    jd = jw_min + prune_rate * me\n",
    "    mean = tf.maximum(tf.reduce_mean(jw) - esp, 0.0)\n",
    "    R_L = tf.reduce_mean(tf.sign(jw - mean))\n",
    "    channel_i_label = tf.where(jw >= jd)[:,0]\n",
    "    return channel_i_label.numpy(), R_L.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62555afb-03be-491e-a73b-8a314b2941ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Group_1(x_L,w,S):\n",
    "    \"\"\"This function is used to collect the key hidden layer outputs of the first block \n",
    "    in ResNet. Here, x_L1 is used to update the list of output from the ResNet \n",
    "    convolutional layers, and x_L2 is used to provide the hidden layer outputs required \n",
    "    for layer group pruning.\"\"\"\n",
    "    wb=w[0]\n",
    "    bn=w[1]\n",
    "    x_L1=[]\n",
    "    x_L2=[]\n",
    "    x_1=layer_xL(\"conv2d\",x_L,w=wb[0])\n",
    "    x_1=layer_xL(\"batch_normalization\",x_1,w=bn[0])\n",
    "    x_1=layer_xL(\"activation\",x_1)\n",
    "    x_L2.append(deepcopy(x_1))\n",
    "    #x_1=layer_xL(\"maxpooling\",x_1)\n",
    "    x_L1.append(deepcopy(x_1))\n",
    "\n",
    "    for i in range(5):\n",
    "        \n",
    "        x_2=layer_xL(\"conv2d\",x_1,w=wb[2*i+1])\n",
    "        x_2=layer_xL(\"batch_normalization\",x_2,w=bn[2*i+1])\n",
    "        x_2=layer_xL(\"activation\",x_2)\n",
    "        x_L1.append(deepcopy(x_2))\n",
    "        \n",
    "        x_2=layer_xL(\"conv2d\",x_2,w=wb[2*i+2])\n",
    "        x_2=layer_xL(\"batch_normalization\",x_2,w=bn[2*i+2])\n",
    "        x_1=layer_xL(\"add\",[x_2,x_1])\n",
    "        x_1=layer_xL(\"activation\",x_1)\n",
    "        x_L1.append(deepcopy(x_1))\n",
    "        x_L2.append(deepcopy(x_1))\n",
    "    return x_L1,x_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b224b60b-5f03-4f1d-9d30-287e9dcc8aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Group_L(x_L,w,S):\n",
    "    \"\"\"This function is used to collect the key hidden layer outputs of the other block \n",
    "    in ResNet. Here, x_L1 is used to update the list of output from the ResNet \n",
    "    convolutional layers, and x_L2 is used to provide the hidden layer outputs required \n",
    "    for layer group pruning.\"\"\"\n",
    "    wb=w[0]\n",
    "    bn=w[1]\n",
    "    x_L1=[]\n",
    "    x_L2=[]\n",
    "    x_1=layer_xL(\"conv2d\",x_L,w=wb[0],S=2)\n",
    "    x_1=layer_xL(\"batch_normalization\",x_1,w=bn[0])\n",
    "    x_1=layer_xL(\"activation\",x_1)\n",
    "    x_L1.append(deepcopy(x_1))\n",
    "            \n",
    "    x_1=layer_xL(\"conv2d\",x_1,w=wb[1])\n",
    "    x_2=layer_xL(\"conv2d\",x_L,w=wb[2],S=2)\n",
    "    x_1=layer_xL(\"batch_normalization\",x_1,w=bn[1])\n",
    "    x_2=layer_xL(\"batch_normalization\",x_2,w=bn[2])\n",
    "    x_1=layer_xL(\"add\",[x_1,x_2])\n",
    "    x_1=layer_xL(\"activation\",x_1)\n",
    "    x_L1.append(deepcopy(x_1))\n",
    "    x_L2.append(deepcopy(x_1))\n",
    "\n",
    "    for i in range(4):\n",
    "        \n",
    "        x_2=layer_xL(\"conv2d\",x_1,w=wb[2*i+3])\n",
    "        x_2=layer_xL(\"batch_normalization\",x_2,w=bn[2*i+3])\n",
    "        x_2=layer_xL(\"activation\",x_2)\n",
    "        x_L1.append(deepcopy(x_2))\n",
    "    \n",
    "        x_2=layer_xL(\"conv2d\",x_2,w=wb[2*i+4])\n",
    "        x_2=layer_xL(\"batch_normalization\",x_2,w=bn[2*i+4])\n",
    "        x_1=layer_xL(\"add\",[x_2,x_1])\n",
    "        x_1=layer_xL(\"activation\",x_1)\n",
    "        x_L1.append(deepcopy(x_1))\n",
    "        x_L2.append(deepcopy(x_1))\n",
    "    return x_L1,x_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b348156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_xL(layer_name,x_L,w=None,F=False,S=1):\n",
    "    \"\"\"This function is used to obtain the outputs of various hidden layers or layer groups in the \n",
    "    network, which is utilized for updating the outputs of hidden layers or for performing layer \n",
    "    group pruning calculations.\"\"\"\n",
    "    if \"conv2d\" in layer_name:\n",
    "        weight=w[0]\n",
    "        strides=[1,S,S,1]\n",
    "        x_L1=tf.nn.conv2d(x_L,weight,strides=strides,padding=\"SAME\")\n",
    "        #x_L1=tf.nn.bias_add(x_L1,bias)\n",
    "        return x_L1\n",
    "    if \"first\" in layer_name:\n",
    "        x_L1,x_L2=Group_1(x_L,w,S)\n",
    "        if F==True:\n",
    "            return x_L2\n",
    "        else:\n",
    "            return x_L1\n",
    "    if \"group\" in layer_name:\n",
    "        x_L1,x_L2=Group_L(x_L,w,S)\n",
    "        if F==True:\n",
    "            return x_L2\n",
    "        else:\n",
    "            return x_L1\n",
    "    if \"batch_normalization\" in layer_name:\n",
    "        gamma,beta,mean,var=w\n",
    "        x_L1=tf.nn.batch_normalization(x_L,mean=mean,\n",
    "                                          variance=var,\n",
    "                                          offset=beta,\n",
    "                                          scale=gamma,variance_epsilon=1e-5)\n",
    "        return x_L1\n",
    "    if \"activation\" in layer_name:\n",
    "        x_L1=tf.nn.relu(x_L)\n",
    "        return x_L1\n",
    "    if \"add\" in layer_name:\n",
    "        x1,x2=x_L\n",
    "        x_L1=tf.math.add(x1,x2)\n",
    "        return x_L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "355a0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_block(a,b,G,weight_list,x_LG,First=False,Group=False,R=False,F=False):\n",
    "    \"\"\"This function is used to update the list of hidden layer outputs \n",
    "    of ResNet after function pruning. For single-hidden-layer pruning, \n",
    "    it only needs to update the output of the convolutional layer within \n",
    "    the block. However, for layer group pruning, since it involves multiple \n",
    "    blocks, the output of all convolutional layers in these blocks needs \n",
    "    to be updated.\"\"\"\n",
    "    if Group==False:\n",
    "        if R==True:\n",
    "            x_1=x_LG[b]\n",
    "            layer_l=G[a][0]\n",
    "            wc=weight_list[layer_l]\n",
    "            wb=weight_list[layer_l+1]\n",
    "            x_2=layer_xL(\"conv2d\",x_1,wc,S=2)\n",
    "            x_2=layer_xL(\"batch_normalization\",x_2,wb)\n",
    "            x_2=layer_xL(\"activation\",x_2)\n",
    "            x_LG[b+1]=x_2\n",
    "            layer_l=G[a+5][0]\n",
    "            wc=weight_list[layer_l]\n",
    "            wb=weight_list[layer_l+2]\n",
    "            x_2=layer_xL(\"conv2d\",x_2,wc)\n",
    "            x_2=layer_xL(\"batch_normalization\",x_2,wb)\n",
    "            layer_l=G[a+5][1]\n",
    "            wc=weight_list[layer_l]\n",
    "            wb=weight_list[layer_l+2]\n",
    "            x_1=layer_xL(\"conv2d\",x_1,wc,S=2)\n",
    "            x_1=layer_xL(\"batch_normalization\",x_1,wb)\n",
    "            x_1=layer_xL(\"add\",[x_2,x_1])\n",
    "            x_1=layer_xL(\"activation\",x_1)\n",
    "            x_LG[b+2]=x_1\n",
    "            return x_LG\n",
    "        if R==False:\n",
    "            x_1=x_LG[b]\n",
    "            layer_l=G[a][0]\n",
    "            wc=weight_list[layer_l]\n",
    "            wb=weight_list[layer_l+1]\n",
    "            x_2=layer_xL(\"conv2d\",x_1,wc)\n",
    "            x_2=layer_xL(\"batch_normalization\",x_2,wb)\n",
    "            x_2=layer_xL(\"activation\",x_2)\n",
    "            x_LG[b+1]=x_2\n",
    "            if First==True:\n",
    "                layer_l=G[a+5][1]\n",
    "            else:\n",
    "                aaa=int(((a+1)//6+1)*6)\n",
    "                layer_l=G[aaa-1][a%6+1]\n",
    "            wc=weight_list[layer_l]\n",
    "            wb=weight_list[layer_l+1]\n",
    "            x_2=layer_xL(\"conv2d\",x_2,wc)\n",
    "            x_2=layer_xL(\"batch_normalization\",x_2,wb)\n",
    "            x_1=layer_xL(\"add\",[x_2,x_1])\n",
    "            x_1=layer_xL(\"activation\",x_1)\n",
    "            x_LG[b+2]=x_1\n",
    "            return x_LG\n",
    "    if Group==True:\n",
    "        if First==True:\n",
    "            label=[G[a][0],G[a-5][0],G[a][1],G[a-4][0],G[a][2],G[a-3][0],G[a][3],G[a-2][0],G[a][4],G[a-1][0],G[a][5]]\n",
    "            w1=[]\n",
    "            w2=[]\n",
    "            x_1=x_LG[b]\n",
    "            for i in range(11):\n",
    "                w1.append(weight_list[label[i]])\n",
    "                w2.append(weight_list[label[i]+1])\n",
    "            w=[w1,w2]\n",
    "            x_1=layer_xL(\"first\",x_1,w,F=False,S=1)\n",
    "            for i in range(11):\n",
    "                x_LG[b+i+1]=x_1[i]\n",
    "            return x_LG\n",
    "        else:\n",
    "            label=[G[a-5][0],G[a][0],G[a][1],G[a-4][0],G[a][2],G[a-3][0],G[a][3],G[a-2][0],G[a][4],G[a-1][0],G[a][5]]\n",
    "            w1=[]\n",
    "            w2=[]\n",
    "            x_1=x_LG[b]\n",
    "            l=[1,2,2,1,1,1,1,1,1,1,1]\n",
    "            for i in range(11):\n",
    "                w1.append(weight_list[label[i]])\n",
    "                w2.append(weight_list[label[i]+l[i]])\n",
    "            w=[w1,w2]\n",
    "            x_1=layer_xL(\"group\",x_1,w)\n",
    "            for i in range(10):\n",
    "                x_LG[b+i+1]=x_1[i]\n",
    "            return x_LG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4ac50a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(a,b,G,x_LG,weight_list,First=False):\n",
    "    \"\"\"This function is used to provide the required output list \n",
    "    for 'prune_channel' function when performing layer group pruning.\"\"\"\n",
    "    x_l=x_LG[b]\n",
    "    if First==True:\n",
    "        label=[G[a][0],G[a-5][0],G[a][1],G[a-4][0],G[a][2],G[a-3][0],G[a][3],G[a-2][0],G[a][4],G[a-1][0],G[a][5]]\n",
    "    else:\n",
    "        label=[G[a-5][0],G[a][0],G[a][1],G[a-4][0],G[a][2],G[a-3][0],G[a][3],G[a-2][0],G[a][4],G[a-1][0],G[a][5]]\n",
    "    w1=[]\n",
    "    w2=[]\n",
    "    for i in range(11):\n",
    "        w1.append(weight_list[label[i]])\n",
    "    l=[1,2,2,1,1,1,1,1,1,1,1]\n",
    "    if First==True:\n",
    "        for i in range(11):\n",
    "            w2.append(weight_list[label[i]+1])\n",
    "        w=[w1,w2]\n",
    "        x_L=layer_xL('first',x_l,w,F=True)\n",
    "    else:\n",
    "        for i in range(11):\n",
    "            w2.append(weight_list[label[i]+l[i]])\n",
    "        w=[w1,w2]\n",
    "        x_L=layer_xL('group',x_l,w,F=True)\n",
    "    return x_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f4a2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model,G,P,x,y,prune_rate,q=114):\n",
    "    \"\"\"\n",
    "    Structured Channel Group Pruning Function Based on MD-LP (Channel-wise Pruning) \n",
    "    \n",
    "    This function is the main function for pruning in ResNet-32. It achieves \n",
    "    the pruning of layer groups by using the provided convolution layer groups G.\n",
    "    The main process of this function is as follows:\n",
    "    - Pruning Preparation: Based on the hidden layer positions provided by P, \n",
    "      construct the input/output lists for the pruning-required convolutional layers, \n",
    "      as well as the list of convolution kernel parameters and BN layer parameters.\n",
    "    - Layer Group Pruning: In accordance with the sequence in G, the pruning_channel \n",
    "      function is used to perform pruning successively, resulting in the channels \n",
    "      that are retained after pruning, which are labeled as channel_new_label.\n",
    "    - Output/Parameter Update: Based on the channel_new_label, the parameters of the \n",
    "      convolutional layers included in the group, as well as the parameters of the \n",
    "      convolutional layers whose outputs are used as inputs, are updated. And the \n",
    "      input/output lists of the convolutional layers are updated according to the \n",
    "      updated parameters.\n",
    "    \n",
    "    Input:\n",
    "    model : Original Keras ResNet-18\n",
    "    x : Network input samples (used for forward propagation and channel evaluation)\n",
    "    y : Sample labels (used for metric calculation in prune_channel)\n",
    "    prune_rate : Pruning parameter\n",
    "    G : The grouped list of network convolution layers, based on the ReNet network \n",
    "    structure, divides the hidden layers of ResNet. It is used to ensure that the \n",
    "    output results of the network hidden layers within the same group after pruning \n",
    "    can still maintain the same size and can be added together.\n",
    "    \n",
    "    Output:\n",
    "    weight_list: List of weights for each convolutional / BN layer after pruning.\n",
    "    channel_label: Record of the number of retained channels for each layer group.   \n",
    "    \"\"\"\n",
    "    layer_outputs =[layer.output for layer in model.layers] \n",
    "    activation_model = tf.keras.models.Model(inputs=model.input,outputs=layer_outputs)\n",
    "    layer_x=activation_model.predict(x)\n",
    "    weight_list=[]\n",
    "    # =========================\n",
    "    # Collect the input/output of the convolutional layers, and the parameters\n",
    "    # of the convolutional layers and BN layers in the ResNet network.\n",
    "    # =========================\n",
    "    for i in range(q+1):\n",
    "        layer=model.layers[i]\n",
    "        if \"conv\" in layer.name:\n",
    "            w=layer.get_weights()\n",
    "            weight_list.append(w)\n",
    "            #print(i)\n",
    "        elif \"dense\" in layer.name:\n",
    "            w,b=layer.get_weights()\n",
    "            weight_list.append([w,b])\n",
    "        elif \"batch_normalization\" in layer.name:\n",
    "            g,b,m,v=layer.get_weights()\n",
    "            weight_list.append([g,b,m,v])\n",
    "        else:\n",
    "            weight_list.append(None)\n",
    "    x_LG=[]\n",
    "    for i in range(len(P)):\n",
    "        x_LG.append(layer_x[P[i]])\n",
    "    channel_label=[]\n",
    "    b=1\n",
    "    for i in range(len(G)):\n",
    "        if len(G[i])==1:\n",
    "            # =========================\n",
    "            # According to the prune_channel function, the single convolution layer in G is pruned, \n",
    "            # resulting in the retained channels after pruning. Based on the pruning results, the \n",
    "            # network parameters and network input/output are updated.\n",
    "            # =========================\n",
    "            if (b-1)%10==0:\n",
    "                a=G[i][0]\n",
    "                channel_new_label,r_L=prune_channel([x_LG[b+1]],y,prune_rate)\n",
    "                print(len(channel_new_label),0)\n",
    "                weight_list[a][0]=weight_list[a][0][:,:,:,channel_new_label]\n",
    "                #weight_list[a][1]=weight_list[a][1][channel_new_label]\n",
    "                for j in range(4):\n",
    "                    weight_list[a+1][j]=weight_list[a+1][j][channel_new_label]\n",
    "                if i==0:\n",
    "                    a1=G[i+5][1]\n",
    "                    weight_list[a1][0]=weight_list[a1][0][:,:,channel_new_label,:]\n",
    "                    x_LG=x_block(i,b,G,weight_list,x_LG,First=True)\n",
    "                else:\n",
    "                    a1=G[i+5][0]\n",
    "                    weight_list[a1][0]=weight_list[a1][0][:,:,channel_new_label,:]\n",
    "                    x_LG=x_block(i,b,G,weight_list,x_LG,First=False,R=True)\n",
    "                channel_label.append(len(channel_new_label))\n",
    "                b+=2\n",
    "                continue\n",
    "            else:\n",
    "                a=G[i][0]\n",
    "                aaa=int(((i+1)//6+1)*6)\n",
    "                channel_new_label,r_L=prune_channel([x_LG[b+1]],y,prune_rate)\n",
    "                print(len(channel_new_label),1)\n",
    "                weight_list[a][0]=weight_list[a][0][:,:,:,channel_new_label]\n",
    "                #weight_list[a][1]=weight_list[a][1][channel_new_label]\n",
    "                for j in range(4):\n",
    "                    weight_list[a+1][j]=weight_list[a+1][j][channel_new_label]\n",
    "                channel_label.append(len(channel_new_label))\n",
    "                a1=G[aaa-1][i%6+1]\n",
    "                weight_list[a1][0]=weight_list[a1][0][:,:,channel_new_label,:]\n",
    "                x_LG=x_block(i,b,G,weight_list,x_LG)\n",
    "                if (b-1)%10==8:\n",
    "                    if i==4:\n",
    "                        b-=9\n",
    "                    else:\n",
    "                        b-=8\n",
    "                else:\n",
    "                    b+=2\n",
    "                continue\n",
    "        if len(G[i])>1:\n",
    "            # =========================\n",
    "            # According to the prune_channel function, the layer group in G with multiple convolutional \n",
    "            # layers is pruned to obtain a unified set of pruned channels. Based on the pruning results, \n",
    "            # the network parameters and network input/output are updated.\n",
    "            # =========================\n",
    "            if i==5:\n",
    "                x_LP=get_x(i,b,G,x_LG,weight_list,First=True)\n",
    "            else:\n",
    "                x_LP=get_x(i,b,G,x_LG,weight_list)\n",
    "            channel_new_label,r_L=prune_channel(x_LP,y,prune_rate)\n",
    "            print(len(channel_new_label),2)\n",
    "            for g in G[i]:\n",
    "                weight_list[g][0]=weight_list[g][0][:,:,:,channel_new_label]\n",
    "                #weight_list[g][1]=weight_list[g][1][channel_new_label]\n",
    "            for j in range(4):\n",
    "                weight_list[G[i-4+j][0]][0]=weight_list[G[i-4+j][0]][0][:,:,channel_new_label,:]\n",
    "            if i==5:\n",
    "                for g in G[i]:\n",
    "                    for j in range(4):\n",
    "                        weight_list[g+1][j]=weight_list[g+1][j][channel_new_label]\n",
    "                weight_list[G[i-5][0]][0]=weight_list[G[i-5][0]][0][:,:,channel_new_label,:]\n",
    "            else:\n",
    "                l=[2,2,1,1,1,1]\n",
    "                for g in range(6):\n",
    "                    for j in range(4):\n",
    "                        weight_list[G[i][g]+l[g]][j]=weight_list[G[i][g]+l[g]][j][channel_new_label]\n",
    "            if i!=len(G)-1:\n",
    "                weight_list[G[i+1][0]][0]=weight_list[G[i+1][0]][0][:,:,channel_new_label,:]\n",
    "                weight_list[G[i+6][1]][0]=weight_list[G[i+6][1]][0][:,:,channel_new_label,:]\n",
    "            if i==5:\n",
    "                x_LG=x_block(i,b,G,weight_list,x_LG,First=True,R=False,Group=True)\n",
    "                b+=11\n",
    "            else:\n",
    "                x_LG=x_block(i,b,G,weight_list,x_LG,First=False,R=False,Group=True)\n",
    "                b+=10\n",
    "            channel_label.append(len(channel_new_label))\n",
    "            continue\n",
    "    weight_list[q][0]=weight_list[q][0][channel_new_label]\n",
    "    return weight_list,channel_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21977bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "G=[[4],[11],[18],[25],[32],[1,7,14,21,28,35],[39],[48],[55],[62],[69],[42,43,51,58,65,72],[76],[85],[92],[99],[106],[79,80,88,95,102,109]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73763289",
   "metadata": {},
   "outputs": [],
   "source": [
    "P=[0,3,6,10,13,17,20,24,27,31,34,38,41,47,50,54,57,61,64,68,71,75,78,84,87,91,94,98,101,105,108,112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2428f648-7af9-4278-8686-5024da702c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pr(model,weight_list,channel_label):\n",
    "    \"\"\"This function is used to construct a pruned network by using \n",
    "    the given pruned network structure and parameters.\"\"\"\n",
    "    model_p=Res_model(channel_label)\n",
    "    for i in range(len(weight_list)):\n",
    "        if weight_list[i]!=None:\n",
    "            w = [ww for ww in weight_list[i]]\n",
    "            model_p.layers[i].set_weights(w)\n",
    "    return model_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c451b93e-3b25-44cf-b461-e4edd32e51c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d9dd391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain(model,x_train,y_train,x_test,y_test):\n",
    "    \"\"\"This function is used to fine-tune the pruned network \n",
    "    using the same method as the original network training.\"\"\"\n",
    "    total_steps = epochs * (x_train.shape[0] // batch_size)\n",
    "    warmup_steps = warmup_epochs * (x_train.shape[0] // batch_size)\n",
    "    lr_schedule = WarmUpCosine(initial_lr, total_steps, warmup_steps)\n",
    "    optimizer = CustomWeightDecaySGD(weight_decay=weight_decay,learning_rate=lr_schedule,momentum=0.9,nesterov=True)\n",
    "    loss_fn=tf.keras.losses.CategoricalCrossentropy()\n",
    "    model.compile(optimizer=optimizer,loss=loss_fn,metrics=['accuracy'])\n",
    "    saver = LastNSaver(n=20)\n",
    "    model.fit(datagen.flow(x_train, y_train_onehot,batch_size=batch_size),\n",
    "                            steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=(x_test, y_test_onehot),verbose=2,callbacks=[saver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34c12c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"These functions are used to calculate the FLOPs \n",
    "and the number of parameters of the network.\"\"\"\n",
    "def conv_flops_params(layer, input_shape):\n",
    "    h_in, w_in, cin = input_shape[1:]\n",
    "    h_out, w_out, cout = layer.output_shape[1:]\n",
    "    k_h, k_w = layer.kernel_size\n",
    "    flops = h_out * w_out * cin * cout * k_h * k_w\n",
    "    params = cin * cout * k_h * k_w\n",
    "    if layer.use_bias:\n",
    "        params += cout\n",
    "    return flops, params, (h_out, w_out, cout)\n",
    "def dense_flops_params(layer, input_shape):\n",
    "    cin = input_shape[-1]\n",
    "    cout = layer.units\n",
    "    flops = cin * cout\n",
    "    params = cin * cout\n",
    "    if layer.use_bias:\n",
    "        params += cout\n",
    "    return flops, params, (cout,)\n",
    "def compute_flops_params(model, input_shape=(32, 32, 3)):\n",
    "    total_flops = 0\n",
    "    total_params = 0\n",
    "    dummy_input = tf.zeros((1, *input_shape))\n",
    "    _ = model(dummy_input)\n",
    "    current_shape = (1, *input_shape)\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            flops, params, out_shape = conv_flops_params(layer, current_shape)\n",
    "            total_flops += flops\n",
    "            total_params += params\n",
    "            current_shape = (1, *out_shape)\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            flops, params, out_shape = dense_flops_params(layer, current_shape)\n",
    "            total_flops += flops\n",
    "            total_params += params\n",
    "            current_shape = (1, *out_shape)\n",
    "    return total_flops, total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c45c7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_layers(model,x,y,R=P[1:]):\n",
    "    \"\"\"This function is used to obtain the structural redundancy \n",
    "    criterion of each block in the ResNet-18 network.\"\"\"\n",
    "    layer_outputs =[layer.output for layer in model.layers] \n",
    "    activation_model = tf.keras.models.Model(inputs=model.input,outputs=layer_outputs)\n",
    "    layer_x=activation_model.predict(x)\n",
    "    R_L=[]\n",
    "    channel_label=[]\n",
    "    for i in range(len(R)):\n",
    "        print('start')\n",
    "        x_L=layer_x[R[i]]\n",
    "        #print(x_L)\n",
    "        channel_new_label,r_L=prune_channel([x_L],y,0,nnn=15)\n",
    "        r_L=float(r_L)\n",
    "        print('finish')\n",
    "        R_L.append(r_L)\n",
    "        print(r_L)\n",
    "    R_L=np.array(R_L)\n",
    "    LLL=[1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2]\n",
    "    RR_L=[]\n",
    "    iii=0\n",
    "    for k in range(len(LLL)):\n",
    "        #print(R_L)\n",
    "        if LLL[k]==1:\n",
    "            RR_L.append(R_L[0])\n",
    "        if LLL[k]==2:\n",
    "            RR_L.append(R_L[iii:iii+2].sum()/2)\n",
    "        iii+=LLL[k]\n",
    "    return R_L,RR_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "590d84b3-a696-4d89-aea1-bfbc1f329efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_G(model):\n",
    "    \"\"\"This function is used to obtain the number of channels of the layer group.\"\"\"\n",
    "    C=[]\n",
    "    for i in range(len(G)):\n",
    "        CG=0\n",
    "        for g in G[i]:\n",
    "            a,b,d,c=model.layers[g].output.shape\n",
    "            CG+=c\n",
    "        C.append(CG)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2592c954",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 10ms/step - loss: 0.2195 - accuracy: 0.9290\n",
      "69386880\n"
     ]
    }
   ],
   "source": [
    "P_list=[]\n",
    "E_list=[]\n",
    "F_list=[]\n",
    "#RP_list=[]\n",
    "#RRP_list=[]\n",
    "C_list=[]\n",
    "flops,par=compute_flops_params(model)\n",
    "loss, acc = model.evaluate(x_test, y_test_onehot)\n",
    "C_0=channel_G(model)\n",
    "print(flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92de2655-2a74-4fe2-9336-75b1d5504cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_FILE = \"training_ResNet32_log.json\"\n",
    "def load_progress():\n",
    "    if os.path.exists(SAVE_FILE):\n",
    "        with open(SAVE_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"results\": [], \n",
    "            \"RR_L\": [],\n",
    "            \"P_list\": [],\n",
    "            \"E_list\": [],\n",
    "            \"F_list\": [],\n",
    "            \"C_list\": [],\n",
    "            \"last_lam_idx\": 0,\n",
    "            \"last_repeat\": 0,\n",
    "            \"RL_exist\": 0,\n",
    "            \"Cri_exist\": 0}\n",
    "def save_progress(progress):\n",
    "    with open(SAVE_FILE, \"w\") as f:\n",
    "        json.dump(progress, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08883edb-6759-4e92-a0ee-3d6f8910aa50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "progress = load_progress()\n",
    "start_lr_idx = progress[\"last_lam_idx\"]\n",
    "start_repeat = progress[\"last_repeat\"]\n",
    "If_RL = progress[\"RL_exist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe227ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if If_RL == 0:\n",
    "    model=load_Res()\n",
    "    R_L,RR_L=R_layers(model,x_dist,y_dist)\n",
    "    progress[\"RR_L\"].append(RR_L)\n",
    "    progress[\"RL_exist\"] = 1\n",
    "    save_progress(progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3213016-20c5-4f54-af6a-a8d867b7f45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.125, 0.0625, 0.0625, 0.125, 0.25, 0.25, 0.09375, 0.21875, 0.21875, 0.125, 0.0, 0.1875, 0.0625, 0.140625, 0.078125, 0.28125]]\n"
     ]
    }
   ],
   "source": [
    "print(progress[\"RR_L\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ee315f9-b018-4a59-80f8-c53a69f74835",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fb7b567-0835-482f-ad43-cb999c9f1045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " lambda: Lam=0.6, Repeat=3/3\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "39234750 69386880\n",
      "Epoch 1/200\n",
      "390/390 - 27s - loss: 0.8157 - accuracy: 0.7223 - val_loss: 6.4666 - val_accuracy: 0.2626 - 27s/epoch - 68ms/step\n",
      "Epoch 2/200\n",
      "390/390 - 22s - loss: 0.6795 - accuracy: 0.7659 - val_loss: 2.8086 - val_accuracy: 0.4675 - 22s/epoch - 55ms/step\n",
      "Epoch 3/200\n",
      "390/390 - 21s - loss: 0.6024 - accuracy: 0.7931 - val_loss: 1.0704 - val_accuracy: 0.6857 - 21s/epoch - 55ms/step\n",
      "Epoch 4/200\n",
      "390/390 - 21s - loss: 0.5543 - accuracy: 0.8118 - val_loss: 1.4939 - val_accuracy: 0.5950 - 21s/epoch - 54ms/step\n",
      "Epoch 5/200\n",
      "390/390 - 22s - loss: 0.5157 - accuracy: 0.8229 - val_loss: 0.8493 - val_accuracy: 0.7324 - 22s/epoch - 55ms/step\n",
      "Epoch 6/200\n",
      "390/390 - 22s - loss: 0.4779 - accuracy: 0.8349 - val_loss: 0.7652 - val_accuracy: 0.7793 - 22s/epoch - 56ms/step\n",
      "Epoch 7/200\n",
      "390/390 - 22s - loss: 0.4272 - accuracy: 0.8540 - val_loss: 0.6708 - val_accuracy: 0.7891 - 22s/epoch - 56ms/step\n",
      "Epoch 8/200\n",
      "390/390 - 21s - loss: 0.4066 - accuracy: 0.8596 - val_loss: 0.8274 - val_accuracy: 0.7196 - 21s/epoch - 55ms/step\n",
      "Epoch 9/200\n",
      "390/390 - 22s - loss: 0.3846 - accuracy: 0.8676 - val_loss: 0.7270 - val_accuracy: 0.7814 - 22s/epoch - 55ms/step\n",
      "Epoch 10/200\n",
      "390/390 - 22s - loss: 0.3671 - accuracy: 0.8745 - val_loss: 0.6941 - val_accuracy: 0.7868 - 22s/epoch - 56ms/step\n",
      "Epoch 11/200\n",
      "390/390 - 22s - loss: 0.3526 - accuracy: 0.8792 - val_loss: 0.6293 - val_accuracy: 0.8108 - 22s/epoch - 55ms/step\n",
      "Epoch 12/200\n",
      "390/390 - 22s - loss: 0.3445 - accuracy: 0.8820 - val_loss: 0.6642 - val_accuracy: 0.7972 - 22s/epoch - 56ms/step\n",
      "Epoch 13/200\n",
      "390/390 - 21s - loss: 0.3324 - accuracy: 0.8834 - val_loss: 0.5518 - val_accuracy: 0.8293 - 21s/epoch - 55ms/step\n",
      "Epoch 14/200\n",
      "390/390 - 21s - loss: 0.3295 - accuracy: 0.8856 - val_loss: 0.7323 - val_accuracy: 0.7795 - 21s/epoch - 55ms/step\n",
      "Epoch 15/200\n",
      "390/390 - 22s - loss: 0.3182 - accuracy: 0.8893 - val_loss: 0.5397 - val_accuracy: 0.8254 - 22s/epoch - 56ms/step\n",
      "Epoch 16/200\n",
      "390/390 - 22s - loss: 0.3104 - accuracy: 0.8911 - val_loss: 0.4973 - val_accuracy: 0.8425 - 22s/epoch - 56ms/step\n",
      "Epoch 17/200\n",
      "390/390 - 21s - loss: 0.3012 - accuracy: 0.8953 - val_loss: 0.4067 - val_accuracy: 0.8615 - 21s/epoch - 55ms/step\n",
      "Epoch 18/200\n",
      "390/390 - 22s - loss: 0.3001 - accuracy: 0.8946 - val_loss: 0.4563 - val_accuracy: 0.8498 - 22s/epoch - 56ms/step\n",
      "Epoch 19/200\n",
      "390/390 - 22s - loss: 0.2953 - accuracy: 0.8960 - val_loss: 0.7034 - val_accuracy: 0.7861 - 22s/epoch - 56ms/step\n",
      "Epoch 20/200\n",
      "390/390 - 21s - loss: 0.2918 - accuracy: 0.8984 - val_loss: 0.5901 - val_accuracy: 0.8163 - 21s/epoch - 55ms/step\n",
      "Epoch 21/200\n",
      "390/390 - 21s - loss: 0.2870 - accuracy: 0.9002 - val_loss: 0.4674 - val_accuracy: 0.8487 - 21s/epoch - 55ms/step\n",
      "Epoch 22/200\n",
      "390/390 - 21s - loss: 0.2832 - accuracy: 0.9029 - val_loss: 0.6266 - val_accuracy: 0.8057 - 21s/epoch - 55ms/step\n",
      "Epoch 23/200\n",
      "390/390 - 22s - loss: 0.2806 - accuracy: 0.9027 - val_loss: 0.7492 - val_accuracy: 0.7879 - 22s/epoch - 56ms/step\n",
      "Epoch 24/200\n",
      "390/390 - 22s - loss: 0.2763 - accuracy: 0.9039 - val_loss: 0.5314 - val_accuracy: 0.8361 - 22s/epoch - 56ms/step\n",
      "Epoch 25/200\n",
      "390/390 - 21s - loss: 0.2726 - accuracy: 0.9049 - val_loss: 0.5799 - val_accuracy: 0.8275 - 21s/epoch - 55ms/step\n",
      "Epoch 26/200\n",
      "390/390 - 21s - loss: 0.2628 - accuracy: 0.9081 - val_loss: 0.5970 - val_accuracy: 0.8215 - 21s/epoch - 55ms/step\n",
      "Epoch 27/200\n",
      "390/390 - 22s - loss: 0.2653 - accuracy: 0.9075 - val_loss: 0.4982 - val_accuracy: 0.8418 - 22s/epoch - 56ms/step\n",
      "Epoch 28/200\n",
      "390/390 - 22s - loss: 0.2606 - accuracy: 0.9083 - val_loss: 0.8011 - val_accuracy: 0.7792 - 22s/epoch - 56ms/step\n",
      "Epoch 29/200\n",
      "390/390 - 22s - loss: 0.2629 - accuracy: 0.9083 - val_loss: 0.5433 - val_accuracy: 0.8362 - 22s/epoch - 57ms/step\n",
      "Epoch 30/200\n",
      "390/390 - 22s - loss: 0.2612 - accuracy: 0.9091 - val_loss: 0.5259 - val_accuracy: 0.8400 - 22s/epoch - 56ms/step\n",
      "Epoch 31/200\n",
      "390/390 - 22s - loss: 0.2593 - accuracy: 0.9104 - val_loss: 0.5177 - val_accuracy: 0.8365 - 22s/epoch - 56ms/step\n",
      "Epoch 32/200\n",
      "390/390 - 22s - loss: 0.2539 - accuracy: 0.9109 - val_loss: 0.4716 - val_accuracy: 0.8513 - 22s/epoch - 56ms/step\n",
      "Epoch 33/200\n",
      "390/390 - 22s - loss: 0.2555 - accuracy: 0.9103 - val_loss: 0.5777 - val_accuracy: 0.8184 - 22s/epoch - 56ms/step\n",
      "Epoch 34/200\n",
      "390/390 - 22s - loss: 0.2505 - accuracy: 0.9116 - val_loss: 0.5333 - val_accuracy: 0.8280 - 22s/epoch - 56ms/step\n",
      "Epoch 35/200\n",
      "390/390 - 22s - loss: 0.2457 - accuracy: 0.9136 - val_loss: 0.4471 - val_accuracy: 0.8539 - 22s/epoch - 56ms/step\n",
      "Epoch 36/200\n",
      "390/390 - 21s - loss: 0.2446 - accuracy: 0.9152 - val_loss: 0.4800 - val_accuracy: 0.8445 - 21s/epoch - 55ms/step\n",
      "Epoch 37/200\n",
      "390/390 - 22s - loss: 0.2476 - accuracy: 0.9123 - val_loss: 0.7801 - val_accuracy: 0.7773 - 22s/epoch - 55ms/step\n",
      "Epoch 38/200\n",
      "390/390 - 22s - loss: 0.2439 - accuracy: 0.9151 - val_loss: 0.4774 - val_accuracy: 0.8440 - 22s/epoch - 56ms/step\n",
      "Epoch 39/200\n",
      "390/390 - 21s - loss: 0.2446 - accuracy: 0.9136 - val_loss: 0.4560 - val_accuracy: 0.8533 - 21s/epoch - 55ms/step\n",
      "Epoch 40/200\n",
      "390/390 - 22s - loss: 0.2405 - accuracy: 0.9161 - val_loss: 0.7312 - val_accuracy: 0.7814 - 22s/epoch - 56ms/step\n",
      "Epoch 41/200\n",
      "390/390 - 22s - loss: 0.2382 - accuracy: 0.9177 - val_loss: 0.4410 - val_accuracy: 0.8597 - 22s/epoch - 56ms/step\n",
      "Epoch 42/200\n",
      "390/390 - 22s - loss: 0.2382 - accuracy: 0.9157 - val_loss: 0.5346 - val_accuracy: 0.8419 - 22s/epoch - 55ms/step\n",
      "Epoch 43/200\n",
      "390/390 - 22s - loss: 0.2360 - accuracy: 0.9166 - val_loss: 0.8500 - val_accuracy: 0.7529 - 22s/epoch - 56ms/step\n",
      "Epoch 44/200\n",
      "390/390 - 21s - loss: 0.2323 - accuracy: 0.9187 - val_loss: 0.5589 - val_accuracy: 0.8313 - 21s/epoch - 55ms/step\n",
      "Epoch 45/200\n",
      "390/390 - 22s - loss: 0.2363 - accuracy: 0.9174 - val_loss: 0.5842 - val_accuracy: 0.8312 - 22s/epoch - 56ms/step\n",
      "Epoch 46/200\n",
      "390/390 - 22s - loss: 0.2337 - accuracy: 0.9183 - val_loss: 0.6047 - val_accuracy: 0.8184 - 22s/epoch - 56ms/step\n",
      "Epoch 47/200\n",
      "390/390 - 22s - loss: 0.2307 - accuracy: 0.9179 - val_loss: 0.4092 - val_accuracy: 0.8727 - 22s/epoch - 56ms/step\n",
      "Epoch 48/200\n",
      "390/390 - 22s - loss: 0.2370 - accuracy: 0.9177 - val_loss: 0.4908 - val_accuracy: 0.8455 - 22s/epoch - 56ms/step\n",
      "Epoch 49/200\n",
      "390/390 - 21s - loss: 0.2344 - accuracy: 0.9173 - val_loss: 0.5390 - val_accuracy: 0.8368 - 21s/epoch - 55ms/step\n",
      "Epoch 50/200\n",
      "390/390 - 22s - loss: 0.2275 - accuracy: 0.9222 - val_loss: 0.4700 - val_accuracy: 0.8573 - 22s/epoch - 55ms/step\n",
      "Epoch 51/200\n",
      "390/390 - 22s - loss: 0.2321 - accuracy: 0.9184 - val_loss: 0.5014 - val_accuracy: 0.8413 - 22s/epoch - 56ms/step\n",
      "Epoch 52/200\n",
      "390/390 - 22s - loss: 0.2278 - accuracy: 0.9196 - val_loss: 0.4051 - val_accuracy: 0.8748 - 22s/epoch - 55ms/step\n",
      "Epoch 53/200\n",
      "390/390 - 21s - loss: 0.2255 - accuracy: 0.9212 - val_loss: 0.6048 - val_accuracy: 0.8155 - 21s/epoch - 55ms/step\n",
      "Epoch 54/200\n",
      "390/390 - 22s - loss: 0.2227 - accuracy: 0.9223 - val_loss: 0.7982 - val_accuracy: 0.7708 - 22s/epoch - 56ms/step\n",
      "Epoch 55/200\n",
      "390/390 - 22s - loss: 0.2265 - accuracy: 0.9212 - val_loss: 0.4375 - val_accuracy: 0.8637 - 22s/epoch - 55ms/step\n",
      "Epoch 56/200\n",
      "390/390 - 21s - loss: 0.2239 - accuracy: 0.9218 - val_loss: 0.4570 - val_accuracy: 0.8580 - 21s/epoch - 55ms/step\n",
      "Epoch 57/200\n",
      "390/390 - 22s - loss: 0.2238 - accuracy: 0.9223 - val_loss: 0.6575 - val_accuracy: 0.8081 - 22s/epoch - 56ms/step\n",
      "Epoch 58/200\n",
      "390/390 - 21s - loss: 0.2216 - accuracy: 0.9217 - val_loss: 0.3747 - val_accuracy: 0.8763 - 21s/epoch - 55ms/step\n",
      "Epoch 59/200\n",
      "390/390 - 22s - loss: 0.2212 - accuracy: 0.9226 - val_loss: 0.4482 - val_accuracy: 0.8628 - 22s/epoch - 55ms/step\n",
      "Epoch 60/200\n",
      "390/390 - 22s - loss: 0.2192 - accuracy: 0.9237 - val_loss: 0.6290 - val_accuracy: 0.8200 - 22s/epoch - 56ms/step\n",
      "Epoch 61/200\n",
      "390/390 - 22s - loss: 0.2184 - accuracy: 0.9238 - val_loss: 0.5913 - val_accuracy: 0.8332 - 22s/epoch - 56ms/step\n",
      "Epoch 62/200\n",
      "390/390 - 21s - loss: 0.2177 - accuracy: 0.9232 - val_loss: 0.4889 - val_accuracy: 0.8484 - 21s/epoch - 54ms/step\n",
      "Epoch 63/200\n",
      "390/390 - 22s - loss: 0.2134 - accuracy: 0.9251 - val_loss: 0.5107 - val_accuracy: 0.8398 - 22s/epoch - 55ms/step\n",
      "Epoch 64/200\n",
      "390/390 - 22s - loss: 0.2177 - accuracy: 0.9240 - val_loss: 0.4179 - val_accuracy: 0.8624 - 22s/epoch - 55ms/step\n",
      "Epoch 65/200\n",
      "390/390 - 22s - loss: 0.2158 - accuracy: 0.9237 - val_loss: 0.4673 - val_accuracy: 0.8582 - 22s/epoch - 56ms/step\n",
      "Epoch 66/200\n",
      "390/390 - 22s - loss: 0.2193 - accuracy: 0.9236 - val_loss: 0.4795 - val_accuracy: 0.8427 - 22s/epoch - 56ms/step\n",
      "Epoch 67/200\n",
      "390/390 - 22s - loss: 0.2114 - accuracy: 0.9260 - val_loss: 0.5110 - val_accuracy: 0.8462 - 22s/epoch - 56ms/step\n",
      "Epoch 68/200\n",
      "390/390 - 22s - loss: 0.2174 - accuracy: 0.9238 - val_loss: 0.4113 - val_accuracy: 0.8696 - 22s/epoch - 56ms/step\n",
      "Epoch 69/200\n",
      "390/390 - 22s - loss: 0.2095 - accuracy: 0.9277 - val_loss: 0.5054 - val_accuracy: 0.8444 - 22s/epoch - 55ms/step\n",
      "Epoch 70/200\n",
      "390/390 - 22s - loss: 0.2098 - accuracy: 0.9254 - val_loss: 0.4159 - val_accuracy: 0.8638 - 22s/epoch - 56ms/step\n",
      "Epoch 71/200\n",
      "390/390 - 21s - loss: 0.2116 - accuracy: 0.9248 - val_loss: 0.5559 - val_accuracy: 0.8290 - 21s/epoch - 55ms/step\n",
      "Epoch 72/200\n",
      "390/390 - 21s - loss: 0.2091 - accuracy: 0.9259 - val_loss: 0.5437 - val_accuracy: 0.8399 - 21s/epoch - 55ms/step\n",
      "Epoch 73/200\n",
      "390/390 - 22s - loss: 0.2094 - accuracy: 0.9263 - val_loss: 0.5787 - val_accuracy: 0.8346 - 22s/epoch - 56ms/step\n",
      "Epoch 74/200\n",
      "390/390 - 21s - loss: 0.2086 - accuracy: 0.9262 - val_loss: 0.5024 - val_accuracy: 0.8416 - 21s/epoch - 55ms/step\n",
      "Epoch 75/200\n",
      "390/390 - 21s - loss: 0.2046 - accuracy: 0.9284 - val_loss: 0.6392 - val_accuracy: 0.8167 - 21s/epoch - 55ms/step\n",
      "Epoch 76/200\n",
      "390/390 - 22s - loss: 0.2089 - accuracy: 0.9258 - val_loss: 0.4833 - val_accuracy: 0.8501 - 22s/epoch - 55ms/step\n",
      "Epoch 77/200\n",
      "390/390 - 22s - loss: 0.2088 - accuracy: 0.9273 - val_loss: 0.4611 - val_accuracy: 0.8585 - 22s/epoch - 56ms/step\n",
      "Epoch 78/200\n",
      "390/390 - 21s - loss: 0.2073 - accuracy: 0.9278 - val_loss: 0.4153 - val_accuracy: 0.8654 - 21s/epoch - 55ms/step\n",
      "Epoch 79/200\n",
      "390/390 - 22s - loss: 0.2029 - accuracy: 0.9279 - val_loss: 0.6250 - val_accuracy: 0.8154 - 22s/epoch - 56ms/step\n",
      "Epoch 80/200\n",
      "390/390 - 22s - loss: 0.2050 - accuracy: 0.9270 - val_loss: 0.4889 - val_accuracy: 0.8588 - 22s/epoch - 56ms/step\n",
      "Epoch 81/200\n",
      "390/390 - 21s - loss: 0.2031 - accuracy: 0.9275 - val_loss: 0.5822 - val_accuracy: 0.8285 - 21s/epoch - 55ms/step\n",
      "Epoch 82/200\n",
      "390/390 - 21s - loss: 0.2032 - accuracy: 0.9296 - val_loss: 0.5515 - val_accuracy: 0.8284 - 21s/epoch - 55ms/step\n",
      "Epoch 83/200\n",
      "390/390 - 22s - loss: 0.1998 - accuracy: 0.9299 - val_loss: 0.5490 - val_accuracy: 0.8368 - 22s/epoch - 55ms/step\n",
      "Epoch 84/200\n",
      "390/390 - 21s - loss: 0.1982 - accuracy: 0.9302 - val_loss: 0.3818 - val_accuracy: 0.8760 - 21s/epoch - 54ms/step\n",
      "Epoch 85/200\n",
      "390/390 - 22s - loss: 0.2008 - accuracy: 0.9293 - val_loss: 0.5978 - val_accuracy: 0.8282 - 22s/epoch - 56ms/step\n",
      "Epoch 86/200\n",
      "390/390 - 22s - loss: 0.1975 - accuracy: 0.9309 - val_loss: 0.4619 - val_accuracy: 0.8579 - 22s/epoch - 56ms/step\n",
      "Epoch 87/200\n",
      "390/390 - 21s - loss: 0.1968 - accuracy: 0.9312 - val_loss: 0.5607 - val_accuracy: 0.8350 - 21s/epoch - 55ms/step\n",
      "Epoch 88/200\n",
      "390/390 - 22s - loss: 0.2005 - accuracy: 0.9295 - val_loss: 0.4483 - val_accuracy: 0.8662 - 22s/epoch - 56ms/step\n",
      "Epoch 89/200\n",
      "390/390 - 22s - loss: 0.1998 - accuracy: 0.9288 - val_loss: 0.4752 - val_accuracy: 0.8546 - 22s/epoch - 55ms/step\n",
      "Epoch 90/200\n",
      "390/390 - 22s - loss: 0.1995 - accuracy: 0.9304 - val_loss: 0.4323 - val_accuracy: 0.8707 - 22s/epoch - 56ms/step\n",
      "Epoch 91/200\n",
      "390/390 - 22s - loss: 0.1984 - accuracy: 0.9301 - val_loss: 0.4328 - val_accuracy: 0.8640 - 22s/epoch - 56ms/step\n",
      "Epoch 92/200\n",
      "390/390 - 22s - loss: 0.1961 - accuracy: 0.9301 - val_loss: 0.4070 - val_accuracy: 0.8693 - 22s/epoch - 56ms/step\n",
      "Epoch 93/200\n",
      "390/390 - 22s - loss: 0.1966 - accuracy: 0.9295 - val_loss: 0.4286 - val_accuracy: 0.8720 - 22s/epoch - 56ms/step\n",
      "Epoch 94/200\n",
      "390/390 - 21s - loss: 0.1952 - accuracy: 0.9322 - val_loss: 0.4861 - val_accuracy: 0.8550 - 21s/epoch - 55ms/step\n",
      "Epoch 95/200\n",
      "390/390 - 22s - loss: 0.1955 - accuracy: 0.9310 - val_loss: 0.4374 - val_accuracy: 0.8673 - 22s/epoch - 57ms/step\n",
      "Epoch 96/200\n",
      "390/390 - 22s - loss: 0.1911 - accuracy: 0.9324 - val_loss: 0.3970 - val_accuracy: 0.8696 - 22s/epoch - 56ms/step\n",
      "Epoch 97/200\n",
      "390/390 - 21s - loss: 0.1945 - accuracy: 0.9311 - val_loss: 0.4612 - val_accuracy: 0.8619 - 21s/epoch - 55ms/step\n",
      "Epoch 98/200\n",
      "390/390 - 21s - loss: 0.1897 - accuracy: 0.9322 - val_loss: 0.6119 - val_accuracy: 0.8253 - 21s/epoch - 55ms/step\n",
      "Epoch 99/200\n",
      "390/390 - 22s - loss: 0.1938 - accuracy: 0.9327 - val_loss: 0.4966 - val_accuracy: 0.8447 - 22s/epoch - 56ms/step\n",
      "Epoch 100/200\n",
      "390/390 - 22s - loss: 0.1943 - accuracy: 0.9329 - val_loss: 0.4610 - val_accuracy: 0.8608 - 22s/epoch - 55ms/step\n",
      "Epoch 101/200\n",
      "390/390 - 22s - loss: 0.1930 - accuracy: 0.9323 - val_loss: 0.4433 - val_accuracy: 0.8660 - 22s/epoch - 56ms/step\n",
      "Epoch 102/200\n",
      "390/390 - 22s - loss: 0.1905 - accuracy: 0.9334 - val_loss: 0.4207 - val_accuracy: 0.8705 - 22s/epoch - 55ms/step\n",
      "Epoch 103/200\n",
      "390/390 - 22s - loss: 0.1857 - accuracy: 0.9351 - val_loss: 0.7139 - val_accuracy: 0.8120 - 22s/epoch - 55ms/step\n",
      "Epoch 104/200\n",
      "390/390 - 22s - loss: 0.1853 - accuracy: 0.9346 - val_loss: 0.5016 - val_accuracy: 0.8545 - 22s/epoch - 55ms/step\n",
      "Epoch 105/200\n",
      "390/390 - 21s - loss: 0.1877 - accuracy: 0.9339 - val_loss: 0.4401 - val_accuracy: 0.8630 - 21s/epoch - 55ms/step\n",
      "Epoch 106/200\n",
      "390/390 - 22s - loss: 0.1842 - accuracy: 0.9350 - val_loss: 0.5241 - val_accuracy: 0.8494 - 22s/epoch - 57ms/step\n",
      "Epoch 107/200\n",
      "390/390 - 22s - loss: 0.1854 - accuracy: 0.9340 - val_loss: 0.3954 - val_accuracy: 0.8781 - 22s/epoch - 55ms/step\n",
      "Epoch 108/200\n",
      "390/390 - 22s - loss: 0.1845 - accuracy: 0.9343 - val_loss: 0.5486 - val_accuracy: 0.8322 - 22s/epoch - 55ms/step\n",
      "Epoch 109/200\n",
      "390/390 - 22s - loss: 0.1852 - accuracy: 0.9334 - val_loss: 0.5348 - val_accuracy: 0.8497 - 22s/epoch - 56ms/step\n",
      "Epoch 110/200\n",
      "390/390 - 22s - loss: 0.1815 - accuracy: 0.9356 - val_loss: 0.4056 - val_accuracy: 0.8718 - 22s/epoch - 55ms/step\n",
      "Epoch 111/200\n",
      "390/390 - 22s - loss: 0.1836 - accuracy: 0.9362 - val_loss: 0.5746 - val_accuracy: 0.8312 - 22s/epoch - 56ms/step\n",
      "Epoch 112/200\n",
      "390/390 - 21s - loss: 0.1869 - accuracy: 0.9340 - val_loss: 0.4551 - val_accuracy: 0.8592 - 21s/epoch - 55ms/step\n",
      "Epoch 113/200\n",
      "390/390 - 21s - loss: 0.1814 - accuracy: 0.9351 - val_loss: 0.4077 - val_accuracy: 0.8718 - 21s/epoch - 55ms/step\n",
      "Epoch 114/200\n",
      "390/390 - 22s - loss: 0.1818 - accuracy: 0.9356 - val_loss: 0.5357 - val_accuracy: 0.8493 - 22s/epoch - 56ms/step\n",
      "Epoch 115/200\n",
      "390/390 - 22s - loss: 0.1758 - accuracy: 0.9378 - val_loss: 0.4032 - val_accuracy: 0.8758 - 22s/epoch - 55ms/step\n",
      "Epoch 116/200\n",
      "390/390 - 22s - loss: 0.1794 - accuracy: 0.9375 - val_loss: 0.4777 - val_accuracy: 0.8528 - 22s/epoch - 56ms/step\n",
      "Epoch 117/200\n",
      "390/390 - 22s - loss: 0.1791 - accuracy: 0.9367 - val_loss: 0.4290 - val_accuracy: 0.8624 - 22s/epoch - 56ms/step\n",
      "Epoch 118/200\n",
      "390/390 - 22s - loss: 0.1797 - accuracy: 0.9371 - val_loss: 0.4213 - val_accuracy: 0.8680 - 22s/epoch - 55ms/step\n",
      "Epoch 119/200\n",
      "390/390 - 22s - loss: 0.1755 - accuracy: 0.9379 - val_loss: 0.4415 - val_accuracy: 0.8647 - 22s/epoch - 55ms/step\n",
      "Epoch 120/200\n",
      "390/390 - 21s - loss: 0.1753 - accuracy: 0.9382 - val_loss: 0.6143 - val_accuracy: 0.8346 - 21s/epoch - 55ms/step\n",
      "Epoch 121/200\n",
      "390/390 - 21s - loss: 0.1737 - accuracy: 0.9385 - val_loss: 0.4313 - val_accuracy: 0.8663 - 21s/epoch - 55ms/step\n",
      "Epoch 122/200\n",
      "390/390 - 22s - loss: 0.1758 - accuracy: 0.9376 - val_loss: 0.4400 - val_accuracy: 0.8678 - 22s/epoch - 56ms/step\n",
      "Epoch 123/200\n",
      "390/390 - 22s - loss: 0.1740 - accuracy: 0.9385 - val_loss: 0.4173 - val_accuracy: 0.8737 - 22s/epoch - 56ms/step\n",
      "Epoch 124/200\n",
      "390/390 - 22s - loss: 0.1720 - accuracy: 0.9397 - val_loss: 0.4326 - val_accuracy: 0.8704 - 22s/epoch - 55ms/step\n",
      "Epoch 125/200\n",
      "390/390 - 22s - loss: 0.1742 - accuracy: 0.9388 - val_loss: 0.6314 - val_accuracy: 0.8093 - 22s/epoch - 56ms/step\n",
      "Epoch 126/200\n",
      "390/390 - 22s - loss: 0.1690 - accuracy: 0.9417 - val_loss: 0.5820 - val_accuracy: 0.8341 - 22s/epoch - 55ms/step\n",
      "Epoch 127/200\n",
      "390/390 - 22s - loss: 0.1754 - accuracy: 0.9379 - val_loss: 0.4091 - val_accuracy: 0.8722 - 22s/epoch - 55ms/step\n",
      "Epoch 128/200\n",
      "390/390 - 22s - loss: 0.1712 - accuracy: 0.9401 - val_loss: 0.5194 - val_accuracy: 0.8413 - 22s/epoch - 55ms/step\n",
      "Epoch 129/200\n",
      "390/390 - 21s - loss: 0.1660 - accuracy: 0.9415 - val_loss: 0.3723 - val_accuracy: 0.8844 - 21s/epoch - 55ms/step\n",
      "Epoch 130/200\n",
      "390/390 - 21s - loss: 0.1701 - accuracy: 0.9395 - val_loss: 0.3490 - val_accuracy: 0.8945 - 21s/epoch - 55ms/step\n",
      "Epoch 131/200\n",
      "390/390 - 21s - loss: 0.1675 - accuracy: 0.9408 - val_loss: 0.4494 - val_accuracy: 0.8648 - 21s/epoch - 55ms/step\n",
      "Epoch 132/200\n",
      "390/390 - 22s - loss: 0.1684 - accuracy: 0.9398 - val_loss: 0.3942 - val_accuracy: 0.8766 - 22s/epoch - 55ms/step\n",
      "Epoch 133/200\n",
      "390/390 - 22s - loss: 0.1660 - accuracy: 0.9417 - val_loss: 0.5025 - val_accuracy: 0.8515 - 22s/epoch - 55ms/step\n",
      "Epoch 134/200\n",
      "390/390 - 22s - loss: 0.1658 - accuracy: 0.9417 - val_loss: 0.3912 - val_accuracy: 0.8807 - 22s/epoch - 56ms/step\n",
      "Epoch 135/200\n",
      "390/390 - 22s - loss: 0.1615 - accuracy: 0.9438 - val_loss: 0.3978 - val_accuracy: 0.8754 - 22s/epoch - 55ms/step\n",
      "Epoch 136/200\n",
      "390/390 - 22s - loss: 0.1651 - accuracy: 0.9416 - val_loss: 0.4488 - val_accuracy: 0.8639 - 22s/epoch - 56ms/step\n",
      "Epoch 137/200\n",
      "390/390 - 22s - loss: 0.1631 - accuracy: 0.9428 - val_loss: 0.5079 - val_accuracy: 0.8490 - 22s/epoch - 55ms/step\n",
      "Epoch 138/200\n",
      "390/390 - 22s - loss: 0.1606 - accuracy: 0.9423 - val_loss: 0.5580 - val_accuracy: 0.8367 - 22s/epoch - 56ms/step\n",
      "Epoch 139/200\n",
      "390/390 - 22s - loss: 0.1608 - accuracy: 0.9434 - val_loss: 0.3570 - val_accuracy: 0.8885 - 22s/epoch - 56ms/step\n",
      "Epoch 140/200\n",
      "390/390 - 21s - loss: 0.1598 - accuracy: 0.9444 - val_loss: 0.5087 - val_accuracy: 0.8520 - 21s/epoch - 55ms/step\n",
      "Epoch 141/200\n",
      "390/390 - 21s - loss: 0.1622 - accuracy: 0.9427 - val_loss: 0.4665 - val_accuracy: 0.8614 - 21s/epoch - 55ms/step\n",
      "Epoch 142/200\n",
      "390/390 - 22s - loss: 0.1518 - accuracy: 0.9467 - val_loss: 0.4451 - val_accuracy: 0.8696 - 22s/epoch - 55ms/step\n",
      "Epoch 143/200\n",
      "390/390 - 21s - loss: 0.1610 - accuracy: 0.9421 - val_loss: 0.3757 - val_accuracy: 0.8796 - 21s/epoch - 55ms/step\n",
      "Epoch 144/200\n",
      "390/390 - 22s - loss: 0.1553 - accuracy: 0.9439 - val_loss: 0.4148 - val_accuracy: 0.8712 - 22s/epoch - 55ms/step\n",
      "Epoch 145/200\n",
      "390/390 - 22s - loss: 0.1531 - accuracy: 0.9462 - val_loss: 0.5208 - val_accuracy: 0.8513 - 22s/epoch - 56ms/step\n",
      "Epoch 146/200\n",
      "390/390 - 22s - loss: 0.1517 - accuracy: 0.9466 - val_loss: 0.4277 - val_accuracy: 0.8699 - 22s/epoch - 56ms/step\n",
      "Epoch 147/200\n",
      "390/390 - 22s - loss: 0.1540 - accuracy: 0.9470 - val_loss: 0.4714 - val_accuracy: 0.8605 - 22s/epoch - 56ms/step\n",
      "Epoch 148/200\n",
      "390/390 - 22s - loss: 0.1533 - accuracy: 0.9466 - val_loss: 0.8990 - val_accuracy: 0.7775 - 22s/epoch - 56ms/step\n",
      "Epoch 149/200\n",
      "390/390 - 22s - loss: 0.1537 - accuracy: 0.9452 - val_loss: 0.4923 - val_accuracy: 0.8528 - 22s/epoch - 56ms/step\n",
      "Epoch 150/200\n",
      "390/390 - 22s - loss: 0.1528 - accuracy: 0.9463 - val_loss: 0.3901 - val_accuracy: 0.8789 - 22s/epoch - 57ms/step\n",
      "Epoch 151/200\n",
      "390/390 - 22s - loss: 0.1486 - accuracy: 0.9478 - val_loss: 0.4464 - val_accuracy: 0.8663 - 22s/epoch - 57ms/step\n",
      "Epoch 152/200\n",
      "390/390 - 22s - loss: 0.1517 - accuracy: 0.9477 - val_loss: 0.3645 - val_accuracy: 0.8848 - 22s/epoch - 55ms/step\n",
      "Epoch 153/200\n",
      "390/390 - 22s - loss: 0.1447 - accuracy: 0.9497 - val_loss: 0.4817 - val_accuracy: 0.8602 - 22s/epoch - 56ms/step\n",
      "Epoch 154/200\n",
      "390/390 - 22s - loss: 0.1457 - accuracy: 0.9491 - val_loss: 0.3440 - val_accuracy: 0.8936 - 22s/epoch - 55ms/step\n",
      "Epoch 155/200\n",
      "390/390 - 21s - loss: 0.1433 - accuracy: 0.9502 - val_loss: 0.4344 - val_accuracy: 0.8683 - 21s/epoch - 55ms/step\n",
      "Epoch 156/200\n",
      "390/390 - 22s - loss: 0.1406 - accuracy: 0.9506 - val_loss: 0.7044 - val_accuracy: 0.8197 - 22s/epoch - 56ms/step\n",
      "Epoch 157/200\n",
      "390/390 - 22s - loss: 0.1376 - accuracy: 0.9521 - val_loss: 0.5979 - val_accuracy: 0.8380 - 22s/epoch - 56ms/step\n",
      "Epoch 158/200\n",
      "390/390 - 22s - loss: 0.1382 - accuracy: 0.9520 - val_loss: 0.5244 - val_accuracy: 0.8503 - 22s/epoch - 56ms/step\n",
      "Epoch 159/200\n",
      "390/390 - 22s - loss: 0.1400 - accuracy: 0.9510 - val_loss: 0.4123 - val_accuracy: 0.8748 - 22s/epoch - 55ms/step\n",
      "Epoch 160/200\n",
      "390/390 - 21s - loss: 0.1341 - accuracy: 0.9529 - val_loss: 0.3658 - val_accuracy: 0.8846 - 21s/epoch - 55ms/step\n",
      "Epoch 161/200\n",
      "390/390 - 22s - loss: 0.1385 - accuracy: 0.9513 - val_loss: 0.3896 - val_accuracy: 0.8874 - 22s/epoch - 55ms/step\n",
      "Epoch 162/200\n",
      "390/390 - 22s - loss: 0.1334 - accuracy: 0.9532 - val_loss: 0.3323 - val_accuracy: 0.8972 - 22s/epoch - 56ms/step\n",
      "Epoch 163/200\n",
      "390/390 - 22s - loss: 0.1303 - accuracy: 0.9538 - val_loss: 0.4638 - val_accuracy: 0.8662 - 22s/epoch - 56ms/step\n",
      "Epoch 164/200\n",
      "390/390 - 22s - loss: 0.1279 - accuracy: 0.9562 - val_loss: 0.4259 - val_accuracy: 0.8688 - 22s/epoch - 56ms/step\n",
      "Epoch 165/200\n",
      "390/390 - 22s - loss: 0.1296 - accuracy: 0.9540 - val_loss: 0.4248 - val_accuracy: 0.8712 - 22s/epoch - 56ms/step\n",
      "Epoch 166/200\n",
      "390/390 - 21s - loss: 0.1204 - accuracy: 0.9592 - val_loss: 0.3673 - val_accuracy: 0.8858 - 21s/epoch - 55ms/step\n",
      "Epoch 167/200\n",
      "390/390 - 22s - loss: 0.1250 - accuracy: 0.9574 - val_loss: 0.4920 - val_accuracy: 0.8591 - 22s/epoch - 55ms/step\n",
      "Epoch 168/200\n",
      "390/390 - 22s - loss: 0.1172 - accuracy: 0.9595 - val_loss: 0.4698 - val_accuracy: 0.8606 - 22s/epoch - 56ms/step\n",
      "Epoch 169/200\n",
      "390/390 - 21s - loss: 0.1244 - accuracy: 0.9563 - val_loss: 0.7348 - val_accuracy: 0.8048 - 21s/epoch - 55ms/step\n",
      "Epoch 170/200\n",
      "390/390 - 22s - loss: 0.1176 - accuracy: 0.9592 - val_loss: 0.2906 - val_accuracy: 0.9080 - 22s/epoch - 56ms/step\n",
      "Epoch 171/200\n",
      "390/390 - 22s - loss: 0.1166 - accuracy: 0.9601 - val_loss: 0.3698 - val_accuracy: 0.8869 - 22s/epoch - 56ms/step\n",
      "Epoch 172/200\n",
      "390/390 - 22s - loss: 0.1121 - accuracy: 0.9616 - val_loss: 0.3669 - val_accuracy: 0.8853 - 22s/epoch - 56ms/step\n",
      "Epoch 173/200\n",
      "390/390 - 22s - loss: 0.1098 - accuracy: 0.9625 - val_loss: 0.3463 - val_accuracy: 0.8978 - 22s/epoch - 56ms/step\n",
      "Epoch 174/200\n",
      "390/390 - 22s - loss: 0.1126 - accuracy: 0.9612 - val_loss: 0.3336 - val_accuracy: 0.8987 - 22s/epoch - 55ms/step\n",
      "Epoch 175/200\n",
      "390/390 - 22s - loss: 0.1080 - accuracy: 0.9637 - val_loss: 0.3926 - val_accuracy: 0.8855 - 22s/epoch - 56ms/step\n",
      "Epoch 176/200\n",
      "390/390 - 21s - loss: 0.0975 - accuracy: 0.9665 - val_loss: 0.3479 - val_accuracy: 0.8920 - 21s/epoch - 55ms/step\n",
      "Epoch 177/200\n",
      "390/390 - 22s - loss: 0.1015 - accuracy: 0.9656 - val_loss: 0.3506 - val_accuracy: 0.8926 - 22s/epoch - 56ms/step\n",
      "Epoch 178/200\n",
      "390/390 - 22s - loss: 0.0991 - accuracy: 0.9659 - val_loss: 0.3118 - val_accuracy: 0.9064 - 22s/epoch - 56ms/step\n",
      "Epoch 179/200\n",
      "390/390 - 22s - loss: 0.0976 - accuracy: 0.9672 - val_loss: 0.3137 - val_accuracy: 0.9068 - 22s/epoch - 55ms/step\n",
      "Epoch 180/200\n",
      "390/390 - 22s - loss: 0.0902 - accuracy: 0.9698 - val_loss: 0.3350 - val_accuracy: 0.8980 - 22s/epoch - 55ms/step\n",
      "Epoch 181/200\n",
      "390/390 - 22s - loss: 0.0875 - accuracy: 0.9712 - val_loss: 0.3338 - val_accuracy: 0.8971 - 22s/epoch - 56ms/step\n",
      "Epoch 182/200\n",
      "390/390 - 21s - loss: 0.0853 - accuracy: 0.9721 - val_loss: 0.3322 - val_accuracy: 0.8971 - 21s/epoch - 55ms/step\n",
      "Epoch 183/200\n",
      "390/390 - 22s - loss: 0.0835 - accuracy: 0.9717 - val_loss: 0.3392 - val_accuracy: 0.8972 - 22s/epoch - 57ms/step\n",
      "Epoch 184/200\n",
      "390/390 - 21s - loss: 0.0806 - accuracy: 0.9740 - val_loss: 0.3032 - val_accuracy: 0.9064 - 21s/epoch - 54ms/step\n",
      "Epoch 185/200\n",
      "390/390 - 21s - loss: 0.0763 - accuracy: 0.9754 - val_loss: 0.3025 - val_accuracy: 0.9048 - 21s/epoch - 55ms/step\n",
      "Epoch 186/200\n",
      "390/390 - 22s - loss: 0.0732 - accuracy: 0.9770 - val_loss: 0.3067 - val_accuracy: 0.9059 - 22s/epoch - 56ms/step\n",
      "Epoch 187/200\n",
      "390/390 - 21s - loss: 0.0689 - accuracy: 0.9795 - val_loss: 0.2839 - val_accuracy: 0.9115 - 21s/epoch - 55ms/step\n",
      "Epoch 188/200\n",
      "390/390 - 22s - loss: 0.0667 - accuracy: 0.9788 - val_loss: 0.2761 - val_accuracy: 0.9144 - 22s/epoch - 56ms/step\n",
      "Epoch 189/200\n",
      "390/390 - 21s - loss: 0.0647 - accuracy: 0.9808 - val_loss: 0.2660 - val_accuracy: 0.9172 - 21s/epoch - 55ms/step\n",
      "Epoch 190/200\n",
      "390/390 - 21s - loss: 0.0609 - accuracy: 0.9830 - val_loss: 0.3001 - val_accuracy: 0.9073 - 21s/epoch - 55ms/step\n",
      "Epoch 191/200\n",
      "390/390 - 22s - loss: 0.0589 - accuracy: 0.9838 - val_loss: 0.2593 - val_accuracy: 0.9205 - 22s/epoch - 56ms/step\n",
      "Epoch 192/200\n",
      "390/390 - 22s - loss: 0.0578 - accuracy: 0.9845 - val_loss: 0.2631 - val_accuracy: 0.9184 - 22s/epoch - 56ms/step\n",
      "Epoch 193/200\n",
      "390/390 - 22s - loss: 0.0563 - accuracy: 0.9865 - val_loss: 0.2427 - val_accuracy: 0.9231 - 22s/epoch - 56ms/step\n",
      "Epoch 194/200\n",
      "390/390 - 22s - loss: 0.0545 - accuracy: 0.9870 - val_loss: 0.2358 - val_accuracy: 0.9276 - 22s/epoch - 56ms/step\n",
      "Epoch 195/200\n",
      "390/390 - 22s - loss: 0.0558 - accuracy: 0.9862 - val_loss: 0.2454 - val_accuracy: 0.9217 - 22s/epoch - 56ms/step\n",
      "Epoch 196/200\n",
      "390/390 - 22s - loss: 0.0550 - accuracy: 0.9885 - val_loss: 0.2480 - val_accuracy: 0.9210 - 22s/epoch - 56ms/step\n",
      "Epoch 197/200\n",
      "390/390 - 21s - loss: 0.0566 - accuracy: 0.9886 - val_loss: 0.2382 - val_accuracy: 0.9222 - 21s/epoch - 55ms/step\n",
      "Epoch 198/200\n",
      "390/390 - 22s - loss: 0.0590 - accuracy: 0.9886 - val_loss: 0.2331 - val_accuracy: 0.9248 - 22s/epoch - 56ms/step\n",
      "Epoch 199/200\n",
      "390/390 - 21s - loss: 0.0627 - accuracy: 0.9894 - val_loss: 0.2360 - val_accuracy: 0.9248 - 21s/epoch - 55ms/step\n",
      "Epoch 200/200\n",
      "390/390 - 22s - loss: 0.0685 - accuracy: 0.9888 - val_loss: 0.2352 - val_accuracy: 0.9251 - 22s/epoch - 56ms/step\n",
      " Using best val_acc=0.9276 from last 20 epochs\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.2358 - accuracy: 0.9276\n",
      " Finished: Lam=0.6, Repeat=3, Acc=0.9276\n"
     ]
    }
   ],
   "source": [
    "for lam_idx in range(start_lr_idx, len(Lam)):\n",
    "    lam = Lam[lam_idx]\n",
    "    for rep in range(start_repeat, repeats):\n",
    "        print(f\"\\n lambda: Lam={lam}, Repeat={rep+1}/{repeats}\")\n",
    "        if progress[\"Cri_exist\"] == 0:\n",
    "            model=load_Res()\n",
    "            weight_list,channel_label=prune_model(model,G,P,x_dist,y_dist,lam)\n",
    "            model_p=model_pr(model,weight_list,channel_label)\n",
    "            flops_p,par_p=compute_flops_params(model_p)\n",
    "            P_=par_p/par\n",
    "            F=flops_p/flops\n",
    "            C_P=channel_G(model_p)\n",
    "            print(flops_p,flops)\n",
    "            progress[\"P_list\"].append(P_)\n",
    "            progress[\"F_list\"].append([flops_p,F])\n",
    "            progress[\"C_list\"].append([C_P])\n",
    "            progress[\"Cri_exist\"] = 1\n",
    "            save_progress(progress)\n",
    "            model_p.save(\"Res_32_pruned.h5\")\n",
    "        else:\n",
    "            model_p=tf.keras.models.load_model('Res_32_pruned.h5',custom_objects={\n",
    "                'CustomWeightDecaySGD': CustomWeightDecaySGD,\n",
    "                'WarmUpCosine': WarmUpCosine})\n",
    "            flops_p,par_p=compute_flops_params(model_p)\n",
    "            F=flops_p/flops\n",
    "            print(flops_p,flops)\n",
    "        retrain(model_p,x_train,y_train_onehot,x_test,y_test_onehot)\n",
    "        loss_p, acc_p = model_p.evaluate(x_test, y_test_onehot)\n",
    "        print(f\" Finished: Lam={lam}, Repeat={rep+1}, Acc={acc_p:.4f}\")\n",
    "        progress[\"results\"].append(acc_p)\n",
    "        progress[\"last_lam_idx\"] = lam_idx\n",
    "        progress[\"last_repeat\"] = rep+1\n",
    "        save_progress(progress)\n",
    "    progress[\"E_list\"].append(sum(progress[\"results\"])/(repeats*acc))\n",
    "    progress[\"results\"]=[]\n",
    "    progress[\"Cri_exist\"] = 0\n",
    "    progress[\"last_repeat\"] = 0\n",
    "    progress[\"last_lam_idx\"] = lam_idx + 1\n",
    "    start_repeat=0\n",
    "    save_progress(progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929510ee-f0ef-499a-8187-ca704c61a56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a17c636-278c-489b-9e1a-e7fcfc1fef75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9239, 0.923, 0.92]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.9239,0.9230,0.9200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "415d2ad0-82e2-4956-8711-9b726f738112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9248, 0.9267, 0.925]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.9248,0.9267,0.9250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d5ece55-6dc6-4738-9d8b-9860a2fc068f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9252, 0.9264, 0.9267]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.9252,0.9264,0.9267]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32437501-7dd5-494f-a3c4-66a86716e424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9272, 0.9277]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.9272,0.9277,0.9276]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e74050-cd7f-4025-b577-876917e7a8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2495a61-a930-4293-9e26-12b60993b03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fa2ba3-760b-43bc-954b-ef74e3068a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d05cea08-167f-4079-973e-c2499f1e4adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4812548982659603, 0.572333737328748, 0.5709354563792018, 0.6180637342024231]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress[\"P_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "231c0ba1-47f8-4e6d-bdda-14f4c676c44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30934594, 0.4458277126742116],\n",
       " [35613010, 0.513252793611703],\n",
       " [36925418, 0.5321671474492008],\n",
       " [39234750, 0.5654491166053294]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress[\"F_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abab6d05-d731-40a3-966b-18fc8ab29075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9927879236843274,\n",
       " 0.9962324818060408,\n",
       " 0.9968783377905288,\n",
       " 0.9983853493454461]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress[\"E_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4bf44490-4833-4b01-a8fb-567c61a41fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[7, 11, 9, 10, 13, 54, 22, 27, 25, 23, 21, 132, 47, 46, 44, 48, 36, 270]],\n",
       " [[7, 11, 11, 11, 13, 60, 23, 27, 25, 23, 22, 138, 51, 49, 45, 48, 38, 318]],\n",
       " [[9, 11, 12, 11, 13, 60, 25, 28, 26, 25, 22, 144, 53, 51, 47, 51, 40, 294]],\n",
       " [[9, 12, 13, 11, 13, 60, 26, 28, 28, 27, 24, 144, 54, 56, 50, 54, 40, 306]]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress[\"C_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196ec9cf-130b-4d04-a2cd-992f01fb5a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
